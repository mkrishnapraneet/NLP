{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import collections\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 315157\n",
      "['this', 'has', 'some', 'great', 'tips', 'as', 'always', 'and', 'is', 'helping', 'me', 'to', 'complete', 'my', 'good', 'eats', 'collection', '.']\n"
     ]
    }
   ],
   "source": [
    "input_file = '../../reviews_Movies_and_TV.json'\n",
    "# input_file = 'try.json'\n",
    "\n",
    "# Load the data\n",
    "sentences = []\n",
    "counter = 0\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if counter > 50000:\n",
    "            break\n",
    "        # add each sentence as a list of words to the sentences list, but each line of the json object is a document containing multiple sentences\n",
    "        # sentences.append(word_tokenize(json.loads(line)['reviewText']))\n",
    "        doc_sentences = sent_tokenize(json.loads(line)['reviewText'])\n",
    "        # sentences.append([word_tokenize(sentence) for sentence in doc_sentences])\n",
    "        for sentence in doc_sentences:\n",
    "            sentences.append([word.lower() for word in word_tokenize(sentence)])\n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print(sentences[0])\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 27522\n",
      "The 10 most common words are: \n",
      "[('the', 343600), (',', 278064), ('.', 272598), ('and', 181342), ('a', 160983), ('of', 154338), ('to', 138051), ('is', 124059), ('it', 108617), ('i', 103332)]\n"
     ]
    }
   ],
   "source": [
    "# form the vocabulary\n",
    "# Flatten the list of sentences into a single list of words\n",
    "words = itertools.chain.from_iterable(sentences)\n",
    "\n",
    "# Create a Counter object to count the frequency of each word\n",
    "word_counter = collections.Counter(words)\n",
    "\n",
    "# Extract the unique words from the Counter object to form the vocabulary\n",
    "min_freq = 5\n",
    "# vocabulary = set(word_counter.keys())\n",
    "# vocabulary = set(word for word, count in word_counter.items() if count >= min_freq)\n",
    "# add the word if it occurs more than min_freq times, else add <unk> token\n",
    "vocabulary = set(word if count >= min_freq else '<unk>' for word, count in word_counter.items())\n",
    "\n",
    "# add the <pad> token\n",
    "vocabulary.add('<pad>')\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print('Vocabulary size: {}'.format(len(vocabulary)))\n",
    "\n",
    "# Create a dictionary to map each word to an index\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create a dictionary to map each index to a word\n",
    "idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# print the 10 most common words\n",
    "print('The 10 most common words are: ')\n",
    "print(word_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for training\n",
    "window_size = 2\n",
    "sliding_window_size = window_size*2 + 1\n",
    "num_neg_samples_per_context = 3\n",
    "\n",
    "vocab_indices = list(word2idx.values())\n",
    "vocab_size = len(vocab_indices)\n",
    "\n",
    "# create data with X being indices of the context words and the target word, and y being 0 or 1 based on whether the target word is correct for the context words\n",
    "# also add negative samples\n",
    "def create_data_with_negative_sampling(sentences, word2idx, window_size, num_neg_samples_per_context):\n",
    "    X = []\n",
    "    y = []\n",
    "    # counter = 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            # a list of indices of context words and the target word\n",
    "            # if it goes out of bounds, add <pad> tokens            \n",
    "            context_words = sentence[max(0, i-window_size):i] + sentence[i+1:min(len(sentence), i+window_size+1)]\n",
    "            target_word = sentence[i]\n",
    "            # if the any of the words are not in the vocabulary, replace it with <unk>\n",
    "            context_words = [word if word in word2idx else '<unk>' for word in context_words]\n",
    "            target_word = target_word if target_word in word2idx else '<unk>'\n",
    "            \n",
    "            data_point = [word2idx[context_word] for context_word in context_words]\n",
    "            # if the size of the data point is less than the sliding window size, add <pad> tokens\n",
    "            # if len(data_point) < sliding_window_size:\n",
    "            data_point += [word2idx['<pad>']]*(sliding_window_size-len(data_point)-1)\n",
    "            data_point.append(word2idx[target_word])\n",
    "\n",
    "            # add this to X and y\n",
    "            X.append(data_point)\n",
    "            y.append(1)\n",
    "\n",
    "            # add negative samples\n",
    "            for _ in range(num_neg_samples_per_context):\n",
    "                # generate a random index between 0 and vocab_size\n",
    "                negative_word = random.randint(0, vocab_size-1)\n",
    "                X.append(data_point[:-1] + [negative_word])                \n",
    "                y.append(0)\n",
    "        # counter += 1\n",
    "        # print(counter)\n",
    "    return X, y \n",
    "            \n",
    "\n",
    "    #         # convert the words to indices and add to X as [target_index, context_index1]\n",
    "    #         for context_word in context_words:\n",
    "    #             data_point = [word2idx[target_word], word2idx[context_word]]\n",
    "    #             X.append(data_point)\n",
    "    #             y.append(1)\n",
    "    #             # add negative samples\n",
    "    #             for _ in range(num_neg_samples_per_context):\n",
    "    #                 # generate a random index between 0 and vocab_size\n",
    "    #                 negative_word = random.randint(0, vocab_size-1)\n",
    "    #                 X.append([word2idx[target_word], negative_word])                \n",
    "    #                 y.append(0)\n",
    "    # return X, y\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "X, y = create_data_with_negative_sampling(sentences, word2idx, window_size, num_neg_samples_per_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# split the data into train and test\n",
    "\n",
    "# save the data to a file so that it can be loaded later\n",
    "# np.savez('data.npz', X=X, y=y)\n",
    "\n",
    "# load the data from the file\n",
    "def load_data(filename):\n",
    "    data = np.load(filename)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 27932812\n",
      "Number of labels: 27932812\n",
      "index of <unk> is: 8584\n",
      "index of <pad> is: 11114\n",
      "[ 6337 18920  4842 19852 11350]   0\n",
      "[15477 11610 18544 17481 20024]   1\n",
      "[15306  6860 13880 22112  1241]   1\n",
      "[17534 13777 22909 25789 15340]   0\n",
      "[10685 11690 15306 15467 24283]   1\n",
      "[18630  8584  4269  5990  8072]   0\n",
      "[23119 11659 11114 11114 11006]   0\n",
      "[ 7139 12097 15882  3256 22405]   0\n",
      "[20881 20867 15853  4731 12444]   0\n",
      "[21800 17534 20466 24884  7112]   0\n",
      "[ 7778 25661 19683 10285 10173]   0\n",
      "[17534  2658  8584 21221 20339]   1\n",
      "[ 4731  1912 17059 11114 12788]   0\n",
      "[17481 24110  4440 21587  2342]   0\n",
      "[25427 13931 10147 22313 20653]   0\n",
      "[20246 21587  8421 14400 21336]   0\n",
      "[13274 10986 17534 26758 12511]   0\n",
      "[ 2315 17534 11586 25374 15893]   1\n",
      "[ 5643 17534 14169 11114 18434]   1\n",
      "[ 8548 17481  9366 17481  5151]   1\n",
      "[17534 13777  5631 11659 14309]   0\n",
      "[17481  5774 23156  4178 26464]   0\n",
      "[ 8872 17229  8421 21221 25940]   1\n",
      "[ 3637 17481 25254 17481  3487]   1\n",
      "[26467  8797 15093 25789 16202]   0\n",
      "[ 8867 12985  7128 14400  1560]   0\n",
      "[27158 15306 23755 22627  5227]   1\n",
      "[ 6860  7139  7778 11690 22992]   0\n",
      "[17481  6860 16916 20121 11464]   0\n",
      "[17481 21221 26793 17534 27033]   0\n",
      "[ 6860 13639  6478 22627 17321]   1\n",
      "[  125 11314 13989 25725 19859]   0\n",
      "[ 1853 12360 13697  8610  7711]   0\n",
      "[ 8475 11586 14169 11114 18338]   0\n",
      "[17321 14703 24640 27061   225]   0\n",
      "[13557 21221 21576  2611 20466]   1\n",
      "[17534  8584 21587 19327 25272]   1\n",
      "[17059 14936 11114 11114 15417]   0\n",
      "[15306 24823 14169 11114 11084]   0\n",
      "[14400 11153  7778  7915 24891]   0\n",
      "[27278 18172  4468 12985  6804]   0\n",
      "[ 8421 20445  7501 17481  3100]   0\n",
      "[ 4074 22627 21587 23581 18433]   0\n",
      "[13618  5429 15306 21219  4481]   0\n",
      "[19231 11153 19889 22188  4319]   0\n",
      "[13697  7778  5559 22126  6860]   1\n",
      "[24349  8421  5966  4731 17534]   1\n",
      "[ 6490 20024 25849 17534 23634]   0\n",
      "[19367 16397 11114 11114 19341]   1\n",
      "[17481 21221 14400 13707 24890]   0\n"
     ]
    }
   ],
   "source": [
    "print('Number of data points: {}'.format(len(X)))\n",
    "print('Number of labels: {}'.format(len(y)))\n",
    "\n",
    "# print(vocab_indices)\n",
    "print('index of <unk> is: {}'.format(word2idx['<unk>']))\n",
    "print('index of <pad> is: {}'.format(word2idx['<pad>']))\n",
    "\n",
    "for i in range (50):\n",
    "    print('{}   {}'.format(X[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow with negative sampling\n",
    "# hyperparameters\n",
    "embedding_size = 100\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# initialize the weights\n",
    "# embedding matrix\n",
    "# embeddings = np.random.uniform(-1, 1, (len(vocabulary), embedding_size))\n",
    "\n",
    "# use the same embedding matrix for both context and target\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# create batches\n",
    "def create_batches(X, y, batch_size):\n",
    "    batches = []\n",
    "    num_batches = len(X) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch = (X[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size])\n",
    "        batches.append(batch)\n",
    "    return batches       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.38412303612435933\n",
      "Epoch: 1, Loss: 0.25830396371112274\n",
      "Epoch: 2, Loss: 0.24422869358112104\n",
      "Epoch: 3, Loss: 0.2395569920404676\n",
      "Epoch: 4, Loss: 0.23916197209992945\n",
      "Epoch: 5, Loss: 0.2401143232171849\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "'''\n",
    "first, we get the embeddings of the context words and the target word\n",
    "we then average the embeddings of the context words to get the context embedding\n",
    "we then take the cosine similarity between the context embedding and the target embedding\n",
    "we then use the sigmoid function to get the probability of the target word being the correct word for the context words\n",
    "we then calculate the loss by subtracting the probability from the actual label\n",
    "we then backpropagate the loss to update the weights\n",
    "'''\n",
    "# we can use tensors to perform the operations\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# initialize the weights\n",
    "# embedding matrix\n",
    "# embeddings = torch.randn(len(vocabulary), embedding_size, requires_grad=True)\n",
    "embeddings = torch.randn(len(vocabulary), embedding_size, requires_grad=True, device=device)\n",
    "# embeddings = torch.zeros(len(vocabulary), embedding_size, requires_grad=True, device=device)\n",
    "\n",
    "# embeddings = embeddings.to(device)\n",
    "\n",
    "# # write a manual loss function\n",
    "# def loss_fn(y_pred, y):\n",
    "#     return torch.sum(y - y_pred)\n",
    "\n",
    "# write a function to train the model using gpu\n",
    "\n",
    "def train(X, y, embeddings, learning_rate, epochs, batch_size):\n",
    "    # convert X and y to torch tensors\n",
    "    X = torch.LongTensor(X)\n",
    "    X = X.to(device)\n",
    "    y = torch.FloatTensor(y)\n",
    "    y = y.to(device)\n",
    "    # create batches\n",
    "    batches = create_batches(X, y, batch_size)\n",
    "    # create an optimizer\n",
    "    optimizer = torch.optim.Adam([embeddings], lr=learning_rate)\n",
    "    # create a loss function for regression\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    prev_loss = 1000\n",
    "    # train the model\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in batches:\n",
    "            # get the embeddings of the context words and the target word\n",
    "            # print(X_batch.shape)\n",
    "            context_embeddings = embeddings[X_batch[:, :-1]]\n",
    "            target_embeddings = embeddings[X_batch[:, -1]]\n",
    "            # context_embeddings = embeddings[X_batch[:, 1]]\n",
    "            # target_embeddings = embeddings[X_batch[:, 0]]\n",
    "            # print(context_embeddings.shape)\n",
    "            # print(target_embeddings.shape)\n",
    "\n",
    "            # average the context embeddings\n",
    "            context_embeddings = torch.mean(context_embeddings, dim=1)\n",
    "\n",
    "            # calculate the dot product between the context embedding and the target embedding\n",
    "            logits = torch.sum(context_embeddings * target_embeddings, dim=1)\n",
    "\n",
    "            \n",
    "            # # normalize the logits\n",
    "            # logits = logits / (torch.norm(context_embeddings, dim=1) * torch.norm(target_embeddings, dim=1))\n",
    "            # use the sigmoid function to get the probability of the target word being the correct word for the context words\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = loss_fn(probs, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "            # backpropagate the loss to update the weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print('Epoch: {}, Loss: {}'.format(epoch, epoch_loss/len(batches)))\n",
    "        if epoch_loss/len(batches) > prev_loss:\n",
    "            break\n",
    "        prev_loss = epoch_loss/len(batches)\n",
    "\n",
    "\n",
    "# train the model\n",
    "train(X, y, embeddings, learning_rate, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t\t41.74229049682617\n",
      "it\t\t40.50118637084961\n",
      "this\t\t31.16708755493164\n",
      "you\t\t30.292802810668945\n",
      "<pad>\t\t29.952171325683594\n",
      "film\t\t28.937746047973633\n",
      "a\t\t25.20366668701172\n",
      ";\t\t24.518835067749023\n",
      "is\t\t23.531322479248047\n",
      ",\t\t23.40203857421875\n"
     ]
    }
   ],
   "source": [
    "# print 10 most similar words to a given word\n",
    "def most_similar(word, embeddings, k):\n",
    "    # get the embedding of the word\n",
    "    word_embedding = embeddings[word2idx[word]]\n",
    "    # calculate the cosine similarity between the word embedding and the embeddings of all the words\n",
    "    similarities = torch.matmul(word_embedding, embeddings.T)\n",
    "    # get the k most similar words\n",
    "    top_k = torch.topk(similarities, k+1)[1].tolist()\n",
    "    most_similar = []\n",
    "    for idx in top_k:\n",
    "        if idx != word2idx[word]:\n",
    "            most_similar.append([idx2word[idx], similarities[idx].item()])\n",
    "    return most_similar\n",
    "\n",
    "\n",
    "sim_words = most_similar('movie', embeddings, 10)\n",
    "for word, similarity in sim_words:\n",
    "    print('{}\\t\\t{}'.format(word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embeddings\n",
    "def save_embeddings(embeddings, filename):\n",
    "    embeddings = embeddings.cpu().detach().numpy()\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "# load the embeddings\n",
    "def load_embeddings(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_embeddings(embeddings, 'embeddings.pkl')\n",
    "# embeddings = load_embeddings('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
