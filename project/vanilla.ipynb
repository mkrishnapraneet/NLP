{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "import torchtext\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "batch_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "('two options the us views the transitional national council as the sole / only legitimate interlocutor of the libyan people during this interim period , as libyans come together to plan their own future and a permanent , inclusive constitutional system that protects the rights of all libyans . <eos> this is in contrast to the qadhafi regime , which has lost all legitimacy to rule . <eos> the us views the transitional national council as the legitimate interlocutor of the libyan people during this interim period , as libyans come together to plan their own future and a permanent , inclusive constitutional system that protects the rights of all libyans . <eos> this is in contrast to the qadhafi regime , which has lost all legitimacy to rule . <eos> the inc is the institution through which we are engaging the libyan people at this time . <eos>', 2)\n",
      "(\"ambassador , we just received an email from the adoption service provider about these cases . <eos> i am currently reviewing the files to determine if they qualify within the guidelines established recently by uscis and dos . <eos> out of the 60 cases , 40 are being adopted by usc . <eos> out of the 40 , 5 of the cases were children escorted today and per cnn have just landed in miami . <eos> the remaining 35 we are reviewing now . <eos> some correspondence i 've seen says that they plan on bringing the children straight to the airport , and we have responded that they first need to come to the embassy . <eos> once case in particular will be an issue , as it requires the presidential waiver . <eos> did you receive the talking points about presidential waivers ? <eos> the majority of the cases will qualify for humanitarian parole . <eos> in the last 30 minutes a number of emails have been coming in about private planes , ted turner offering to fly children out , etc . <eos> we have a case coming in tomorrow morning in which the 2 adoptive families are being flown in by cbs helicopter from the dr . <eos> we are preparing as many cases in advance as possible tonight . <eos> kind regards , linda <eos>\", 0)\n",
      "(\"hillary , as you have been so kind to share your advice and mentorship with me in the past , i was hoping that we might find some time to chat about some of my current thinking . <eos> with the fabulous results of the presidents ' re - election , i am privileged to be able to continue to serve at the cftc . <eos> i recall your prior advice upon my being offered the post <eos> and it was invaluable . <eos> the challenges and opportunities to bring about common sense financial reforms have been fabulous . <eos> we are now moving beyond agency rule writing and helping the swaps markets transition to a new era of transparency and oversight . <eos> ( the cftc just completed final determinations such that the us has met the pittsburgh g-20 commitment deadline of december 2012 to have our swaps clearing mandate fully in place . ) <eos> if we might be able to find a moment to chat , i would love to share my thoughts on possible new challenges and opportunities within the administration . <eos> i hope all is well with you as well as you gear up for this holiday time and a much deserved break from the day to day stresses of your current post . <eos> gary <eos>\", 1)\n",
      "200\n",
      "(\"madame secretary : thank you for reaching out to secretary solis and convincing her to join the verification commission . <eos> she and ricardo lagos will make for a very high profile and effective international component of the commission . <eos> we will meet with her this morning at 10 am to brief her for tuesday 's journey to honduras . <eos> at this point , it appears we have a military aircraft available . <eos> the plan is for the u.s. delegation to depart d.c. , stop in miami to pick up ricardo lagos , and arrive in tegucigalpa together . <eos> we think this will send a powerful message to hondurans and leave no doubt about our commitment to seeing this process through to a successful conclusion . <eos> you should be aware that ambassador hugo llorens is under public assault . <eos> the wall street journal dedicates its america 's column this morning to attacking him and calling for his removal . <eos> last friday , representative connie mack did the same . <eos> this chorus will grow as the extent of our accomplishment is understood . <eos> llorens is a tough , stalwart guy . <eos> he and his mission have held firm during this crisis . <eos> a call from you would be a big boost . <eos> finally , we will hold an ipc today to identify further steps . <eos> we will keep you up to date on these steps and identify further opportunities for your engagement . <eos> i want to thank you for your leadership and support during this long crisis . <eos> your willingness to engage at key moments and take risks at the right time have propelled us much further than anyone expected . <eos> your diplomacy prevented a debilitating civil conflict in honduras that would have destabilized central america and undermined two decades of our efforts . <eos> we now have a big opportunity in front of us , and for that we are grateful to you . <eos> regards , tom <eos>\", 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_inp_file = 'processed_data/GCDC/Clinton_train.jsonl'\n",
    "with open(train_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        # put <EOS> at the end of each sentence and add each sentence to the list\n",
    "        for i in range(len(json_obj['sentences'])):\n",
    "            json_obj['sentences'][i].append('<EOS>')\n",
    "        # merge all sentences into one\n",
    "        document = \" \".join([word for sentence in json_obj['sentences'] for word in sentence])\n",
    "        document = document.lower()\n",
    "        label = json_obj['label']\n",
    "        train_data.append((document, label-1))\n",
    "   \n",
    "\n",
    "test_data = []\n",
    "test_inp_file = 'processed_data/GCDC/Clinton_test.jsonl'\n",
    "with open(test_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        # put <EOS> at the end of each sentence and add each sentence to the list\n",
    "        for i in range(len(json_obj['sentences'])):\n",
    "            json_obj['sentences'][i].append('<EOS>')\n",
    "        # merge all sentences into one\n",
    "        document = \" \".join([word for sentence in json_obj['sentences'] for word in sentence])\n",
    "        document = document.lower()\n",
    "        label = json_obj['label']\n",
    "        test_data.append((document, label-1))\n",
    "        \n",
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "print(train_data[1])\n",
    "print(train_data[2])\n",
    "print(len(test_data))\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(bert.config.hidden_size)\n",
    "\n",
    "max_pad_len = 512\n",
    "pad_id = 0\n",
    "\n",
    "class GCDCDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        document, label = self.data[idx]\n",
    "        tokenized_document = self.tokenizer.tokenize(document)\n",
    "        indexed_document = self.tokenizer.convert_tokens_to_ids(tokenized_document)\n",
    "        indexed_document = indexed_document[:max_pad_len]\n",
    "        indexed_document = indexed_document + [pad_id] * (max_pad_len - len(indexed_document))\n",
    "        return torch.tensor(indexed_document), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n",
      "(tensor([ 2048,  7047,  1996,  2149,  5328,  1996, 17459,  2120,  2473,  2004,\n",
      "         1996,  7082,  1013,  2069, 11476,  6970,  4135, 12690,  2953,  1997,\n",
      "         1996, 19232,  2111,  2076,  2023,  9455,  2558,  1010,  2004, 19232,\n",
      "         2015,  2272,  2362,  2000,  2933,  2037,  2219,  2925,  1998,  1037,\n",
      "         4568,  1010, 18678,  6543,  2291,  2008, 18227,  1996,  2916,  1997,\n",
      "         2035, 19232,  2015,  1012,  1026,  1041,  2891,  1028,  2023,  2003,\n",
      "         1999,  5688,  2000,  1996,  1053,  4215,  3270,  8873,  6939,  1010,\n",
      "         2029,  2038,  2439,  2035, 22568,  2000,  3627,  1012,  1026,  1041,\n",
      "         2891,  1028,  1996,  2149,  5328,  1996, 17459,  2120,  2473,  2004,\n",
      "         1996, 11476,  6970,  4135, 12690,  2953,  1997,  1996, 19232,  2111,\n",
      "         2076,  2023,  9455,  2558,  1010,  2004, 19232,  2015,  2272,  2362,\n",
      "         2000,  2933,  2037,  2219,  2925,  1998,  1037,  4568,  1010, 18678,\n",
      "         6543,  2291,  2008, 18227,  1996,  2916,  1997,  2035, 19232,  2015,\n",
      "         1012,  1026,  1041,  2891,  1028,  2023,  2003,  1999,  5688,  2000,\n",
      "         1996,  1053,  4215,  3270,  8873,  6939,  1010,  2029,  2038,  2439,\n",
      "         2035, 22568,  2000,  3627,  1012,  1026,  1041,  2891,  1028,  1996,\n",
      "         4297,  2003,  1996,  5145,  2083,  2029,  2057,  2024, 11973,  1996,\n",
      "        19232,  2111,  2012,  2023,  2051,  1012,  1026,  1041,  2891,  1028,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), tensor(2))\n",
      "(tensor([10602,  3187,  1024,  4067,  2017,  2005,  4285,  2041,  2000,  3187,\n",
      "        14017,  2483,  1998, 13359,  2014,  2000,  3693,  1996, 22616,  3222,\n",
      "         1012,  1026,  1041,  2891,  1028,  2016,  1998, 13559, 16738,  2097,\n",
      "         2191,  2005,  1037,  2200,  2152,  6337,  1998,  4621,  2248,  6922,\n",
      "         1997,  1996,  3222,  1012,  1026,  1041,  2891,  1028,  2057,  2097,\n",
      "         3113,  2007,  2014,  2023,  2851,  2012,  2184,  2572,  2000,  4766,\n",
      "         2014,  2005,  9857,  1005,  1055,  4990,  2000, 14373,  1012,  1026,\n",
      "         1041,  2891,  1028,  2012,  2023,  2391,  1010,  2009,  3544,  2057,\n",
      "         2031,  1037,  2510,  2948,  2800,  1012,  1026,  1041,  2891,  1028,\n",
      "         1996,  2933,  2003,  2005,  1996,  1057,  1012,  1055,  1012, 10656,\n",
      "         2000, 18280,  1040,  1012,  1039,  1012,  1010,  2644,  1999,  5631,\n",
      "         2000,  4060,  2039, 13559, 16738,  1010,  1998,  7180,  1999,  8915,\n",
      "        12193,  6895,  9692,  4502,  2362,  1012,  1026,  1041,  2891,  1028,\n",
      "         2057,  2228,  2023,  2097,  4604,  1037,  3928,  4471,  2000, 10189,\n",
      "        24979,  6962,  1998,  2681,  2053,  4797,  2055,  2256,  8426,  2000,\n",
      "         3773,  2023,  2832,  2083,  2000,  1037,  3144,  7091,  1012,  1026,\n",
      "         1041,  2891,  1028,  2017,  2323,  2022,  5204,  2008,  6059,  9395,\n",
      "         2222,  5686,  3619,  2003,  2104,  2270,  6101,  1012,  1026,  1041,\n",
      "         2891,  1028,  1996,  2813,  2395,  3485,  2139, 16467,  2015,  2049,\n",
      "         2637,  1005,  1055,  5930,  2023,  2851,  2000,  7866,  2032,  1998,\n",
      "         4214,  2005,  2010,  8208,  1012,  1026,  1041,  2891,  1028,  2197,\n",
      "         5958,  1010,  4387, 16560, 11349,  2106,  1996,  2168,  1012,  1026,\n",
      "         1041,  2891,  1028,  2023,  7165,  2097,  4982,  2004,  1996,  6698,\n",
      "         1997,  2256, 24718,  2003,  5319,  1012,  1026,  1041,  2891,  1028,\n",
      "         2222,  5686,  3619,  2003,  1037,  7823,  1010,  2358,  2389, 18367,\n",
      "         3124,  1012,  1026,  1041,  2891,  1028,  2002,  1998,  2010,  3260,\n",
      "         2031,  2218,  3813,  2076,  2023,  5325,  1012,  1026,  1041,  2891,\n",
      "         1028,  1037,  2655,  2013,  2017,  2052,  2022,  1037,  2502, 12992,\n",
      "         1012,  1026,  1041,  2891,  1028,  2633,  1010,  2057,  2097,  2907,\n",
      "         2019, 12997,  2278,  2651,  2000,  6709,  2582,  4084,  1012,  1026,\n",
      "         1041,  2891,  1028,  2057,  2097,  2562,  2017,  2039,  2000,  3058,\n",
      "         2006,  2122,  4084,  1998,  6709,  2582,  6695,  2005,  2115,  8147,\n",
      "         1012,  1026,  1041,  2891,  1028,  1045,  2215,  2000,  4067,  2017,\n",
      "         2005,  2115,  4105,  1998,  2490,  2076,  2023,  2146,  5325,  1012,\n",
      "         1026,  1041,  2891,  1028,  2115, 19732,  2000,  8526,  2012,  3145,\n",
      "         5312,  1998,  2202, 10831,  2012,  1996,  2157,  2051,  2031, 15801,\n",
      "         2149,  2172,  2582,  2084,  3087,  3517,  1012,  1026,  1041,  2891,\n",
      "         1028,  2115, 17610,  8729,  1037,  2139, 14454, 16518,  2942,  4736,\n",
      "         1999, 14373,  2008,  2052,  2031,  4078,  2696, 14454,  3550,  2430,\n",
      "         2637,  1998, 25174,  2094,  2048,  5109,  1997,  2256,  4073,  1012,\n",
      "         1026,  1041,  2891,  1028,  2057,  2085,  2031,  1037,  2502,  4495,\n",
      "         1999,  2392,  1997,  2149,  1010,  1998,  2005,  2008,  2057,  2024,\n",
      "         8794,  2000,  2017,  1012,  1026,  1041,  2891,  1028, 12362,  1010,\n",
      "         3419,  1026,  1041,  2891,  1028,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "train_dataset = GCDCDataset(train_data, tokenizer)\n",
    "test_dataset = GCDCDataset(test_data, tokenizer)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# define a transformer model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = Classifier(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        # output: (batch_size, hidden_size)\n",
    "        output = self.bert(x)[0][:, 0, :]\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to train the model\n",
    "def train(model, optimizer, criterion, train_loader, test_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for document, label in train_loader:\n",
    "            # document: (batch_size, seq_len)\n",
    "            # label: (batch_size)\n",
    "            document = document.to(device)\n",
    "            label = label.to(device)\n",
    "            # output: (batch_size, num_classes)\n",
    "            output = model(document)\n",
    "            # output: (batch_size, num_classes)\n",
    "            loss = criterion(output, label)\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "        test(model, test_loader)\n",
    "\n",
    "# define a function to test the model\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for document, label in test_loader:\n",
    "            # document: (batch_size, seq_len)\n",
    "            # label: (batch_size)\n",
    "            document = document.to(device)\n",
    "            label = label.to(device)\n",
    "            # output: (batch_size, num_classes)\n",
    "            output = model(document)\n",
    "            # print(output)\n",
    "            # output: (batch_size)\n",
    "            output = torch.argmax(output, dim=1)\n",
    "            correct += torch.sum(torch.eq(output, label)).item()\n",
    "            total += len(label)\n",
    "    print('Accuracy: {}'.format(correct / total))\n",
    "\n",
    "# define a function to predict the label of a document\n",
    "def predict(model, document):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # document: (seq_len)\n",
    "        document = torch.LongTensor(document).unsqueeze(0).to(device)\n",
    "        # output: (1, num_classes)\n",
    "        output = model(document)\n",
    "        # output: (1)\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        return output.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transformer model\n",
    "hidden_size = bert.config.hidden_size\n",
    "num_classes = 3\n",
    "model = Transformer(hidden_size, num_classes).to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train(model, optimizer, criterion, train_loader, test_loader, num_epochs)\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39m# backpropagation\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "num_epochs = 10\n",
    "train(model, optimizer, criterion, train_loader, test_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "226\n",
      "165\n",
      "409\n"
     ]
    }
   ],
   "source": [
    "# print the labels of the test data\n",
    "num_1 = 0\n",
    "num_2 = 0\n",
    "for document, label in train_dataset:\n",
    "    print(label)\n",
    "    if label == 2:\n",
    "        num_2 += 1\n",
    "    elif label == 1:\n",
    "        num_1 += 1\n",
    "\n",
    "print(800 - num_1 - num_2)\n",
    "print(num_1)\n",
    "print(num_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
