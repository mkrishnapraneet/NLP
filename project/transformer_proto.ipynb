{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "('two options the us views the transitional national council as the sole / only legitimate interlocutor of the libyan people during this interim period , as libyans come together to plan their own future and a permanent , inclusive constitutional system that protects the rights of all libyans . <eos> this is in contrast to the qadhafi regime , which has lost all legitimacy to rule . <eos> the us views the transitional national council as the legitimate interlocutor of the libyan people during this interim period , as libyans come together to plan their own future and a permanent , inclusive constitutional system that protects the rights of all libyans . <eos> this is in contrast to the qadhafi regime , which has lost all legitimacy to rule . <eos> the inc is the institution through which we are engaging the libyan people at this time . <eos>', 3)\n",
      "(\"ambassador , we just received an email from the adoption service provider about these cases . <eos> i am currently reviewing the files to determine if they qualify within the guidelines established recently by uscis and dos . <eos> out of the 60 cases , 40 are being adopted by usc . <eos> out of the 40 , 5 of the cases were children escorted today and per cnn have just landed in miami . <eos> the remaining 35 we are reviewing now . <eos> some correspondence i 've seen says that they plan on bringing the children straight to the airport , and we have responded that they first need to come to the embassy . <eos> once case in particular will be an issue , as it requires the presidential waiver . <eos> did you receive the talking points about presidential waivers ? <eos> the majority of the cases will qualify for humanitarian parole . <eos> in the last 30 minutes a number of emails have been coming in about private planes , ted turner offering to fly children out , etc . <eos> we have a case coming in tomorrow morning in which the 2 adoptive families are being flown in by cbs helicopter from the dr . <eos> we are preparing as many cases in advance as possible tonight . <eos> kind regards , linda <eos>\", 1)\n",
      "(\"hillary , as you have been so kind to share your advice and mentorship with me in the past , i was hoping that we might find some time to chat about some of my current thinking . <eos> with the fabulous results of the presidents ' re - election , i am privileged to be able to continue to serve at the cftc . <eos> i recall your prior advice upon my being offered the post <eos> and it was invaluable . <eos> the challenges and opportunities to bring about common sense financial reforms have been fabulous . <eos> we are now moving beyond agency rule writing and helping the swaps markets transition to a new era of transparency and oversight . <eos> ( the cftc just completed final determinations such that the us has met the pittsburgh g-20 commitment deadline of december 2012 to have our swaps clearing mandate fully in place . ) <eos> if we might be able to find a moment to chat , i would love to share my thoughts on possible new challenges and opportunities within the administration . <eos> i hope all is well with you as well as you gear up for this holiday time and a much deserved break from the day to day stresses of your current post . <eos> gary <eos>\", 2)\n",
      "200\n",
      "(\"madame secretary : thank you for reaching out to secretary solis and convincing her to join the verification commission . <eos> she and ricardo lagos will make for a very high profile and effective international component of the commission . <eos> we will meet with her this morning at 10 am to brief her for tuesday 's journey to honduras . <eos> at this point , it appears we have a military aircraft available . <eos> the plan is for the u.s. delegation to depart d.c. , stop in miami to pick up ricardo lagos , and arrive in tegucigalpa together . <eos> we think this will send a powerful message to hondurans and leave no doubt about our commitment to seeing this process through to a successful conclusion . <eos> you should be aware that ambassador hugo llorens is under public assault . <eos> the wall street journal dedicates its america 's column this morning to attacking him and calling for his removal . <eos> last friday , representative connie mack did the same . <eos> this chorus will grow as the extent of our accomplishment is understood . <eos> llorens is a tough , stalwart guy . <eos> he and his mission have held firm during this crisis . <eos> a call from you would be a big boost . <eos> finally , we will hold an ipc today to identify further steps . <eos> we will keep you up to date on these steps and identify further opportunities for your engagement . <eos> i want to thank you for your leadership and support during this long crisis . <eos> your willingness to engage at key moments and take risks at the right time have propelled us much further than anyone expected . <eos> your diplomacy prevented a debilitating civil conflict in honduras that would have destabilized central america and undermined two decades of our efforts . <eos> we now have a big opportunity in front of us , and for that we are grateful to you . <eos> regards , tom <eos>\", 3)\n",
      "11943\n",
      "['another', 'luxor', 'wheldon', 'impinging', '10:21am', 'lessons', 'exciting', 'kanjorski', 'documenting', 'exaction']\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_inp_file = 'processed_data/GCDC/Clinton_train.jsonl'\n",
    "with open(train_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        # put <EOS> at the end of each sentence and add each sentence to the list\n",
    "        for i in range(len(json_obj['sentences'])):\n",
    "            json_obj['sentences'][i].append('<EOS>')\n",
    "        # merge all sentences into one\n",
    "        document = \" \".join([word for sentence in json_obj['sentences'] for word in sentence])\n",
    "        document = document.lower()\n",
    "        label = json_obj['label']\n",
    "        train_data.append((document, label))\n",
    "   \n",
    "\n",
    "test_data = []\n",
    "test_inp_file = 'processed_data/GCDC/Clinton_test.jsonl'\n",
    "with open(test_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        # put <EOS> at the end of each sentence and add each sentence to the list\n",
    "        for i in range(len(json_obj['sentences'])):\n",
    "            json_obj['sentences'][i].append('<EOS>')\n",
    "        # merge all sentences into one\n",
    "        document = \" \".join([word for sentence in json_obj['sentences'] for word in sentence])\n",
    "        document = document.lower()\n",
    "        label = json_obj['label']\n",
    "        test_data.append((document, label))\n",
    "        \n",
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "print(train_data[1])\n",
    "print(train_data[2])\n",
    "print(len(test_data))\n",
    "print(test_data[0])\n",
    "\n",
    "# create vocabulary\n",
    "vocab = set()\n",
    "for document, label in train_data:\n",
    "    for word in document.split():\n",
    "        vocab.add(word.lower())\n",
    "vocab = list(vocab)\n",
    "vocab.append('<PAD>')\n",
    "vocab.append('<UNK>')\n",
    "print(len(vocab))\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max doc length : 415\n",
      "average doc length : 194.9475\n"
     ]
    }
   ],
   "source": [
    "# transform the documents into list of indices\n",
    "def transform_doc(document, vocab):\n",
    "    indices = []\n",
    "    for word in document.split():\n",
    "        if word.lower() in vocab:\n",
    "            indices.append(vocab.index(word.lower()))\n",
    "        else:\n",
    "            indices.append(vocab.index('<UNK>'))\n",
    "    return indices\n",
    "\n",
    "# do it for all the sentences\n",
    "train_data = [(transform_doc(document, vocab), label) for document, label in train_data]\n",
    "test_data = [(transform_doc(document, vocab), label) for document, label in test_data]\n",
    "# print(train_data[0])\n",
    "# print(test_data[0])\n",
    "\n",
    "# get the max length of the documents and also average length\n",
    "total_len = 0\n",
    "max_len = 0\n",
    "count = 0\n",
    "for document, label in train_data:\n",
    "    max_len = max(max_len, len(document))\n",
    "    total_len += len(document)\n",
    "    count += 1\n",
    "\n",
    "print(f'max doc length : {max_len}')\n",
    "print(f'average doc length : {total_len/count}')\n",
    "\n",
    "# pad the sentences to make them of same length\n",
    "def pad_doc(document, max_len):\n",
    "    if len(document) < max_len:\n",
    "        document += [vocab.index('<PAD>')] * (max_len - len(document))\n",
    "    return document\n",
    "\n",
    "train_data = [(pad_doc(document, max_len), label) for document, label in train_data]\n",
    "test_data = [(pad_doc(document, max_len), label) for document, label in test_data]\n",
    "# print(train_data[0])\n",
    "# print(len(train_data[0][0]))\n",
    "# print(test_data[0])\n",
    "\n",
    "# batchify the data after converting to tensors\n",
    "class Batchify(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
    "    \n",
    "train_data = Batchify(train_data)\n",
    "test_data = Batchify(test_data)\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transformer model for document coherence\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, max_len):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer = nn.Transformer(embedding_dim, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, 2)\n",
    "        self.max_len = max_len\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src)\n",
    "        src = src.transpose(0, 1)\n",
    "        src = self.transformer(src, src)\n",
    "        src = src.transpose(0, 1)\n",
    "        src = self.linear(src)\n",
    "        return src\n",
    "    \n",
    "model = TransformerModel(len(vocab), 128, 4, 2, 2, 256, 0.2, 100)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [32, 2], got [32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m             correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39meq(label\u001b[39m.\u001b[39mview_as(pred))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(test_loader), correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39mdataset)))\n\u001b[0;32m---> 33\u001b[0m train(model, train_loader, criterion, optimizer, \u001b[39m10\u001b[39;49m)\n\u001b[1;32m     34\u001b[0m test(model, test_loader, criterion)\n\u001b[1;32m     36\u001b[0m \u001b[39m# save the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m output \u001b[39m=\u001b[39m model(sentence)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m     14\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [32, 2], got [32]"
     ]
    }
   ],
   "source": [
    "# define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train the model\n",
    "def train(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (sentence, label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(sentence)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print('Epoch: {}, Loss: {}'.format(epoch, total_loss / len(train_loader)))\n",
    "\n",
    "# test the model\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (sentence, label) in enumerate(test_loader):\n",
    "            output = model(sentence)\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    print('Loss: {}, Accuracy: {}'.format(total_loss / len(test_loader), correct / len(test_loader.dataset)))\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, 10)\n",
    "test(model, test_loader, criterion)\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'transformer.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
