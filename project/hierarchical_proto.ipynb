{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "import torchtext\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n",
      "34\n",
      "10.12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGfCAYAAAB1KinVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdwElEQVR4nO3df5BV9X3/8dfya6EKi2DYZSvgRqloVJKqwY02TYURjTESmTakdkoSR9sEbJAmBtog1Zrgj9YQDJHGpmpmRBs7wdTYmGYwYjNBoliT2FqilhRS3XUay65iWKmc7x9+c2dWUNRcuJ+Vx2PmzHjPPXv27Scn43POvXtvU1VVVQAACjKo0QMAALycQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKM+T1/sB9992Xa665Jhs3bsxTTz2VNWvWZNasWbXnq6rK0qVLc8MNN2Tbtm055ZRTcv3112fy5Mm1Y5555plcdNFFufPOOzNo0KDMnj07X/jCF3LwwQe/phl27dqVJ598MiNHjkxTU9Pr/VcAABqgqqo8++yzaW9vz6BBe7lHUr1O//RP/1T9+Z//efX1r3+9SlKtWbOm3/NXXnll1dLSUt1xxx3VD3/4w+r9739/1dHRUf3iF7+oHXPGGWdUU6dOre6///7qX/7lX6ojjzyy+tCHPvSaZ9i6dWuVxGaz2Ww22wDctm7dutf/1jdV1Rv/ssCmpqZ+d1Cqqkp7e3v+9E//NJ/85CeTJD09PWltbc1NN92UOXPm5NFHH80xxxyTBx54ICeeeGKS5O6778573/ve/OxnP0t7e/tef29PT09Gjx6drVu3ZtSoUW90fABgP+rt7c2ECROybdu2tLS0vOqxr/slnlezefPmdHV1ZcaMGbV9LS0tmTZtWtavX585c+Zk/fr1GT16dC1OkmTGjBkZNGhQNmzYkA984AO7nbevry99fX21x88++2ySZNSoUQIFAAaY1/L2jLq+SbarqytJ0tra2m9/a2tr7bmurq6MGzeu3/NDhgzJmDFjase83LJly9LS0lLbJkyYUM+xAYDCDIi/4lm8eHF6enpq29atWxs9EgCwD9U1UNra2pIk3d3d/fZ3d3fXnmtra8vTTz/d7/n/+7//yzPPPFM75uWam5trL+d4WQcA3vzqGigdHR1pa2vL2rVra/t6e3uzYcOGdHZ2Jkk6Ozuzbdu2bNy4sXbMPffck127dmXatGn1HAcAGKBe95tkn3vuuTz++OO1x5s3b87DDz+cMWPGZOLEiVmwYEGuuOKKTJ48OR0dHVmyZEna29trf+lz9NFH54wzzsgFF1yQVatWZefOnZk/f37mzJnzmv6CBwB483vdgfLggw/md37nd2qPFy5cmCSZO3dubrrpplxyySXZvn17Lrzwwmzbti2nnnpq7r777gwfPrz2M7fcckvmz5+f6dOn1z6obcWKFXX41wEA3gx+pc9BaZTe3t60tLSkp6fH+1EAYIB4Pf/9HhB/xQMAHFgECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAU53V/UBscvuiufo9/euVZDZoEgDcrd1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDhDGj0AvJrDF93V7/FPrzyrQZMAsD+5gwIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAUR6AAAMURKABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRnSKMHYP85fNFd/R7/9MqzGjQJALw6d1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4Puqe/eLlH7Of+Kh9AF6ZOygAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUJy6B8qLL76YJUuWpKOjIyNGjMgRRxyRv/zLv0xVVbVjqqrKpZdemvHjx2fEiBGZMWNGHnvssXqPAgAMUHUPlKuuuirXX399vvjFL+bRRx/NVVddlauvvjrXXXdd7Zirr746K1asyKpVq7Jhw4YcdNBBmTlzZnbs2FHvcQCAAajuH9T2/e9/P+ecc07OOuulD+E6/PDDc+utt+YHP/hBkpfunixfvjyf+cxncs455yRJvvrVr6a1tTV33HFH5syZU++RAIABpu53UN71rndl7dq1+clPfpIk+eEPf5jvfe97OfPMM5MkmzdvTldXV2bMmFH7mZaWlkybNi3r16/f4zn7+vrS29vbbwMA3rzqfgdl0aJF6e3tzZQpUzJ48OC8+OKL+exnP5vzzjsvSdLV1ZUkaW1t7fdzra2ttedebtmyZbnsssvqPSpvUj5WH2Dgq/sdlK997Wu55ZZbsnr16jz00EO5+eab81d/9Ve5+eab3/A5Fy9enJ6entq2devWOk4MAJSm7ndQPvWpT2XRokW195Icd9xx+a//+q8sW7Ysc+fOTVtbW5Kku7s748ePr/1cd3d33v72t+/xnM3NzWlubq73qABAoep+B+X555/PoEH9Tzt48ODs2rUrSdLR0ZG2trasXbu29nxvb282bNiQzs7Oeo8DAAxAdb+DcvbZZ+ezn/1sJk6cmLe97W3513/911x77bX56Ec/miRpamrKggULcsUVV2Ty5Mnp6OjIkiVL0t7enlmzZtV7HABgAKp7oFx33XVZsmRJPv7xj+fpp59Oe3t7/uiP/iiXXnpp7ZhLLrkk27dvz4UXXpht27bl1FNPzd13353hw4fXexwAYACqe6CMHDkyy5cvz/Lly1/xmKamplx++eW5/PLL6/3rAYA3Ad/FAwAUR6AAAMURKABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAUR6AAAMURKABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAUR6AAAMURKABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRnSKMH4PU7fNFdu+376ZVnNWASANg33EEBAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKs08C5b//+7/zB3/wBxk7dmxGjBiR4447Lg8++GDt+aqqcumll2b8+PEZMWJEZsyYkccee2xfjAIADEB1D5T//d//zSmnnJKhQ4fmW9/6Vv793/89f/3Xf51DDjmkdszVV1+dFStWZNWqVdmwYUMOOuigzJw5Mzt27Kj3OADAADSk3ie86qqrMmHChNx44421fR0dHbV/rqoqy5cvz2c+85mcc845SZKvfvWraW1tzR133JE5c+bsds6+vr709fXVHvf29tZ7bACgIHW/g/KP//iPOfHEE/O7v/u7GTduXN7xjnfkhhtuqD2/efPmdHV1ZcaMGbV9LS0tmTZtWtavX7/Hcy5btiwtLS21bcKECfUeG3Zz+KK7dtsA2D/qHij/+Z//meuvvz6TJ0/Ot7/97XzsYx/Ln/zJn+Tmm29OknR1dSVJWltb+/1ca2tr7bmXW7x4cXp6emrb1q1b6z02AFCQur/Es2vXrpx44on53Oc+lyR5xzvekUceeSSrVq3K3Llz39A5m5ub09zcXM8xAYCC1f0Oyvjx43PMMcf023f00Udny5YtSZK2trYkSXd3d79juru7a88BAAe2ugfKKaeckk2bNvXb95Of/CSTJk1K8tIbZtva2rJ27dra8729vdmwYUM6OzvrPQ4AMADV/SWeiy++OO9617vyuc99Lr/3e7+XH/zgB/nyl7+cL3/5y0mSpqamLFiwIFdccUUmT56cjo6OLFmyJO3t7Zk1a1a9xwEABqC6B8pJJ52UNWvWZPHixbn88svT0dGR5cuX57zzzqsdc8kll2T79u258MILs23btpx66qm5++67M3z48HqPAwAMQHUPlCR53/vel/e9732v+HxTU1Muv/zyXH755fvi1wMAA5zv4gEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIMafQAQHL4ort22/fTK89qwCQAZXAHBQAojkABAIojUACA4uzzQLnyyivT1NSUBQsW1Pbt2LEj8+bNy9ixY3PwwQdn9uzZ6e7u3tejFOnwRXf12wCAfRwoDzzwQP7mb/4mxx9/fL/9F198ce68887cfvvtWbduXZ588smce+65+3IUAGAA2WeB8txzz+W8887LDTfckEMOOaS2v6enJ1/5yldy7bXX5rTTTssJJ5yQG2+8Md///vdz//3376txAIABZJ8Fyrx583LWWWdlxowZ/fZv3LgxO3fu7Ld/ypQpmThxYtavX7/Hc/X19aW3t7ffBgC8ee2Tz0G57bbb8tBDD+WBBx7Y7bmurq4MGzYso0eP7re/tbU1XV1dezzfsmXLctlll+2LUQGAAtX9DsrWrVvziU98IrfcckuGDx9el3MuXrw4PT09tW3r1q11OS8AUKa6B8rGjRvz9NNP5zd/8zczZMiQDBkyJOvWrcuKFSsyZMiQtLa25oUXXsi2bdv6/Vx3d3fa2tr2eM7m5uaMGjWq3wYAvHnV/SWe6dOn58c//nG/fR/5yEcyZcqUfPrTn86ECRMydOjQrF27NrNnz06SbNq0KVu2bElnZ2e9x4H97uV/Lu4j6wFev7oHysiRI3Psscf223fQQQdl7Nixtf3nn39+Fi5cmDFjxmTUqFG56KKL0tnZmZNPPrne4wAAA1BDvizw85//fAYNGpTZs2enr68vM2fOzJe+9KVGjAIAFGi/BMq9997b7/Hw4cOzcuXKrFy5cn/8egBggPFdPABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAUR6AAAMURKABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAUR6AAAMURKABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAUR6AAAMURKABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAUR6AAAMURKABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAUR6AAAMURKABAcQQKAFCcugfKsmXLctJJJ2XkyJEZN25cZs2alU2bNvU7ZseOHZk3b17Gjh2bgw8+OLNnz053d3e9RwEABqi6B8q6desyb9683H///fnOd76TnTt35vTTT8/27dtrx1x88cW58847c/vtt2fdunV58sknc+6559Z7FABggBpS7xPefffd/R7fdNNNGTduXDZu3Jh3v/vd6enpyVe+8pWsXr06p512WpLkxhtvzNFHH537778/J598cr1HAgAGmH3+HpSenp4kyZgxY5IkGzduzM6dOzNjxozaMVOmTMnEiROzfv36PZ6jr68vvb29/TYA4M1rnwbKrl27smDBgpxyyik59thjkyRdXV0ZNmxYRo8e3e/Y1tbWdHV17fE8y5YtS0tLS22bMGHCvhwbAGiwfRoo8+bNyyOPPJLbbrvtVzrP4sWL09PTU9u2bt1apwkBgBLV/T0ovzR//vx885vfzH333ZfDDjustr+trS0vvPBCtm3b1u8uSnd3d9ra2vZ4rubm5jQ3N++rUQGAwtT9DkpVVZk/f37WrFmTe+65Jx0dHf2eP+GEEzJ06NCsXbu2tm/Tpk3ZsmVLOjs76z0OADAA1f0Oyrx587J69ep84xvfyMiRI2vvK2lpacmIESPS0tKS888/PwsXLsyYMWMyatSoXHTRRens7PQXPHAAOHzRXbvt++mVZzVgEqBkdQ+U66+/Pknynve8p9/+G2+8MR/+8IeTJJ///OczaNCgzJ49O319fZk5c2a+9KUv1XsUAGCAqnugVFW112OGDx+elStXZuXKlfX+9QDAm4Dv4gEAiiNQAIDiCBQAoDgCBQAozj77oLYDjT+dpBFeft255oA3C3dQAIDiCBQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOD7qHnjDfNQ+sK+4gwIAFEegAADFESgAQHG8BwUOQN47ApTOHRQAoDgCBQAojkABAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDi+zRgYEHwDMxxY3EEBAIojUACA4ggUAKA4AgUAKI5AAQCKI1AAgOIIFACgOAIFACiOQAEAiiNQAIDiCBQAoDgCBQAojkABAIojUACA4gxp9AAA+8rhi+7abd9PrzyrAZMAr5c7KABAcQQKAFAcgQIAFMd7UPbg5a9be80a2F+8bwZe4g4KAFAcgQIAFMdLPAB74WUX2P/cQQEAiiNQAIDiCBQAoDjegwKwj+zPjywYiB+PMBBnZv9xBwUAKI5AAQCKI1AAgOI09D0oK1euzDXXXJOurq5MnTo11113Xd75znc2ciQAXiefEzPwlfi/YcPuoPz93/99Fi5cmKVLl+ahhx7K1KlTM3PmzDz99NONGgkAKETD7qBce+21ueCCC/KRj3wkSbJq1arcdddd+bu/+7ssWrSo37F9fX3p6+urPe7p6UmS9Pb27pPZdvU93+/xa/k9L/+ZN/pzpf2uep1nIKxPvc5T+vr8Kj9Xj/MMhHWt13lKX596GYgz01+9/r+yN788Z1VVez+4aoC+vr5q8ODB1Zo1a/rt/8M//MPq/e9//27HL126tEpis9lsNpvtTbBt3bp1r63QkDso//M//5MXX3wxra2t/fa3trbmP/7jP3Y7fvHixVm4cGHt8a5du/LMM89k7NixaWpq2ufz7mu9vb2ZMGFCtm7dmlGjRjV6nOJYn72zRntnjfbOGr0667N3e1ujqqry7LPPpr29fa/nGhAf1Nbc3Jzm5uZ++0aPHt2YYfahUaNGuehfhfXZO2u0d9Zo76zRq7M+e/dqa9TS0vKaztGQN8keeuihGTx4cLq7u/vt7+7uTltbWyNGAgAK0pBAGTZsWE444YSsXbu2tm/Xrl1Zu3ZtOjs7GzESAFCQhr3Es3DhwsydOzcnnnhi3vnOd2b58uXZvn177a96DiTNzc1ZunTpbi9j8RLrs3fWaO+s0d5Zo1dnffaunmvUVFWv5W999o0vfvGLtQ9qe/vb354VK1Zk2rRpjRoHAChEQwMFAGBPfBcPAFAcgQIAFEegAADFESgAQHEESoP8xV/8RZqamvptU6ZMafRYDXXffffl7LPPTnt7e5qamnLHHXf0e76qqlx66aUZP358RowYkRkzZuSxxx5rzLANsrc1+vCHP7zbdXXGGWc0ZtgGWLZsWU466aSMHDky48aNy6xZs7Jp06Z+x+zYsSPz5s3L2LFjc/DBB2f27Nm7fWjkm9lrWaP3vOc9u11Hf/zHf9ygife/66+/Pscff3zt01A7OzvzrW99q/b8gX4N7W196nX9CJQGetvb3pannnqqtn3ve99r9EgNtX379kydOjUrV67c4/NXX311VqxYkVWrVmXDhg056KCDMnPmzOzYsWM/T9o4e1ujJDnjjDP6XVe33nrrfpywsdatW5d58+bl/vvvz3e+853s3Lkzp59+erZv31475uKLL86dd96Z22+/PevWrcuTTz6Zc889t4FT71+vZY2S5IILLuh3HV199dUNmnj/O+yww3LllVdm48aNefDBB3PaaaflnHPOyb/9278lcQ3tbX2SOl0/v/p3E/NGLF26tJo6dWqjxyhWkn7fdr1r166qra2tuuaaa2r7tm3bVjU3N1e33nprAyZsvJevUVVV1dy5c6tzzjmnIfOU6Omnn66SVOvWrauq6qVrZujQodXtt99eO+bRRx+tklTr169v1JgN9fI1qqqq+u3f/u3qE5/4ROOGKtAhhxxS/e3f/q1r6BX8cn2qqn7XjzsoDfTYY4+lvb09b33rW3Peeedly5YtjR6pWJs3b05XV1dmzJhR29fS0pJp06Zl/fr1DZysPPfee2/GjRuXo446Kh/72Mfy85//vNEjNUxPT0+SZMyYMUmSjRs3ZufOnf2uoylTpmTixIkH7HX08jX6pVtuuSWHHnpojj322CxevDjPP/98I8ZruBdffDG33XZbtm/fns7OTtfQy7x8fX6pHtfPgPg24zejadOm5aabbspRRx2Vp556Kpdddll+67d+K4888khGjhzZ6PGK09XVlSRpbW3tt7+1tbX2HC+9vHPuueemo6MjTzzxRP7sz/4sZ555ZtavX5/Bgwc3erz9ateuXVmwYEFOOeWUHHvssUleuo6GDRu227ehH6jX0Z7WKEl+//d/P5MmTUp7e3t+9KMf5dOf/nQ2bdqUr3/96w2cdv/68Y9/nM7OzuzYsSMHH3xw1qxZk2OOOSYPP/ywayivvD5J/a4fgdIgZ555Zu2fjz/++EybNi2TJk3K1772tZx//vkNnIyBbM6cObV/Pu6443L88cfniCOOyL333pvp06c3cLL9b968eXnkkUcO+Pd2vZpXWqMLL7yw9s/HHXdcxo8fn+nTp+eJJ57IEUccsb/HbIijjjoqDz/8cHp6evIP//APmTt3btatW9fosYrxSutzzDHH1O368RJPIUaPHp3f+I3fyOOPP97oUYrU1taWJLu9U767u7v2HLt761vfmkMPPfSAu67mz5+fb37zm/nud7+bww47rLa/ra0tL7zwQrZt29bv+APxOnqlNdqTX35H2oF0HQ0bNixHHnlkTjjhhCxbtixTp07NF77wBdfQ//dK67Mnb/T6ESiFeO655/LEE09k/PjxjR6lSB0dHWlra8vatWtr+3p7e7Nhw4Z+r3vS389+9rP8/Oc/P2Cuq6qqMn/+/KxZsyb33HNPOjo6+j1/wgknZOjQof2uo02bNmXLli0HzHW0tzXak4cffjhJDpjraE927dqVvr4+19Ar+OX67MkbvX68xNMgn/zkJ3P22Wdn0qRJefLJJ7N06dIMHjw4H/rQhxo9WsM899xz/Qp78+bNefjhhzNmzJhMnDgxCxYsyBVXXJHJkyeno6MjS5YsSXt7e2bNmtW4ofezV1ujMWPG5LLLLsvs2bPT1taWJ554IpdcckmOPPLIzJw5s4FT7z/z5s3L6tWr841vfCMjR46svSegpaUlI0aMSEtLS84///wsXLgwY8aMyahRo3LRRRels7MzJ598coOn3z/2tkZPPPFEVq9enfe+970ZO3ZsfvSjH+Xiiy/Ou9/97hx//PENnn7/WLx4cc4888xMnDgxzz77bFavXp1777033/72t11DefX1qev18yv/HRBvyAc/+MFq/Pjx1bBhw6pf//Vfrz74wQ9Wjz/+eKPHaqjvfve7VZLdtrlz51ZV9dKfGi9ZsqRqbW2tmpubq+nTp1ebNm1q7ND72aut0fPPP1+dfvrp1Vve8pZq6NCh1aRJk6oLLrig6urqavTY+82e1iZJdeONN9aO+cUvflF9/OMfrw455JDq137t16oPfOAD1VNPPdW4ofezva3Rli1bqne/+93VmDFjqubm5urII4+sPvWpT1U9PT2NHXw/+uhHP1pNmjSpGjZsWPWWt7ylmj59evXP//zPtecP9Gvo1danntdPU1VV1a9aUwAA9eQ9KABAcQQKAFAcgQIAFEegAADFESgAQHEECgBQHIECABRHoAAAxREoAEBxBAoAUByBAgAU5/8BpS7FYB0m650AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the largest sentence length\n",
    "sent_max_len = 0\n",
    "max_sent_count = 0\n",
    "tot_sent_count = 0\n",
    "num_docs = 0\n",
    "sent_counts = []\n",
    "with open('./processed_data/GCDC/Clinton_train.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        num_docs += 1\n",
    "        sent_count = len(data['sentences'])\n",
    "        sent_counts.append(sent_count)\n",
    "        max_sent_count = max(max_sent_count, sent_count)\n",
    "        tot_sent_count += sent_count\n",
    "        for sentence in data['sentences']:\n",
    "            sent_max_len = max(sent_max_len, len(sentence))\n",
    "print(sent_max_len)\n",
    "print(max_sent_count)\n",
    "print(tot_sent_count/num_docs)\n",
    "\n",
    "# plot the sentence length distribution\n",
    "plt.hist(sent_counts, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231, 6251, 1012,  102, 2023, 2003, 2178, 3231,\n",
      "         6251, 1012,  102,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "ex_sent = 'This is a test sentence. [SEP] This is another test sentence.'\n",
    "tokenized_sent = tokenizer(ex_sent, padding='max_length', truncation=True, max_length=20, return_tensors='pt')\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "('Two options The US views the Transitional National Council as the sole / only legitimate interlocutor of the Libyan people during this interim period , as Libyans come together to plan their own future and a permanent , inclusive constitutional system that protects the rights of all Libyans . [SEP] This is in contrast to the Qadhafi regime , which has lost all legitimacy to rule . [SEP] The US views the Transitional National Council as the legitimate interlocutor of the Libyan people during this interim period , as Libyans come together to plan their own future and a permanent , inclusive constitutional system that protects the rights of all Libyans . [SEP] This is in contrast to the Qadhafi regime , which has lost all legitimacy to rule . [SEP] The INC is the institution through which we are engaging the Libyan people at this time . [SEP]', 2)\n",
      "(\"Ambassador , We just received an email from the Adoption Service Provider about these cases . [SEP] I am currently reviewing the files to determine if they qualify within the guidelines established recently by USCIS and DOS . [SEP] Out of the 60 cases , 40 are being adopted by USC . [SEP] Out of the 40 , 5 of the cases were children escorted today and per CNN have just landed in Miami . [SEP] The remaining 35 we are reviewing now . [SEP] Some correspondence I 've seen says that they plan on bringing the children straight to the airport , and we have responded that they first need to come to the embassy . [SEP] Once case in particular will be an issue , as it requires the Presidential Waiver . [SEP] Did you receive the talking points about Presidential Waivers ? [SEP] The majority of the cases will qualify for Humanitarian Parole . [SEP] In the last 30 minutes a number of emails have been coming in about private planes , Ted Turner offering to fly children out , etc . [SEP] We have a case coming in tomorrow morning in which the 2 adoptive families are being flown in by CBS helicopter from the DR . [SEP] We are preparing as many cases in advance as possible tonight . [SEP] Kind Regards , Linda [SEP]\", 0)\n",
      "(\"Hillary , As you have been so kind to share your advice and mentorship with me in the past , I was hoping that we might find some time to chat about some of my current thinking . [SEP] With the fabulous results of the Presidents ' re - election , I am privileged to be able to continue to serve at the CFTC . [SEP] I recall your prior advice upon my being offered the post [SEP] and it was invaluable . [SEP] The challenges and opportunities to bring about common sense financial reforms have been fabulous . [SEP] We are now moving beyond agency rule writing and helping the swaps markets transition to a new era of transparency and oversight . [SEP] ( The CFTC just completed final determinations such that the US has met the Pittsburgh G-20 commitment deadline of December 2012 to have our swaps clearing mandate fully in place . ) [SEP] If we might be able to find a moment to chat , I would love to share my thoughts on possible new challenges and opportunities within the Administration . [SEP] I hope all is well with you as well as you gear up for this holiday time and a much deserved break from the day to day stresses of your current post . [SEP] Gary [SEP]\", 1)\n",
      "200\n",
      "(\"Madame Secretary : Thank you for reaching out to Secretary Solis and convincing her to join the Verification Commission . [SEP] She and Ricardo Lagos will make for a very high profile and effective international component of the Commission . [SEP] We will meet with her this morning at 10 am to brief her for Tuesday 's journey to Honduras . [SEP] At this point , it appears we have a military aircraft available . [SEP] The plan is for the U.S. Delegation to depart D.C. , stop in Miami to pick up Ricardo Lagos , and arrive in Tegucigalpa together . [SEP] We think this will send a powerful message to Hondurans and leave no doubt about our commitment to seeing this process through to a successful conclusion . [SEP] You should be aware that Ambassador Hugo Llorens is under public assault . [SEP] The Wall Street Journal dedicates its America 's column this morning to attacking him and calling for his removal . [SEP] Last Friday , Representative Connie Mack did the same . [SEP] This chorus will grow as the extent of our accomplishment is understood . [SEP] Llorens is a tough , stalwart guy . [SEP] He and his Mission have held firm during this crisis . [SEP] A call from you would be a big boost . [SEP] Finally , we will hold an IPC today to identify further steps . [SEP] We will keep you up to date on these steps and identify further opportunities for your engagement . [SEP] I want to thank you for your leadership and support during this long crisis . [SEP] Your willingness to engage at key moments and take risks at the right time have propelled us much further than anyone expected . [SEP] Your diplomacy prevented a debilitating civil conflict in Honduras that would have destabilized Central America and undermined two decades of our efforts . [SEP] We now have a big opportunity in front of us , and for that we are grateful to you . [SEP] Regards , Tom [SEP]\", 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_inp_file = 'processed_data/GCDC/Clinton_train.jsonl'\n",
    "with open(train_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        # put [SEP] at the end of each sentence and add each sentence to the list\n",
    "        for sentence in json_obj['sentences']:\n",
    "            sentence.append('[SEP]')\n",
    "        # merge all sentences into one\n",
    "        document = \" \".join([word for sentence in json_obj['sentences'] for word in sentence])\n",
    "        document = document\n",
    "        label = json_obj['label']\n",
    "        train_data.append((document, label-1))\n",
    "\n",
    "test_data = []\n",
    "test_inp_file = 'processed_data/GCDC/Clinton_test.jsonl'\n",
    "with open(test_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        # put [SEP] at the end of each sentence and add each sentence to the list\n",
    "        for sentence in json_obj['sentences']:\n",
    "            sentence.append('[SEP]')\n",
    "        # merge all sentences into one\n",
    "        document = \" \".join([word for sentence in json_obj['sentences'] for word in sentence])\n",
    "        document = document\n",
    "        label = json_obj['label']\n",
    "        test_data.append((document, label-1))\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "print(train_data[1])\n",
    "print(train_data[2])\n",
    "print(len(test_data))\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 512])\n",
      "torch.Size([800, 512])\n",
      "torch.Size([800])\n",
      "torch.Size([200, 512])\n",
      "torch.Size([200, 512])\n",
      "torch.Size([200])\n",
      "tensor([  101,  2048,  7047,  1996,  2149,  5328,  1996, 17459,  2120,  2473,\n",
      "         2004,  1996,  7082,  1013,  2069, 11476,  6970,  4135, 12690,  2953,\n",
      "         1997,  1996, 19232,  2111,  2076,  2023,  9455,  2558,  1010,  2004,\n",
      "        19232,  2015,  2272,  2362,  2000,  2933,  2037,  2219,  2925,  1998,\n",
      "         1037,  4568,  1010, 18678,  6543,  2291,  2008, 18227,  1996,  2916,\n",
      "         1997,  2035, 19232,  2015,  1012,   102,  2023,  2003,  1999,  5688,\n",
      "         2000,  1996,  1053,  4215,  3270,  8873,  6939,  1010,  2029,  2038,\n",
      "         2439,  2035, 22568,  2000,  3627,  1012,   102,  1996,  2149,  5328,\n",
      "         1996, 17459,  2120,  2473,  2004,  1996, 11476,  6970,  4135, 12690,\n",
      "         2953,  1997,  1996, 19232,  2111,  2076,  2023,  9455,  2558,  1010,\n",
      "         2004, 19232,  2015,  2272,  2362,  2000,  2933,  2037,  2219,  2925,\n",
      "         1998,  1037,  4568,  1010, 18678,  6543,  2291,  2008, 18227,  1996,\n",
      "         2916,  1997,  2035, 19232,  2015,  1012,   102,  2023,  2003,  1999,\n",
      "         5688,  2000,  1996,  1053,  4215,  3270,  8873,  6939,  1010,  2029,\n",
      "         2038,  2439,  2035, 22568,  2000,  3627,  1012,   102,  1996,  4297,\n",
      "         2003,  1996,  5145,  2083,  2029,  2057,  2024, 11973,  1996, 19232,\n",
      "         2111,  2012,  2023,  2051,  1012,   102,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor(2)\n",
      "tensor([  101, 10602,  3187,  1024,  4067,  2017,  2005,  4285,  2041,  2000,\n",
      "         3187, 14017,  2483,  1998, 13359,  2014,  2000,  3693,  1996, 22616,\n",
      "         3222,  1012,   102,  2016,  1998, 13559, 16738,  2097,  2191,  2005,\n",
      "         1037,  2200,  2152,  6337,  1998,  4621,  2248,  6922,  1997,  1996,\n",
      "         3222,  1012,   102,  2057,  2097,  3113,  2007,  2014,  2023,  2851,\n",
      "         2012,  2184,  2572,  2000,  4766,  2014,  2005,  9857,  1005,  1055,\n",
      "         4990,  2000, 14373,  1012,   102,  2012,  2023,  2391,  1010,  2009,\n",
      "         3544,  2057,  2031,  1037,  2510,  2948,  2800,  1012,   102,  1996,\n",
      "         2933,  2003,  2005,  1996,  1057,  1012,  1055,  1012, 10656,  2000,\n",
      "        18280,  1040,  1012,  1039,  1012,  1010,  2644,  1999,  5631,  2000,\n",
      "         4060,  2039, 13559, 16738,  1010,  1998,  7180,  1999,  8915, 12193,\n",
      "         6895,  9692,  4502,  2362,  1012,   102,  2057,  2228,  2023,  2097,\n",
      "         4604,  1037,  3928,  4471,  2000, 10189, 24979,  6962,  1998,  2681,\n",
      "         2053,  4797,  2055,  2256,  8426,  2000,  3773,  2023,  2832,  2083,\n",
      "         2000,  1037,  3144,  7091,  1012,   102,  2017,  2323,  2022,  5204,\n",
      "         2008,  6059,  9395,  2222,  5686,  3619,  2003,  2104,  2270,  6101,\n",
      "         1012,   102,  1996,  2813,  2395,  3485,  2139, 16467,  2015,  2049,\n",
      "         2637,  1005,  1055,  5930,  2023,  2851,  2000,  7866,  2032,  1998,\n",
      "         4214,  2005,  2010,  8208,  1012,   102,  2197,  5958,  1010,  4387,\n",
      "        16560, 11349,  2106,  1996,  2168,  1012,   102,  2023,  7165,  2097,\n",
      "         4982,  2004,  1996,  6698,  1997,  2256, 24718,  2003,  5319,  1012,\n",
      "          102,  2222,  5686,  3619,  2003,  1037,  7823,  1010,  2358,  2389,\n",
      "        18367,  3124,  1012,   102,  2002,  1998,  2010,  3260,  2031,  2218,\n",
      "         3813,  2076,  2023,  5325,  1012,   102,  1037,  2655,  2013,  2017,\n",
      "         2052,  2022,  1037,  2502, 12992,  1012,   102,  2633,  1010,  2057,\n",
      "         2097,  2907,  2019, 12997,  2278,  2651,  2000,  6709,  2582,  4084,\n",
      "         1012,   102,  2057,  2097,  2562,  2017,  2039,  2000,  3058,  2006,\n",
      "         2122,  4084,  1998,  6709,  2582,  6695,  2005,  2115,  8147,  1012,\n",
      "          102,  1045,  2215,  2000,  4067,  2017,  2005,  2115,  4105,  1998,\n",
      "         2490,  2076,  2023,  2146,  5325,  1012,   102,  2115, 19732,  2000,\n",
      "         8526,  2012,  3145,  5312,  1998,  2202, 10831,  2012,  1996,  2157,\n",
      "         2051,  2031, 15801,  2149,  2172,  2582,  2084,  3087,  3517,  1012,\n",
      "          102,  2115, 17610,  8729,  1037,  2139, 14454, 16518,  2942,  4736,\n",
      "         1999, 14373,  2008,  2052,  2031,  4078,  2696, 14454,  3550,  2430,\n",
      "         2637,  1998, 25174,  2094,  2048,  5109,  1997,  2256,  4073,  1012,\n",
      "          102,  2057,  2085,  2031,  1037,  2502,  4495,  1999,  2392,  1997,\n",
      "         2149,  1010,  1998,  2005,  2008,  2057,  2024,  8794,  2000,  2017,\n",
      "         1012,   102, 12362,  1010,  3419,   102,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_tokenized = tokenizer([data[0] for data in train_data], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "test_tokenized = tokenizer([data[0] for data in test_data], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "train_labels = torch.tensor([data[1] for data in train_data])\n",
    "test_labels = torch.tensor([data[1] for data in test_data])\n",
    "\n",
    "print(train_tokenized['input_ids'].shape)\n",
    "print(train_tokenized['attention_mask'].shape)\n",
    "print(train_labels.shape)\n",
    "print(test_tokenized['input_ids'].shape)\n",
    "print(test_tokenized['attention_mask'].shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "print(train_tokenized['input_ids'][0])\n",
    "print(train_tokenized['attention_mask'][0])\n",
    "print(train_labels[0])\n",
    "print(test_tokenized['input_ids'][0])\n",
    "print(test_tokenized['attention_mask'][0])\n",
    "print(test_labels[0])\n",
    "\n",
    "train_dataset = TensorDataset(train_tokenized['input_ids'], train_tokenized['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_tokenized['input_ids'], test_tokenized['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nEvery document is a list of sentences, separated by [SEP].\\nThe hierarchical model is a two-level model. The first level\\nis a sentece encoder, and the second level is a document encoder.\\nThe sentence encoder takes individual sentences (separated by\\nSEP token) as input. The outputs from each position of the last layer\\nof the sentence encoder are pooled (separately for each sentence) using\\npooling strategies. The pooled outputs are then fed into the document\\nencoder. The document encoder takes the pooled outputs from the sentence\\nencoder as input. Each sentence in the document is encoded by the \\nsentence encoder and passed as input to the document encoder. Finally,\\nthe CLS token's transformed representation from the document encoder is \\nfed to a dense layer with ReLU activation which is then connected to a \\ntask-specific output layer.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Every document is a list of sentences, separated by [SEP].\n",
    "The hierarchical model is a two-level model. The first level\n",
    "is a sentece encoder, and the second level is a document encoder.\n",
    "The sentence encoder takes individual sentences (separated by\n",
    "SEP token) as input. The outputs from each position of the last layer\n",
    "of the sentence encoder are pooled (separately for each sentence) using\n",
    "pooling strategies. The pooled outputs are then fed into the document\n",
    "encoder. The document encoder takes the pooled outputs from the sentence\n",
    "encoder as input. Each sentence in the document is encoded by the \n",
    "sentence encoder and passed as input to the document encoder. Finally,\n",
    "the CLS token's transformed representation from the document encoder is \n",
    "fed to a dense layer with ReLU activation which is then connected to a \n",
    "task-specific output layer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sentence encoder\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, pooling_strategy):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):      \n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs[0]\n",
    "        if self.pooling_strategy == 'CLS':\n",
    "            pooled_output = last_hidden_state[:, 0, :]\n",
    "        elif self.pooling_strategy == 'mean':\n",
    "            pooled_output = torch.mean(last_hidden_state, dim=1)\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            pooled_output = torch.max(last_hidden_state, dim=1)[0]\n",
    "        else:\n",
    "            print('Invalid pooling strategy!')\n",
    "            return None\n",
    "        return pooled_output\n",
    "    \n",
    "# define the document encoder using bert\n",
    "class DocumentEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DocumentEncoder, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs[0]\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        return pooled_output\n",
    "    \n",
    "# define the hierarchical model\n",
    "class HierarchicalModel(nn.Module):\n",
    "    def __init__(self, sentence_encoder, document_encoder, num_classes):\n",
    "        super(HierarchicalModel, self).__init__()\n",
    "        self.sentence_encoder = sentence_encoder\n",
    "        self.document_encoder = document_encoder\n",
    "        self.num_classes = num_classes\n",
    "        self.dense = nn.Linear(768, 768)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(768, self.num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # shape of input_ids: (batch_size, max_doc_length)\n",
    "        # shape of attention_mask: (batch_size, max_doc_length)\n",
    "        # first, we must pass each sentence separately to the sentence encoder\n",
    "        # each sentence in the document is separated by [SEP]\n",
    "        # we must find the indices of [SEP] tokens in the input_ids and attention_mask of each document\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # # first, we must pass each sentence separately to the sentence encoder\n",
    "        # # each sentence in the document is separated by [SEP]\n",
    "        # # we must find the indices of [SEP] tokens in the input_ids\n",
    "        # sep_indices = torch.nonzero(input_ids == 102)\n",
    "        # # divide the input_ids and attention_mask into sentences\n",
    "        # # each sentence is a list of tokens\n",
    "        # input_ids = torch.split(input_ids, sep_indices.shape[0])\n",
    "        # attention_mask = torch.split(attention_mask, sep_indices.shape[0])\n",
    "        # # remove the [SEP] token from the end of each sentence\n",
    "        # input_ids = [sentence[:-1] for sentence in input_ids]\n",
    "        # attention_mask = [sentence[:-1] for sentence in attention_mask]\n",
    "        # # pad each sentence to the same length\n",
    "        # input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "        # attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        # # shape of input_ids: (batch_size, max_num_sentences, max_sentence_length)\n",
    "        # # pass each sentence to the sentence encoder\n",
    "        # sentence_embeddings = self.sentence_encoder(input_ids[:, 0, :], attention_mask[:, 0, :])\n",
    "        # for i in range(1, input_ids.shape[1]):\n",
    "        #     sentence_embeddings = torch.cat((sentence_embeddings, self.sentence_encoder(input_ids[:, i, :], attention_mask[:, i, :])), dim=1)\n",
    "        # # shape of sentence_embeddings: (batch_size, max_num_sentences, 768)\n",
    "        # # pass the sentence embeddings to the document encoder\n",
    "        # document_embeddings = self.document_encoder(sentence_embeddings, attention_mask[:, 0, :])\n",
    "        # # shape of document_embeddings: (batch_size, 768)\n",
    "        # # pass the document embeddings to the dense layer\n",
    "        # dense_output = self.dense(document_embeddings)\n",
    "        # dense_output = self.relu(dense_output)\n",
    "        # # pass the dense output to the output layer\n",
    "        # output = self.output(dense_output)\n",
    "        # return output\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "num_classes = 3\n",
    "\n",
    "# define the model\n",
    "sentence_encoder = SentenceEncoder(pooling_strategy='mean')\n",
    "document_encoder = DocumentEncoder()\n",
    "model = HierarchicalModel(sentence_encoder, document_encoder, num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54, 2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m     10\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m---> 11\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m     12\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        # print(input_ids.shape)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        # forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy: {}%'.format(correct / total * 100))\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'hierarchical_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
