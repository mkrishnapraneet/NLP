{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "import torchtext\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "from torchtext.models import RobertaClassificationHead, XLMR_BASE_ENCODER\n",
    "# from torchtext.functional import to_tensor\n",
    "\n",
    "batch_size = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "('two options the us views the transitional national council as the sole / only legitimate interlocutor of the libyan people during this interim period , as libyans come together to plan their own future and a permanent , inclusive constitutional system that protects the rights of all libyans . this is in contrast to the qadhafi regime , which has lost all legitimacy to rule . the us views the transitional national council as the legitimate interlocutor of the libyan people during this interim period , as libyans come together to plan their own future and a permanent , inclusive constitutional system that protects the rights of all libyans . this is in contrast to the qadhafi regime , which has lost all legitimacy to rule . the inc is the institution through which we are engaging the libyan people at this time .', 2)\n",
      "(\"ambassador , we just received an email from the adoption service provider about these cases . i am currently reviewing the files to determine if they qualify within the guidelines established recently by uscis and dos . out of the 60 cases , 40 are being adopted by usc . out of the 40 , 5 of the cases were children escorted today and per cnn have just landed in miami . the remaining 35 we are reviewing now . some correspondence i 've seen says that they plan on bringing the children straight to the airport , and we have responded that they first need to come to the embassy . once case in particular will be an issue , as it requires the presidential waiver . did you receive the talking points about presidential waivers ? the majority of the cases will qualify for humanitarian parole . in the last 30 minutes a number of emails have been coming in about private planes , ted turner offering to fly children out , etc . we have a case coming in tomorrow morning in which the 2 adoptive families are being flown in by cbs helicopter from the dr . we are preparing as many cases in advance as possible tonight . kind regards , linda\", 0)\n",
      "(\"hillary , as you have been so kind to share your advice and mentorship with me in the past , i was hoping that we might find some time to chat about some of my current thinking . with the fabulous results of the presidents ' re - election , i am privileged to be able to continue to serve at the cftc . i recall your prior advice upon my being offered the post and it was invaluable . the challenges and opportunities to bring about common sense financial reforms have been fabulous . we are now moving beyond agency rule writing and helping the swaps markets transition to a new era of transparency and oversight . ( the cftc just completed final determinations such that the us has met the pittsburgh g-20 commitment deadline of december 2012 to have our swaps clearing mandate fully in place . ) if we might be able to find a moment to chat , i would love to share my thoughts on possible new challenges and opportunities within the administration . i hope all is well with you as well as you gear up for this holiday time and a much deserved break from the day to day stresses of your current post . gary\", 1)\n",
      "200\n",
      "(\"madame secretary : thank you for reaching out to secretary solis and convincing her to join the verification commission . she and ricardo lagos will make for a very high profile and effective international component of the commission . we will meet with her this morning at 10 am to brief her for tuesday 's journey to honduras . at this point , it appears we have a military aircraft available . the plan is for the u.s. delegation to depart d.c. , stop in miami to pick up ricardo lagos , and arrive in tegucigalpa together . we think this will send a powerful message to hondurans and leave no doubt about our commitment to seeing this process through to a successful conclusion . you should be aware that ambassador hugo llorens is under public assault . the wall street journal dedicates its america 's column this morning to attacking him and calling for his removal . last friday , representative connie mack did the same . this chorus will grow as the extent of our accomplishment is understood . llorens is a tough , stalwart guy . he and his mission have held firm during this crisis . a call from you would be a big boost . finally , we will hold an ipc today to identify further steps . we will keep you up to date on these steps and identify further opportunities for your engagement . i want to thank you for your leadership and support during this long crisis . your willingness to engage at key moments and take risks at the right time have propelled us much further than anyone expected . your diplomacy prevented a debilitating civil conflict in honduras that would have destabilized central america and undermined two decades of our efforts . we now have a big opportunity in front of us , and for that we are grateful to you . regards , tom\", 2)\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "train_data = []\n",
    "train_inp_file = 'processed_data/GCDC/Clinton_train.jsonl'\n",
    "with open(train_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        # put <EOS> at the end of each sentence and add each sentence to the list\n",
    "        for i in range(len(json_obj['sentences'])):\n",
    "            # json_obj['sentences'][i].append('<EOS>')\n",
    "            # truncate each sentence to max_length\n",
    "            json_obj['sentences'][i] = json_obj['sentences'][i][:max_length]\n",
    "            # # add padding to each sentence\n",
    "            # json_obj['sentences'][i] = json_obj['sentences'][i] + ['[PAD]'] * (max_length - len(json_obj['sentences'][i]))\n",
    "\n",
    "        # merge all sentences into one\n",
    "        document = \" \".join([word for sentence in json_obj['sentences'] for word in sentence])\n",
    "        document = document.lower()\n",
    "        label = json_obj['label']\n",
    "        train_data.append((document, label-1))\n",
    "   \n",
    "\n",
    "test_data = []\n",
    "test_inp_file = 'processed_data/GCDC/Clinton_test.jsonl'\n",
    "with open(test_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        # put <EOS> at the end of each sentence and add each sentence to the list\n",
    "        for i in range(len(json_obj['sentences'])):\n",
    "            # json_obj['sentences'][i].append('<EOS>')\n",
    "            # truncate each sentence to max_length\n",
    "            json_obj['sentences'][i] = json_obj['sentences'][i][:max_length]\n",
    "            # # add padding to each sentence\n",
    "            # json_obj['sentences'][i] = json_obj['sentences'][i] + ['[PAD]'] * (max_length - len(json_obj['sentences'][i]))\n",
    "        # merge all sentences into one\n",
    "        document = \" \".join([word for sentence in json_obj['sentences'] for word in sentence])\n",
    "        document = document.lower()\n",
    "        label = json_obj['label']\n",
    "        test_data.append((document, label-1))\n",
    "        \n",
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "print(train_data[1])\n",
    "print(train_data[2])\n",
    "print(len(test_data))\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "275\n",
      "255\n",
      "([0, 6626, 50717, 70, 1821, 16912, 70, 149307, 289, 15889, 215394, 237, 70, 31247, 248, 4734, 148436, 67, 230415, 111, 70, 25474, 1643, 3395, 20271, 903, 1940, 464, 14922, 6, 4, 237, 25474, 1643, 7, 1380, 25842, 47, 1774, 2363, 10002, 22690, 136, 10, 28123, 6, 4, 55128, 171484, 289, 5426, 450, 59959, 7, 70, 38109, 111, 756, 25474, 1643, 7, 6, 5, 903, 83, 23, 69822, 47, 70, 44448, 35824, 14, 63647, 6, 4, 3129, 1556, 72856, 756, 148436, 2408, 47, 79986, 6, 5, 70, 1821, 16912, 70, 149307, 289, 15889, 215394, 237, 70, 148436, 67, 230415, 111, 70, 25474, 1643, 3395, 20271, 903, 1940, 464, 14922, 6, 4, 237, 25474, 1643, 7, 1380, 25842, 47, 1774, 2363, 10002, 22690, 136, 10, 28123, 6, 4, 55128, 171484, 289, 5426, 450, 59959, 7, 70, 38109, 111, 756, 25474, 1643, 7, 6, 5, 903, 83, 23, 69822, 47, 70, 44448, 35824, 14, 63647, 6, 4, 3129, 1556, 72856, 756, 148436, 2408, 47, 79986, 6, 5, 70, 23, 238, 83, 70, 38016, 8305, 3129, 642, 621, 44173, 9966, 70, 25474, 1643, 3395, 99, 903, 1733, 6, 5, 2], 2)\n",
      "([0, 72002, 7, 9601, 6, 4, 642, 1660, 75204, 142, 3340, 1295, 70, 6, 180064, 4516, 81450, 1672, 6097, 50218, 6, 5, 17, 444, 82424, 8347, 214, 70, 102158, 47, 83324, 2174, 1836, 14768, 18929, 28032, 70, 226829, 170920, 78684, 390, 1821, 10794, 136, 655, 6, 5, 1810, 111, 70, 1496, 50218, 6, 4, 1112, 621, 8035, 30666, 297, 390, 1821, 238, 6, 5, 1810, 111, 70, 1112, 6, 4, 190, 111, 70, 50218, 3542, 20020, 8453, 297, 18925, 136, 117, 501, 10713, 765, 1660, 3551, 297, 23, 324, 1191, 6, 5, 70, 47143, 214, 2273, 642, 621, 8347, 214, 5036, 6, 5, 3060, 42518, 6620, 17, 242, 272, 51592, 17378, 450, 1836, 1774, 98, 172915, 70, 20020, 80560, 47, 70, 146619, 6, 4, 136, 642, 765, 45739, 71, 450, 1836, 5117, 3871, 47, 1380, 47, 70, 352, 402, 46048, 6, 5, 24145, 7225, 23, 17311, 1221, 186, 142, 31089, 6, 4, 237, 442, 144570, 70, 79169, 289, 259, 14, 814, 6, 5, 6777, 398, 53299, 70, 56661, 26847, 1672, 79169, 289, 259, 14, 7864, 705, 70, 144732, 111, 70, 50218, 1221, 14768, 18929, 100, 75757, 3378, 30989, 6, 5, 23, 70, 4568, 496, 14633, 10, 14012, 111, 3340, 7, 765, 2809, 38162, 23, 1672, 14375, 143561, 6, 4, 6, 3674, 15504, 56, 126416, 47, 12403, 20020, 1810, 6, 4, 3021, 6, 5, 642, 765, 10, 7225, 38162, 23, 127773, 42141, 23, 3129, 70, 116, 30666, 5844, 87143, 621, 8035, 86608, 19, 23, 390, 501, 16145, 764, 9120, 78478, 1295, 70, 3568, 6, 5, 642, 621, 58172, 214, 237, 5941, 50218, 23, 129745, 237, 7722, 179028, 6, 5, 8562, 28601, 7, 6, 4, 55998, 2], 0)\n",
      "([0, 130473, 6635, 6, 4, 237, 398, 765, 2809, 221, 8562, 47, 12008, 935, 67660, 136, 52114, 16070, 678, 163, 23, 70, 11015, 6, 4, 17, 509, 156377, 450, 642, 13648, 7413, 3060, 1733, 47, 3245, 1672, 3060, 111, 759, 43581, 47644, 6, 5, 678, 70, 222522, 50339, 111, 70, 13918, 7, 242, 456, 20, 81843, 6, 4, 17, 444, 219475, 71, 47, 186, 19048, 47, 21342, 47, 21265, 99, 70, 501, 2480, 238, 6, 5, 17, 189232, 935, 41928, 67660, 54799, 759, 8035, 122399, 70, 1305, 136, 442, 509, 23, 27494, 2886, 6, 5, 70, 127125, 136, 107893, 47, 19095, 1672, 39210, 10422, 53477, 17690, 7, 765, 2809, 222522, 6, 5, 642, 621, 5036, 98567, 107314, 122921, 79986, 32562, 136, 120592, 70, 202317, 7, 16839, 7, 149307, 47, 10, 3525, 1615, 111, 3900, 16082, 27771, 136, 645, 7, 22553, 6, 5, 15, 70, 501, 2480, 238, 1660, 140528, 2704, 27354, 5256, 6044, 450, 70, 1821, 1556, 435, 70, 13380, 18, 47446, 127, 706, 11033, 166794, 157206, 111, 13104, 1324, 47, 765, 2446, 202317, 7, 34735, 214, 20777, 13, 89554, 23, 3687, 6, 5, 1388, 2174, 642, 13648, 186, 19048, 47, 7413, 10, 3095, 47, 3245, 6, 4, 17, 2806, 5161, 47, 12008, 759, 38514, 98, 7722, 3525, 127125, 136, 107893, 28032, 70, 86052, 6, 5, 17, 15673, 756, 83, 5299, 678, 398, 237, 5299, 237, 398, 72397, 1257, 100, 903, 65006, 1733, 136, 10, 5045, 150185, 71, 36356, 1295, 70, 5155, 47, 5155, 11405, 90, 111, 935, 43581, 1305, 6, 5, 3671, 53, 2], 1)\n",
      "([0, 25248, 282, 104463, 53, 152, 51544, 398, 100, 58359, 214, 1810, 47, 104463, 53, 20245, 7, 136, 158, 686, 60636, 604, 47, 33284, 70, 493, 41274, 62458, 6, 5, 2412, 136, 43281, 42, 246, 21, 8797, 1221, 3249, 100, 10, 4552, 11192, 60641, 136, 60266, 21640, 82761, 111, 70, 62458, 6, 5, 642, 1221, 23356, 678, 604, 903, 42141, 99, 209, 444, 47, 59335, 604, 100, 370, 90, 5636, 242, 7, 120696, 47, 63345, 85106, 6, 5, 99, 903, 6275, 6, 4, 442, 135179, 642, 765, 10, 116338, 1831, 56379, 19882, 6, 5, 70, 1774, 83, 100, 70, 75, 5, 7, 5, 33743, 1363, 47, 8, 17365, 104, 5, 238, 5, 6, 4, 7279, 23, 324, 1191, 47, 39580, 1257, 43281, 42, 246, 21, 8797, 6, 4, 136, 54410, 23, 47584, 318, 2870, 763, 25842, 6, 5, 642, 5351, 903, 1221, 25379, 10, 113138, 26008, 47, 63345, 14952, 7, 136, 31358, 110, 92814, 1672, 2446, 166794, 47, 86681, 903, 9433, 8305, 47, 10, 65771, 93192, 6, 5, 398, 5608, 186, 107419, 450, 72002, 7, 9601, 35875, 31, 96, 1484, 1755, 83, 1379, 3835, 10, 1192, 7136, 6, 5, 70, 58982, 48800, 68828, 64832, 90, 6863, 6, 152644, 242, 7, 3365, 316, 19, 903, 42141, 47, 52875, 214, 4049, 136, 159029, 100, 1919, 49146, 1405, 6, 5, 4568, 5129, 5636, 6, 4, 99638, 13, 158, 701, 291, 2594, 6777, 70, 5701, 6, 5, 903, 681, 6563, 1221, 55993, 237, 70, 192961, 111, 2446, 163846, 674, 83, 217064, 6, 5, 96, 1484, 1755, 83, 10, 143033, 6, 4, 20974, 35710, 48948, 6, 5, 764, 136, 1919, 29752, 765, 34658, 11037, 20271, 903, 52028, 6, 5, 10, 11782, 1295, 398, 2806, 186, 10, 6957, 97551, 6, 5, 77681, 6, 4, 642, 1221, 16401, 142, 17, 57095, 18925, 47, 135812, 53333, 98441, 6, 5, 642, 1221, 13695, 398, 1257, 47, 5622, 98, 6097, 98441, 136, 135812, 53333, 107893, 100, 935, 90364, 6, 5, 17, 3444, 47, 51544, 398, 100, 935, 133465, 136, 8060, 20271, 903, 4989, 52028, 6, 5, 935, 109269, 7432, 47, 81810, 99, 22799, 53189, 136, 5646, 10512, 7, 99, 70, 7108, 1733, 765, 25793, 2118, 71, 1821, 5045, 53333, 3501, 35672, 84751, 6, 5, 935, 37578, 2408, 56282, 297, 10, 8, 76562, 214, 9782, 79612, 23, 63345, 85106, 450, 2806, 765, 232929, 297, 9879, 6, 152644, 136, 1379, 2962, 71, 6626, 8, 23662, 7, 111, 2446, 79825, 6, 5, 642, 5036, 765, 10, 6957, 54591, 23, 12912, 111, 1821, 6, 4, 136, 100, 450, 642, 621, 225876, 47, 398, 6, 5, 28601, 7, 6, 4, 3627, 2], 2)\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "text_transform = XLMR_BASE_ENCODER.transform()\n",
    "\n",
    "def apply_transform(doc):\n",
    "    # also add padding to the end of the document\n",
    "    return text_transform(doc[0]), doc[1]\n",
    "\n",
    "train_dataset_unpadded = list(map(apply_transform, train_data))\n",
    "test_dataset_unpadded = list(map(apply_transform, test_data))\n",
    "\n",
    "# print(len(train_dataset_proto))\n",
    "# print(train_dataset_proto[0])\n",
    "\n",
    "# train_dataset = to_tensor(train_dataset_proto, padding_value=1)\n",
    "# test_dataset = to_tensor(test_dataset_proto, padding_value=1)\n",
    "\n",
    "print(len(train_dataset_unpadded[0][0]))\n",
    "print(len(train_dataset_unpadded[1][0]))\n",
    "print(len(train_dataset_unpadded[2][0]))\n",
    "print(train_dataset_unpadded[0])\n",
    "print(train_dataset_unpadded[1])\n",
    "print(train_dataset_unpadded[2])\n",
    "print(test_dataset_unpadded[0])\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# print(tokenizer.vocab_size)\n",
    "# print(bert.config.hidden_size)\n",
    "\n",
    "# max_pad_len = 512\n",
    "# pad_id = 0\n",
    "\n",
    "# class GCDCDataset(Dataset):\n",
    "#     def __init__(self, data, tokenizer):\n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         document, label = self.data[idx]\n",
    "#         tokenized_document = self.tokenizer.tokenize(document)\n",
    "#         indexed_document = self.tokenizer.convert_tokens_to_ids(tokenized_document)\n",
    "#         indexed_document = indexed_document[:max_pad_len]\n",
    "#         indexed_document = indexed_document + [pad_id] * (max_pad_len - len(indexed_document))\n",
    "#         return torch.tensor(indexed_document), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n",
      "512\n",
      "([0, 6626, 50717, 70, 1821, 16912, 70, 149307, 289, 15889, 215394, 237, 70, 31247, 248, 4734, 148436, 67, 230415, 111, 70, 25474, 1643, 3395, 20271, 903, 1940, 464, 14922, 6, 4, 237, 25474, 1643, 7, 1380, 25842, 47, 1774, 2363, 10002, 22690, 136, 10, 28123, 6, 4, 55128, 171484, 289, 5426, 450, 59959, 7, 70, 38109, 111, 756, 25474, 1643, 7, 6, 5, 903, 83, 23, 69822, 47, 70, 44448, 35824, 14, 63647, 6, 4, 3129, 1556, 72856, 756, 148436, 2408, 47, 79986, 6, 5, 70, 1821, 16912, 70, 149307, 289, 15889, 215394, 237, 70, 148436, 67, 230415, 111, 70, 25474, 1643, 3395, 20271, 903, 1940, 464, 14922, 6, 4, 237, 25474, 1643, 7, 1380, 25842, 47, 1774, 2363, 10002, 22690, 136, 10, 28123, 6, 4, 55128, 171484, 289, 5426, 450, 59959, 7, 70, 38109, 111, 756, 25474, 1643, 7, 6, 5, 903, 83, 23, 69822, 47, 70, 44448, 35824, 14, 63647, 6, 4, 3129, 1556, 72856, 756, 148436, 2408, 47, 79986, 6, 5, 70, 23, 238, 83, 70, 38016, 8305, 3129, 642, 621, 44173, 9966, 70, 25474, 1643, 3395, 99, 903, 1733, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 2)\n",
      "([0, 72002, 7, 9601, 6, 4, 642, 1660, 75204, 142, 3340, 1295, 70, 6, 180064, 4516, 81450, 1672, 6097, 50218, 6, 5, 17, 444, 82424, 8347, 214, 70, 102158, 47, 83324, 2174, 1836, 14768, 18929, 28032, 70, 226829, 170920, 78684, 390, 1821, 10794, 136, 655, 6, 5, 1810, 111, 70, 1496, 50218, 6, 4, 1112, 621, 8035, 30666, 297, 390, 1821, 238, 6, 5, 1810, 111, 70, 1112, 6, 4, 190, 111, 70, 50218, 3542, 20020, 8453, 297, 18925, 136, 117, 501, 10713, 765, 1660, 3551, 297, 23, 324, 1191, 6, 5, 70, 47143, 214, 2273, 642, 621, 8347, 214, 5036, 6, 5, 3060, 42518, 6620, 17, 242, 272, 51592, 17378, 450, 1836, 1774, 98, 172915, 70, 20020, 80560, 47, 70, 146619, 6, 4, 136, 642, 765, 45739, 71, 450, 1836, 5117, 3871, 47, 1380, 47, 70, 352, 402, 46048, 6, 5, 24145, 7225, 23, 17311, 1221, 186, 142, 31089, 6, 4, 237, 442, 144570, 70, 79169, 289, 259, 14, 814, 6, 5, 6777, 398, 53299, 70, 56661, 26847, 1672, 79169, 289, 259, 14, 7864, 705, 70, 144732, 111, 70, 50218, 1221, 14768, 18929, 100, 75757, 3378, 30989, 6, 5, 23, 70, 4568, 496, 14633, 10, 14012, 111, 3340, 7, 765, 2809, 38162, 23, 1672, 14375, 143561, 6, 4, 6, 3674, 15504, 56, 126416, 47, 12403, 20020, 1810, 6, 4, 3021, 6, 5, 642, 765, 10, 7225, 38162, 23, 127773, 42141, 23, 3129, 70, 116, 30666, 5844, 87143, 621, 8035, 86608, 19, 23, 390, 501, 16145, 764, 9120, 78478, 1295, 70, 3568, 6, 5, 642, 621, 58172, 214, 237, 5941, 50218, 23, 129745, 237, 7722, 179028, 6, 5, 8562, 28601, 7, 6, 4, 55998, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 0)\n",
      "([0, 130473, 6635, 6, 4, 237, 398, 765, 2809, 221, 8562, 47, 12008, 935, 67660, 136, 52114, 16070, 678, 163, 23, 70, 11015, 6, 4, 17, 509, 156377, 450, 642, 13648, 7413, 3060, 1733, 47, 3245, 1672, 3060, 111, 759, 43581, 47644, 6, 5, 678, 70, 222522, 50339, 111, 70, 13918, 7, 242, 456, 20, 81843, 6, 4, 17, 444, 219475, 71, 47, 186, 19048, 47, 21342, 47, 21265, 99, 70, 501, 2480, 238, 6, 5, 17, 189232, 935, 41928, 67660, 54799, 759, 8035, 122399, 70, 1305, 136, 442, 509, 23, 27494, 2886, 6, 5, 70, 127125, 136, 107893, 47, 19095, 1672, 39210, 10422, 53477, 17690, 7, 765, 2809, 222522, 6, 5, 642, 621, 5036, 98567, 107314, 122921, 79986, 32562, 136, 120592, 70, 202317, 7, 16839, 7, 149307, 47, 10, 3525, 1615, 111, 3900, 16082, 27771, 136, 645, 7, 22553, 6, 5, 15, 70, 501, 2480, 238, 1660, 140528, 2704, 27354, 5256, 6044, 450, 70, 1821, 1556, 435, 70, 13380, 18, 47446, 127, 706, 11033, 166794, 157206, 111, 13104, 1324, 47, 765, 2446, 202317, 7, 34735, 214, 20777, 13, 89554, 23, 3687, 6, 5, 1388, 2174, 642, 13648, 186, 19048, 47, 7413, 10, 3095, 47, 3245, 6, 4, 17, 2806, 5161, 47, 12008, 759, 38514, 98, 7722, 3525, 127125, 136, 107893, 28032, 70, 86052, 6, 5, 17, 15673, 756, 83, 5299, 678, 398, 237, 5299, 237, 398, 72397, 1257, 100, 903, 65006, 1733, 136, 10, 5045, 150185, 71, 36356, 1295, 70, 5155, 47, 5155, 11405, 90, 111, 935, 43581, 1305, 6, 5, 3671, 53, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 1)\n",
      "([0, 25248, 282, 104463, 53, 152, 51544, 398, 100, 58359, 214, 1810, 47, 104463, 53, 20245, 7, 136, 158, 686, 60636, 604, 47, 33284, 70, 493, 41274, 62458, 6, 5, 2412, 136, 43281, 42, 246, 21, 8797, 1221, 3249, 100, 10, 4552, 11192, 60641, 136, 60266, 21640, 82761, 111, 70, 62458, 6, 5, 642, 1221, 23356, 678, 604, 903, 42141, 99, 209, 444, 47, 59335, 604, 100, 370, 90, 5636, 242, 7, 120696, 47, 63345, 85106, 6, 5, 99, 903, 6275, 6, 4, 442, 135179, 642, 765, 10, 116338, 1831, 56379, 19882, 6, 5, 70, 1774, 83, 100, 70, 75, 5, 7, 5, 33743, 1363, 47, 8, 17365, 104, 5, 238, 5, 6, 4, 7279, 23, 324, 1191, 47, 39580, 1257, 43281, 42, 246, 21, 8797, 6, 4, 136, 54410, 23, 47584, 318, 2870, 763, 25842, 6, 5, 642, 5351, 903, 1221, 25379, 10, 113138, 26008, 47, 63345, 14952, 7, 136, 31358, 110, 92814, 1672, 2446, 166794, 47, 86681, 903, 9433, 8305, 47, 10, 65771, 93192, 6, 5, 398, 5608, 186, 107419, 450, 72002, 7, 9601, 35875, 31, 96, 1484, 1755, 83, 1379, 3835, 10, 1192, 7136, 6, 5, 70, 58982, 48800, 68828, 64832, 90, 6863, 6, 152644, 242, 7, 3365, 316, 19, 903, 42141, 47, 52875, 214, 4049, 136, 159029, 100, 1919, 49146, 1405, 6, 5, 4568, 5129, 5636, 6, 4, 99638, 13, 158, 701, 291, 2594, 6777, 70, 5701, 6, 5, 903, 681, 6563, 1221, 55993, 237, 70, 192961, 111, 2446, 163846, 674, 83, 217064, 6, 5, 96, 1484, 1755, 83, 10, 143033, 6, 4, 20974, 35710, 48948, 6, 5, 764, 136, 1919, 29752, 765, 34658, 11037, 20271, 903, 52028, 6, 5, 10, 11782, 1295, 398, 2806, 186, 10, 6957, 97551, 6, 5, 77681, 6, 4, 642, 1221, 16401, 142, 17, 57095, 18925, 47, 135812, 53333, 98441, 6, 5, 642, 1221, 13695, 398, 1257, 47, 5622, 98, 6097, 98441, 136, 135812, 53333, 107893, 100, 935, 90364, 6, 5, 17, 3444, 47, 51544, 398, 100, 935, 133465, 136, 8060, 20271, 903, 4989, 52028, 6, 5, 935, 109269, 7432, 47, 81810, 99, 22799, 53189, 136, 5646, 10512, 7, 99, 70, 7108, 1733, 765, 25793, 2118, 71, 1821, 5045, 53333, 3501, 35672, 84751, 6, 5, 935, 37578, 2408, 56282, 297, 10, 8, 76562, 214, 9782, 79612, 23, 63345, 85106, 450, 2806, 765, 232929, 297, 9879, 6, 152644, 136, 1379, 2962, 71, 6626, 8, 23662, 7, 111, 2446, 79825, 6, 5, 642, 5036, 765, 10, 6957, 54591, 23, 12912, 111, 1821, 6, 4, 136, 100, 450, 642, 621, 225876, 47, 398, 6, 5, 28601, 7, 6, 4, 3627, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 2)\n"
     ]
    }
   ],
   "source": [
    "max_pad_len = 512\n",
    "pad_id = 1\n",
    "\n",
    "# pad the document to max_pad_len\n",
    "def pad_document(document):\n",
    "    return document + [pad_id] * (max_pad_len - len(document))\n",
    "\n",
    "train_dataset = list(map(lambda x: (pad_document(x[0]), x[1]), train_dataset_unpadded))\n",
    "test_dataset = list(map(lambda x: (pad_document(x[0]), x[1]), test_dataset_unpadded))\n",
    "\n",
    "print(len(train_dataset[0][0]))\n",
    "print(len(train_dataset[1][0]))\n",
    "print(len(train_dataset[2][0]))\n",
    "print(train_dataset[0])\n",
    "print(train_dataset[1])\n",
    "print(train_dataset[2])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n",
      "(tensor([     0,   6626,  50717,     70,   1821,  16912,     70, 149307,    289,\n",
      "         15889, 215394,    237,     70,  31247,    248,   4734, 148436,     67,\n",
      "        230415,    111,     70,  25474,   1643,   3395,  20271,    903,   1940,\n",
      "           464,  14922,      6,      4,    237,  25474,   1643,      7,   1380,\n",
      "         25842,     47,   1774,   2363,  10002,  22690,    136,     10,  28123,\n",
      "             6,      4,  55128, 171484,    289,   5426,    450,  59959,      7,\n",
      "            70,  38109,    111,    756,  25474,   1643,      7,      6,      5,\n",
      "           903,     83,     23,  69822,     47,     70,  44448,  35824,     14,\n",
      "         63647,      6,      4,   3129,   1556,  72856,    756, 148436,   2408,\n",
      "            47,  79986,      6,      5,     70,   1821,  16912,     70, 149307,\n",
      "           289,  15889, 215394,    237,     70, 148436,     67, 230415,    111,\n",
      "            70,  25474,   1643,   3395,  20271,    903,   1940,    464,  14922,\n",
      "             6,      4,    237,  25474,   1643,      7,   1380,  25842,     47,\n",
      "          1774,   2363,  10002,  22690,    136,     10,  28123,      6,      4,\n",
      "         55128, 171484,    289,   5426,    450,  59959,      7,     70,  38109,\n",
      "           111,    756,  25474,   1643,      7,      6,      5,    903,     83,\n",
      "            23,  69822,     47,     70,  44448,  35824,     14,  63647,      6,\n",
      "             4,   3129,   1556,  72856,    756, 148436,   2408,     47,  79986,\n",
      "             6,      5,     70,     23,    238,     83,     70,  38016,   8305,\n",
      "          3129,    642,    621,  44173,   9966,     70,  25474,   1643,   3395,\n",
      "            99,    903,   1733,      6,      5,      2,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1]), tensor(2))\n",
      "(tensor([     0,  25248,    282, 104463,     53,    152,  51544,    398,    100,\n",
      "         58359,    214,   1810,     47, 104463,     53,  20245,      7,    136,\n",
      "           158,    686,  60636,    604,     47,  33284,     70,    493,  41274,\n",
      "         62458,      6,      5,   2412,    136,  43281,     42,    246,     21,\n",
      "          8797,   1221,   3249,    100,     10,   4552,  11192,  60641,    136,\n",
      "         60266,  21640,  82761,    111,     70,  62458,      6,      5,    642,\n",
      "          1221,  23356,    678,    604,    903,  42141,     99,    209,    444,\n",
      "            47,  59335,    604,    100,    370,     90,   5636,    242,      7,\n",
      "        120696,     47,  63345,  85106,      6,      5,     99,    903,   6275,\n",
      "             6,      4,    442, 135179,    642,    765,     10, 116338,   1831,\n",
      "         56379,  19882,      6,      5,     70,   1774,     83,    100,     70,\n",
      "            75,      5,      7,      5,  33743,   1363,     47,      8,  17365,\n",
      "           104,      5,    238,      5,      6,      4,   7279,     23,    324,\n",
      "          1191,     47,  39580,   1257,  43281,     42,    246,     21,   8797,\n",
      "             6,      4,    136,  54410,     23,  47584,    318,   2870,    763,\n",
      "         25842,      6,      5,    642,   5351,    903,   1221,  25379,     10,\n",
      "        113138,  26008,     47,  63345,  14952,      7,    136,  31358,    110,\n",
      "         92814,   1672,   2446, 166794,     47,  86681,    903,   9433,   8305,\n",
      "            47,     10,  65771,  93192,      6,      5,    398,   5608,    186,\n",
      "        107419,    450,  72002,      7,   9601,  35875,     31,     96,   1484,\n",
      "          1755,     83,   1379,   3835,     10,   1192,   7136,      6,      5,\n",
      "            70,  58982,  48800,  68828,  64832,     90,   6863,      6, 152644,\n",
      "           242,      7,   3365,    316,     19,    903,  42141,     47,  52875,\n",
      "           214,   4049,    136, 159029,    100,   1919,  49146,   1405,      6,\n",
      "             5,   4568,   5129,   5636,      6,      4,  99638,     13,    158,\n",
      "           701,    291,   2594,   6777,     70,   5701,      6,      5,    903,\n",
      "           681,   6563,   1221,  55993,    237,     70, 192961,    111,   2446,\n",
      "        163846,    674,     83, 217064,      6,      5,     96,   1484,   1755,\n",
      "            83,     10, 143033,      6,      4,  20974,  35710,  48948,      6,\n",
      "             5,    764,    136,   1919,  29752,    765,  34658,  11037,  20271,\n",
      "           903,  52028,      6,      5,     10,  11782,   1295,    398,   2806,\n",
      "           186,     10,   6957,  97551,      6,      5,  77681,      6,      4,\n",
      "           642,   1221,  16401,    142,     17,  57095,  18925,     47, 135812,\n",
      "         53333,  98441,      6,      5,    642,   1221,  13695,    398,   1257,\n",
      "            47,   5622,     98,   6097,  98441,    136, 135812,  53333, 107893,\n",
      "           100,    935,  90364,      6,      5,     17,   3444,     47,  51544,\n",
      "           398,    100,    935, 133465,    136,   8060,  20271,    903,   4989,\n",
      "         52028,      6,      5,    935, 109269,   7432,     47,  81810,     99,\n",
      "         22799,  53189,    136,   5646,  10512,      7,     99,     70,   7108,\n",
      "          1733,    765,  25793,   2118,     71,   1821,   5045,  53333,   3501,\n",
      "         35672,  84751,      6,      5,    935,  37578,   2408,  56282,    297,\n",
      "            10,      8,  76562,    214,   9782,  79612,     23,  63345,  85106,\n",
      "           450,   2806,    765, 232929,    297,   9879,      6, 152644,    136,\n",
      "          1379,   2962,     71,   6626,      8,  23662,      7,    111,   2446,\n",
      "         79825,      6,      5,    642,   5036,    765,     10,   6957,  54591,\n",
      "            23,  12912,    111,   1821,      6,      4,    136,    100,    450,\n",
      "           642,    621, 225876,     47,    398,      6,      5,  28601,      7,\n",
      "             6,      4,   3627,      2,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1]), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "# convert the dataset to tensors\n",
    "train_dataset = list(map(lambda x: (torch.tensor(x[0]), torch.tensor(x[1])), train_dataset))\n",
    "test_dataset = list(map(lambda x: (torch.tensor(x[0]), torch.tensor(x[1])), test_dataset))\n",
    "# train_dataset = list(map(lambda x: (torch.tensor(x[0]), torch.tensor(x[1])), train_dataset_unpadded))\n",
    "# test_dataset = list(map(lambda x: (torch.tensor(x[0]), torch.tensor(x[1])), test_dataset_unpadded))\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "input_dim = 768\n",
    "\n",
    "classifier_head = RobertaClassificationHead(num_classes=num_classes, input_dim=input_dim)\n",
    "model = XLMR_BASE_ENCODER.get_model(head=classifier_head)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "learning_rate = 0.001\n",
    "optim = AdamW(model.parameters(), lr=learning_rate)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_loader, optim, criteria):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = criteria(y_pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (torch.argmax(y_pred, dim=1) == y).sum().item()\n",
    "        total += len(y)\n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "def test(model, test_loader, criteria):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criteria(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (torch.argmax(y_pred, dim=1) == y).sum().item()\n",
    "            total += len(y)\n",
    "    return total_loss / total, total_correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 5.80 GiB total capacity; 4.16 GiB already allocated; 527.44 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 3\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_loader, optim, criteria)\n\u001b[1;32m      4\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test(model, test_loader, criteria)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Train Acc: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Test Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Test Acc: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, train_loss, train_acc, test_loss, test_acc))\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optim, criteria)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m criteria(y_pred, y)\n\u001b[1;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 21\u001b[0m optim\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     22\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m total_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39margmax(y_pred, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    163\u001b[0m           grads,\n\u001b[1;32m    164\u001b[0m           exp_avgs,\n\u001b[1;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    167\u001b[0m           state_steps,\n\u001b[1;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m func(params,\n\u001b[1;32m    220\u001b[0m      grads,\n\u001b[1;32m    221\u001b[0m      exp_avgs,\n\u001b[1;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    224\u001b[0m      state_steps,\n\u001b[1;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:316\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    314\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    318\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 5.80 GiB total capacity; 4.16 GiB already allocated; 527.44 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optim, criteria)\n",
    "    test_loss, test_acc = test(model, test_loader, criteria)\n",
    "    print('Epoch: {}, Train Loss: {:.4f}, Train Acc: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.4f}'.format(epoch, train_loss, train_acc, test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
