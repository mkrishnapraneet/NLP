{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "import torchtext\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "(['t', 'w', 'o', 'o', 'p', 't', 'i', 'o', 'n', 's', 't', 'h', 'e', 'u', 's', 'v', 'i', 'e', 'w', 's', 't', 'h', 'e', 't', 'r', 'a', 'n', 's', 'i', 't', 'i', 'o', 'n', 'a', 'l', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'c', 'o', 'u', 'n', 'c', 'i', 'l', 'a', 's', 't', 'h', 'e', 's', 'o', 'l', 'e', '/', 'o', 'n', 'l', 'y', 'l', 'e', 'g', 'i', 't', 'i', 'm', 'a', 't', 'e', 'i', 'n', 't', 'e', 'r', 'l', 'o', 'c', 'u', 't', 'o', 'r', 'o', 'f', 't', 'h', 'e', 'l', 'i', 'b', 'y', 'a', 'n', 'p', 'e', 'o', 'p', 'l', 'e', 'd', 'u', 'r', 'i', 'n', 'g', 't', 'h', 'i', 's', 'i', 'n', 't', 'e', 'r', 'i', 'm', 'p', 'e', 'r', 'i', 'o', 'd', ',', 'a', 's', 'l', 'i', 'b', 'y', 'a', 'n', 's', 'c', 'o', 'm', 'e', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', 't', 'o', 'p', 'l', 'a', 'n', 't', 'h', 'e', 'i', 'r', 'o', 'w', 'n', 'f', 'u', 't', 'u', 'r', 'e', 'a', 'n', 'd', 'a', 'p', 'e', 'r', 'm', 'a', 'n', 'e', 'n', 't', ',', 'i', 'n', 'c', 'l', 'u', 's', 'i', 'v', 'e', 'c', 'o', 'n', 's', 't', 'i', 't', 'u', 't', 'i', 'o', 'n', 'a', 'l', 's', 'y', 's', 't', 'e', 'm', 't', 'h', 'a', 't', 'p', 'r', 'o', 't', 'e', 'c', 't', 's', 't', 'h', 'e', 'r', 'i', 'g', 'h', 't', 's', 'o', 'f', 'a', 'l', 'l', 'l', 'i', 'b', 'y', 'a', 'n', 's', '.', '<', 'E', 'O', 'S', '>', 't', 'h', 'i', 's', 'i', 's', 'i', 'n', 'c', 'o', 'n', 't', 'r', 'a', 's', 't', 't', 'o', 't', 'h', 'e', 'q', 'a', 'd', 'h', 'a', 'f', 'i', 'r', 'e', 'g', 'i', 'm', 'e', ',', 'w', 'h', 'i', 'c', 'h', 'h', 'a', 's', 'l', 'o', 's', 't', 'a', 'l', 'l', 'l', 'e', 'g', 'i', 't', 'i', 'm', 'a', 'c', 'y', 't', 'o', 'r', 'u', 'l', 'e', '.', '<', 'E', 'O', 'S', '>', 't', 'h', 'e', 'u', 's', 'v', 'i', 'e', 'w', 's', 't', 'h', 'e', 't', 'r', 'a', 'n', 's', 'i', 't', 'i', 'o', 'n', 'a', 'l', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'c', 'o', 'u', 'n', 'c', 'i', 'l', 'a', 's', 't', 'h', 'e', 'l', 'e', 'g', 'i', 't', 'i', 'm', 'a', 't', 'e', 'i', 'n', 't', 'e', 'r', 'l', 'o', 'c', 'u', 't', 'o', 'r', 'o', 'f', 't', 'h', 'e', 'l', 'i', 'b', 'y', 'a', 'n', 'p', 'e', 'o', 'p', 'l', 'e', 'd', 'u', 'r', 'i', 'n', 'g', 't', 'h', 'i', 's', 'i', 'n', 't', 'e', 'r', 'i', 'm', 'p', 'e', 'r', 'i', 'o', 'd', ',', 'a', 's', 'l', 'i', 'b', 'y', 'a', 'n', 's', 'c', 'o', 'm', 'e', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', 't', 'o', 'p', 'l', 'a', 'n', 't', 'h', 'e', 'i', 'r', 'o', 'w', 'n', 'f', 'u', 't', 'u', 'r', 'e', 'a', 'n', 'd', 'a', 'p', 'e', 'r', 'm', 'a', 'n', 'e', 'n', 't', ',', 'i', 'n', 'c', 'l', 'u', 's', 'i', 'v', 'e', 'c', 'o', 'n', 's', 't', 'i', 't', 'u', 't', 'i', 'o', 'n', 'a', 'l', 's', 'y', 's', 't', 'e', 'm', 't', 'h', 'a', 't', 'p', 'r', 'o', 't', 'e', 'c', 't', 's', 't', 'h', 'e', 'r', 'i', 'g', 'h', 't', 's', 'o', 'f', 'a', 'l', 'l', 'l', 'i', 'b', 'y', 'a', 'n', 's', '.', '<', 'E', 'O', 'S', '>', 't', 'h', 'i', 's', 'i', 's', 'i', 'n', 'c', 'o', 'n', 't', 'r', 'a', 's', 't', 't', 'o', 't', 'h', 'e', 'q', 'a', 'd', 'h', 'a', 'f', 'i', 'r', 'e', 'g', 'i', 'm', 'e', ',', 'w', 'h', 'i', 'c', 'h', 'h', 'a', 's', 'l', 'o', 's', 't', 'a', 'l', 'l', 'l', 'e', 'g', 'i', 't', 'i', 'm', 'a', 'c', 'y', 't', 'o', 'r', 'u', 'l', 'e', '.', '<', 'E', 'O', 'S', '>', 't', 'h', 'e', 'i', 'n', 'c', 'i', 's', 't', 'h', 'e', 'i', 'n', 's', 't', 'i', 't', 'u', 't', 'i', 'o', 'n', 't', 'h', 'r', 'o', 'u', 'g', 'h', 'w', 'h', 'i', 'c', 'h', 'w', 'e', 'a', 'r', 'e', 'e', 'n', 'g', 'a', 'g', 'i', 'n', 'g', 't', 'h', 'e', 'l', 'i', 'b', 'y', 'a', 'n', 'p', 'e', 'o', 'p', 'l', 'e', 'a', 't', 't', 'h', 'i', 's', 't', 'i', 'm', 'e', '.', '<', 'E', 'O', 'S', '>'], 3)\n",
      "(['a', 'm', 'b', 'a', 's', 's', 'a', 'd', 'o', 'r', ',', 'w', 'e', 'j', 'u', 's', 't', 'r', 'e', 'c', 'e', 'i', 'v', 'e', 'd', 'a', 'n', 'e', 'm', 'a', 'i', 'l', 'f', 'r', 'o', 'm', 't', 'h', 'e', 'a', 'd', 'o', 'p', 't', 'i', 'o', 'n', 's', 'e', 'r', 'v', 'i', 'c', 'e', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 'r', 'a', 'b', 'o', 'u', 't', 't', 'h', 'e', 's', 'e', 'c', 'a', 's', 'e', 's', '.', '<', 'E', 'O', 'S', '>', 'i', 'a', 'm', 'c', 'u', 'r', 'r', 'e', 'n', 't', 'l', 'y', 'r', 'e', 'v', 'i', 'e', 'w', 'i', 'n', 'g', 't', 'h', 'e', 'f', 'i', 'l', 'e', 's', 't', 'o', 'd', 'e', 't', 'e', 'r', 'm', 'i', 'n', 'e', 'i', 'f', 't', 'h', 'e', 'y', 'q', 'u', 'a', 'l', 'i', 'f', 'y', 'w', 'i', 't', 'h', 'i', 'n', 't', 'h', 'e', 'g', 'u', 'i', 'd', 'e', 'l', 'i', 'n', 'e', 's', 'e', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'e', 'd', 'r', 'e', 'c', 'e', 'n', 't', 'l', 'y', 'b', 'y', 'u', 's', 'c', 'i', 's', 'a', 'n', 'd', 'd', 'o', 's', '.', '<', 'E', 'O', 'S', '>', 'o', 'u', 't', 'o', 'f', 't', 'h', 'e', '6', '0', 'c', 'a', 's', 'e', 's', ',', '4', '0', 'a', 'r', 'e', 'b', 'e', 'i', 'n', 'g', 'a', 'd', 'o', 'p', 't', 'e', 'd', 'b', 'y', 'u', 's', 'c', '.', '<', 'E', 'O', 'S', '>', 'o', 'u', 't', 'o', 'f', 't', 'h', 'e', '4', '0', ',', '5', 'o', 'f', 't', 'h', 'e', 'c', 'a', 's', 'e', 's', 'w', 'e', 'r', 'e', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n', 'e', 's', 'c', 'o', 'r', 't', 'e', 'd', 't', 'o', 'd', 'a', 'y', 'a', 'n', 'd', 'p', 'e', 'r', 'c', 'n', 'n', 'h', 'a', 'v', 'e', 'j', 'u', 's', 't', 'l', 'a', 'n', 'd', 'e', 'd', 'i', 'n', 'm', 'i', 'a', 'm', 'i', '.', '<', 'E', 'O', 'S', '>', 't', 'h', 'e', 'r', 'e', 'm', 'a', 'i', 'n', 'i', 'n', 'g', '3', '5', 'w', 'e', 'a', 'r', 'e', 'r', 'e', 'v', 'i', 'e', 'w', 'i', 'n', 'g', 'n', 'o', 'w', '.', '<', 'E', 'O', 'S', '>', 's', 'o', 'm', 'e', 'c', 'o', 'r', 'r', 'e', 's', 'p', 'o', 'n', 'd', 'e', 'n', 'c', 'e', 'i', \"'\", 'v', 'e', 's', 'e', 'e', 'n', 's', 'a', 'y', 's', 't', 'h', 'a', 't', 't', 'h', 'e', 'y', 'p', 'l', 'a', 'n', 'o', 'n', 'b', 'r', 'i', 'n', 'g', 'i', 'n', 'g', 't', 'h', 'e', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n', 's', 't', 'r', 'a', 'i', 'g', 'h', 't', 't', 'o', 't', 'h', 'e', 'a', 'i', 'r', 'p', 'o', 'r', 't', ',', 'a', 'n', 'd', 'w', 'e', 'h', 'a', 'v', 'e', 'r', 'e', 's', 'p', 'o', 'n', 'd', 'e', 'd', 't', 'h', 'a', 't', 't', 'h', 'e', 'y', 'f', 'i', 'r', 's', 't', 'n', 'e', 'e', 'd', 't', 'o', 'c', 'o', 'm', 'e', 't', 'o', 't', 'h', 'e', 'e', 'm', 'b', 'a', 's', 's', 'y', '.', '<', 'E', 'O', 'S', '>', 'o', 'n', 'c', 'e', 'c', 'a', 's', 'e', 'i', 'n', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', 'w', 'i', 'l', 'l', 'b', 'e', 'a', 'n', 'i', 's', 's', 'u', 'e', ',', 'a', 's', 'i', 't', 'r', 'e', 'q', 'u', 'i', 'r', 'e', 's', 't', 'h', 'e', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', 'i', 'a', 'l', 'w', 'a', 'i', 'v', 'e', 'r', '.', '<', 'E', 'O', 'S', '>', 'd', 'i', 'd', 'y', 'o', 'u', 'r', 'e', 'c', 'e', 'i', 'v', 'e', 't', 'h', 'e', 't', 'a', 'l', 'k', 'i', 'n', 'g', 'p', 'o', 'i', 'n', 't', 's', 'a', 'b', 'o', 'u', 't', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', 'i', 'a', 'l', 'w', 'a', 'i', 'v', 'e', 'r', 's', '?', '<', 'E', 'O', 'S', '>', 't', 'h', 'e', 'm', 'a', 'j', 'o', 'r', 'i', 't', 'y', 'o', 'f', 't', 'h', 'e', 'c', 'a', 's', 'e', 's', 'w', 'i', 'l', 'l', 'q', 'u', 'a', 'l', 'i', 'f', 'y', 'f', 'o', 'r', 'h', 'u', 'm', 'a', 'n', 'i', 't', 'a', 'r', 'i', 'a', 'n', 'p', 'a', 'r', 'o', 'l', 'e', '.', '<', 'E', 'O', 'S', '>', 'i', 'n', 't', 'h', 'e', 'l', 'a', 's', 't', '3', '0', 'm', 'i', 'n', 'u', 't', 'e', 's', 'a', 'n', 'u', 'm', 'b', 'e', 'r', 'o', 'f', 'e', 'm', 'a', 'i', 'l', 's', 'h', 'a', 'v', 'e', 'b', 'e', 'e', 'n', 'c', 'o', 'm', 'i', 'n', 'g', 'i', 'n', 'a', 'b', 'o', 'u', 't', 'p', 'r', 'i', 'v', 'a', 't', 'e', 'p', 'l', 'a', 'n', 'e', 's', ',', 't', 'e', 'd', 't', 'u', 'r', 'n', 'e', 'r', 'o', 'f', 'f', 'e', 'r', 'i', 'n', 'g', 't', 'o', 'f', 'l', 'y', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n', 'o', 'u', 't', ',', 'e', 't', 'c', '.', '<', 'E', 'O', 'S', '>', 'w', 'e', 'h', 'a', 'v', 'e', 'a', 'c', 'a', 's', 'e', 'c', 'o', 'm', 'i', 'n', 'g', 'i', 'n', 't', 'o', 'm', 'o', 'r', 'r', 'o', 'w', 'm', 'o', 'r', 'n', 'i', 'n', 'g', 'i', 'n', 'w', 'h', 'i', 'c', 'h', 't', 'h', 'e', '2', 'a', 'd', 'o', 'p', 't', 'i', 'v', 'e', 'f', 'a', 'm', 'i', 'l', 'i', 'e', 's', 'a', 'r', 'e', 'b', 'e', 'i', 'n', 'g', 'f', 'l', 'o', 'w', 'n', 'i', 'n', 'b', 'y', 'c', 'b', 's', 'h', 'e', 'l', 'i', 'c', 'o', 'p', 't', 'e', 'r', 'f', 'r', 'o', 'm', 't', 'h', 'e', 'd', 'r', '.', '<', 'E', 'O', 'S', '>', 'w', 'e', 'a', 'r', 'e', 'p', 'r', 'e', 'p', 'a', 'r', 'i', 'n', 'g', 'a', 's', 'm', 'a', 'n', 'y', 'c', 'a', 's', 'e', 's', 'i', 'n', 'a', 'd', 'v', 'a', 'n', 'c', 'e', 'a', 's', 'p', 'o', 's', 's', 'i', 'b', 'l', 'e', 't', 'o', 'n', 'i', 'g', 'h', 't', '.', '<', 'E', 'O', 'S', '>', 'k', 'i', 'n', 'd', 'r', 'e', 'g', 'a', 'r', 'd', 's', ',', 'l', 'i', 'n', 'd', 'a', '<', 'E', 'O', 'S', '>'], 1)\n",
      "(['h', 'i', 'l', 'l', 'a', 'r', 'y', ',', 'a', 's', 'y', 'o', 'u', 'h', 'a', 'v', 'e', 'b', 'e', 'e', 'n', 's', 'o', 'k', 'i', 'n', 'd', 't', 'o', 's', 'h', 'a', 'r', 'e', 'y', 'o', 'u', 'r', 'a', 'd', 'v', 'i', 'c', 'e', 'a', 'n', 'd', 'm', 'e', 'n', 't', 'o', 'r', 's', 'h', 'i', 'p', 'w', 'i', 't', 'h', 'm', 'e', 'i', 'n', 't', 'h', 'e', 'p', 'a', 's', 't', ',', 'i', 'w', 'a', 's', 'h', 'o', 'p', 'i', 'n', 'g', 't', 'h', 'a', 't', 'w', 'e', 'm', 'i', 'g', 'h', 't', 'f', 'i', 'n', 'd', 's', 'o', 'm', 'e', 't', 'i', 'm', 'e', 't', 'o', 'c', 'h', 'a', 't', 'a', 'b', 'o', 'u', 't', 's', 'o', 'm', 'e', 'o', 'f', 'm', 'y', 'c', 'u', 'r', 'r', 'e', 'n', 't', 't', 'h', 'i', 'n', 'k', 'i', 'n', 'g', '.', '<', 'E', 'O', 'S', '>', 'w', 'i', 't', 'h', 't', 'h', 'e', 'f', 'a', 'b', 'u', 'l', 'o', 'u', 's', 'r', 'e', 's', 'u', 'l', 't', 's', 'o', 'f', 't', 'h', 'e', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', 's', \"'\", 'r', 'e', '-', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', ',', 'i', 'a', 'm', 'p', 'r', 'i', 'v', 'i', 'l', 'e', 'g', 'e', 'd', 't', 'o', 'b', 'e', 'a', 'b', 'l', 'e', 't', 'o', 'c', 'o', 'n', 't', 'i', 'n', 'u', 'e', 't', 'o', 's', 'e', 'r', 'v', 'e', 'a', 't', 't', 'h', 'e', 'c', 'f', 't', 'c', '.', '<', 'E', 'O', 'S', '>', 'i', 'r', 'e', 'c', 'a', 'l', 'l', 'y', 'o', 'u', 'r', 'p', 'r', 'i', 'o', 'r', 'a', 'd', 'v', 'i', 'c', 'e', 'u', 'p', 'o', 'n', 'm', 'y', 'b', 'e', 'i', 'n', 'g', 'o', 'f', 'f', 'e', 'r', 'e', 'd', 't', 'h', 'e', 'p', 'o', 's', 't', '<', 'E', 'O', 'S', '>', 'a', 'n', 'd', 'i', 't', 'w', 'a', 's', 'i', 'n', 'v', 'a', 'l', 'u', 'a', 'b', 'l', 'e', '.', '<', 'E', 'O', 'S', '>', 't', 'h', 'e', 'c', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'e', 's', 'a', 'n', 'd', 'o', 'p', 'p', 'o', 'r', 't', 'u', 'n', 'i', 't', 'i', 'e', 's', 't', 'o', 'b', 'r', 'i', 'n', 'g', 'a', 'b', 'o', 'u', 't', 'c', 'o', 'm', 'm', 'o', 'n', 's', 'e', 'n', 's', 'e', 'f', 'i', 'n', 'a', 'n', 'c', 'i', 'a', 'l', 'r', 'e', 'f', 'o', 'r', 'm', 's', 'h', 'a', 'v', 'e', 'b', 'e', 'e', 'n', 'f', 'a', 'b', 'u', 'l', 'o', 'u', 's', '.', '<', 'E', 'O', 'S', '>', 'w', 'e', 'a', 'r', 'e', 'n', 'o', 'w', 'm', 'o', 'v', 'i', 'n', 'g', 'b', 'e', 'y', 'o', 'n', 'd', 'a', 'g', 'e', 'n', 'c', 'y', 'r', 'u', 'l', 'e', 'w', 'r', 'i', 't', 'i', 'n', 'g', 'a', 'n', 'd', 'h', 'e', 'l', 'p', 'i', 'n', 'g', 't', 'h', 'e', 's', 'w', 'a', 'p', 's', 'm', 'a', 'r', 'k', 'e', 't', 's', 't', 'r', 'a', 'n', 's', 'i', 't', 'i', 'o', 'n', 't', 'o', 'a', 'n', 'e', 'w', 'e', 'r', 'a', 'o', 'f', 't', 'r', 'a', 'n', 's', 'p', 'a', 'r', 'e', 'n', 'c', 'y', 'a', 'n', 'd', 'o', 'v', 'e', 'r', 's', 'i', 'g', 'h', 't', '.', '<', 'E', 'O', 'S', '>', '(', 't', 'h', 'e', 'c', 'f', 't', 'c', 'j', 'u', 's', 't', 'c', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'd', 'f', 'i', 'n', 'a', 'l', 'd', 'e', 't', 'e', 'r', 'm', 'i', 'n', 'a', 't', 'i', 'o', 'n', 's', 's', 'u', 'c', 'h', 't', 'h', 'a', 't', 't', 'h', 'e', 'u', 's', 'h', 'a', 's', 'm', 'e', 't', 't', 'h', 'e', 'p', 'i', 't', 't', 's', 'b', 'u', 'r', 'g', 'h', 'g', '-', '2', '0', 'c', 'o', 'm', 'm', 'i', 't', 'm', 'e', 'n', 't', 'd', 'e', 'a', 'd', 'l', 'i', 'n', 'e', 'o', 'f', 'd', 'e', 'c', 'e', 'm', 'b', 'e', 'r', '2', '0', '1', '2', 't', 'o', 'h', 'a', 'v', 'e', 'o', 'u', 'r', 's', 'w', 'a', 'p', 's', 'c', 'l', 'e', 'a', 'r', 'i', 'n', 'g', 'm', 'a', 'n', 'd', 'a', 't', 'e', 'f', 'u', 'l', 'l', 'y', 'i', 'n', 'p', 'l', 'a', 'c', 'e', '.', ')', '<', 'E', 'O', 'S', '>', 'i', 'f', 'w', 'e', 'm', 'i', 'g', 'h', 't', 'b', 'e', 'a', 'b', 'l', 'e', 't', 'o', 'f', 'i', 'n', 'd', 'a', 'm', 'o', 'm', 'e', 'n', 't', 't', 'o', 'c', 'h', 'a', 't', ',', 'i', 'w', 'o', 'u', 'l', 'd', 'l', 'o', 'v', 'e', 't', 'o', 's', 'h', 'a', 'r', 'e', 'm', 'y', 't', 'h', 'o', 'u', 'g', 'h', 't', 's', 'o', 'n', 'p', 'o', 's', 's', 'i', 'b', 'l', 'e', 'n', 'e', 'w', 'c', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'e', 's', 'a', 'n', 'd', 'o', 'p', 'p', 'o', 'r', 't', 'u', 'n', 'i', 't', 'i', 'e', 's', 'w', 'i', 't', 'h', 'i', 'n', 't', 'h', 'e', 'a', 'd', 'm', 'i', 'n', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', '.', '<', 'E', 'O', 'S', '>', 'i', 'h', 'o', 'p', 'e', 'a', 'l', 'l', 'i', 's', 'w', 'e', 'l', 'l', 'w', 'i', 't', 'h', 'y', 'o', 'u', 'a', 's', 'w', 'e', 'l', 'l', 'a', 's', 'y', 'o', 'u', 'g', 'e', 'a', 'r', 'u', 'p', 'f', 'o', 'r', 't', 'h', 'i', 's', 'h', 'o', 'l', 'i', 'd', 'a', 'y', 't', 'i', 'm', 'e', 'a', 'n', 'd', 'a', 'm', 'u', 'c', 'h', 'd', 'e', 's', 'e', 'r', 'v', 'e', 'd', 'b', 'r', 'e', 'a', 'k', 'f', 'r', 'o', 'm', 't', 'h', 'e', 'd', 'a', 'y', 't', 'o', 'd', 'a', 'y', 's', 't', 'r', 'e', 's', 's', 'e', 's', 'o', 'f', 'y', 'o', 'u', 'r', 'c', 'u', 'r', 'r', 'e', 'n', 't', 'p', 'o', 's', 't', '.', '<', 'E', 'O', 'S', '>', 'g', 'a', 'r', 'y', '<', 'E', 'O', 'S', '>'], 2)\n",
      "200\n",
      "(['madame', 'secretary', ':', 'thank', 'you', 'for', 'reaching', 'out', 'to', 'secretary', 'solis', 'and', 'convincing', 'her', 'to', 'join', 'the', 'verification', 'commission', '.', '<EOS>', 'she', 'and', 'ricardo', 'lagos', 'will', 'make', 'for', 'a', 'very', 'high', 'profile', 'and', 'effective', 'international', 'component', 'of', 'the', 'commission', '.', '<EOS>', 'we', 'will', 'meet', 'with', 'her', 'this', 'morning', 'at', '10', 'am', 'to', 'brief', 'her', 'for', 'tuesday', \"'s\", 'journey', 'to', 'honduras', '.', '<EOS>', 'at', 'this', 'point', ',', 'it', 'appears', 'we', 'have', 'a', 'military', 'aircraft', 'available', '.', '<EOS>', 'the', 'plan', 'is', 'for', 'the', 'u.s.', 'delegation', 'to', 'depart', 'd.c.', ',', 'stop', 'in', 'miami', 'to', 'pick', 'up', 'ricardo', 'lagos', ',', 'and', 'arrive', 'in', 'tegucigalpa', 'together', '.', '<EOS>', 'we', 'think', 'this', 'will', 'send', 'a', 'powerful', 'message', 'to', 'hondurans', 'and', 'leave', 'no', 'doubt', 'about', 'our', 'commitment', 'to', 'seeing', 'this', 'process', 'through', 'to', 'a', 'successful', 'conclusion', '.', '<EOS>', 'you', 'should', 'be', 'aware', 'that', 'ambassador', 'hugo', 'llorens', 'is', 'under', 'public', 'assault', '.', '<EOS>', 'the', 'wall', 'street', 'journal', 'dedicates', 'its', 'america', \"'s\", 'column', 'this', 'morning', 'to', 'attacking', 'him', 'and', 'calling', 'for', 'his', 'removal', '.', '<EOS>', 'last', 'friday', ',', 'representative', 'connie', 'mack', 'did', 'the', 'same', '.', '<EOS>', 'this', 'chorus', 'will', 'grow', 'as', 'the', 'extent', 'of', 'our', 'accomplishment', 'is', 'understood', '.', '<EOS>', 'llorens', 'is', 'a', 'tough', ',', 'stalwart', 'guy', '.', '<EOS>', 'he', 'and', 'his', 'mission', 'have', 'held', 'firm', 'during', 'this', 'crisis', '.', '<EOS>', 'a', 'call', 'from', 'you', 'would', 'be', 'a', 'big', 'boost', '.', '<EOS>', 'finally', ',', 'we', 'will', 'hold', 'an', 'ipc', 'today', 'to', 'identify', 'further', 'steps', '.', '<EOS>', 'we', 'will', 'keep', 'you', 'up', 'to', 'date', 'on', 'these', 'steps', 'and', 'identify', 'further', 'opportunities', 'for', 'your', 'engagement', '.', '<EOS>', 'i', 'want', 'to', 'thank', 'you', 'for', 'your', 'leadership', 'and', 'support', 'during', 'this', 'long', 'crisis', '.', '<EOS>', 'your', 'willingness', 'to', 'engage', 'at', 'key', 'moments', 'and', 'take', 'risks', 'at', 'the', 'right', 'time', 'have', 'propelled', 'us', 'much', 'further', 'than', 'anyone', 'expected', '.', '<EOS>', 'your', 'diplomacy', 'prevented', 'a', 'debilitating', 'civil', 'conflict', 'in', 'honduras', 'that', 'would', 'have', 'destabilized', 'central', 'america', 'and', 'undermined', 'two', 'decades', 'of', 'our', 'efforts', '.', '<EOS>', 'we', 'now', 'have', 'a', 'big', 'opportunity', 'in', 'front', 'of', 'us', ',', 'and', 'for', 'that', 'we', 'are', 'grateful', 'to', 'you', '.', '<EOS>', 'regards', ',', 'tom', '<EOS>'], 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_inp_file = 'processed_data/GCDC/Clinton_train.jsonl'\n",
    "with open(train_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        document = []\n",
    "        for sentence in json_obj['sentences']:\n",
    "            # document.append([word.lower() for word in sentence])\n",
    "            for word in sentence:\n",
    "                document.append(word.lower())\n",
    "            # add <EOS> at the end of each sentence\n",
    "            document.append('<EOS>')\n",
    "        label = json_obj['label']\n",
    "        \n",
    "        train_data.append((document, label))\n",
    "   \n",
    "\n",
    "test_data = []\n",
    "test_inp_file = 'processed_data/GCDC/Clinton_test.jsonl'\n",
    "with open(test_inp_file, 'r') as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        document = []\n",
    "        for sentence in json_obj['sentences']:\n",
    "            # document.append([word.lower() for word in sentence])\n",
    "            for word in sentence:\n",
    "                document.append(word.lower())\n",
    "            # add <EOS> at the end of each sentence\n",
    "            document.append('<EOS>')\n",
    "        label = json_obj['label']\n",
    "        test_data.append((document, label))\n",
    "        \n",
    "        \n",
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "print(train_data[1])\n",
    "print(train_data[2])\n",
    "print(len(test_data))\n",
    "print(test_data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415\n",
      "(['two', 'options', 'the', 'us', 'views', 'the', 'transitional', 'national', 'council', 'as', 'the', 'sole', '/', 'only', 'legitimate', 'interlocutor', 'of', 'the', 'libyan', 'people', 'during', 'this', 'interim', 'period', ',', 'as', 'libyans', 'come', 'together', 'to', 'plan', 'their', 'own', 'future', 'and', 'a', 'permanent', ',', 'inclusive', 'constitutional', 'system', 'that', 'protects', 'the', 'rights', 'of', 'all', 'libyans', '.', '<EOS>', 'this', 'is', 'in', 'contrast', 'to', 'the', 'qadhafi', 'regime', ',', 'which', 'has', 'lost', 'all', 'legitimacy', 'to', 'rule', '.', '<EOS>', 'the', 'us', 'views', 'the', 'transitional', 'national', 'council', 'as', 'the', 'legitimate', 'interlocutor', 'of', 'the', 'libyan', 'people', 'during', 'this', 'interim', 'period', ',', 'as', 'libyans', 'come', 'together', 'to', 'plan', 'their', 'own', 'future', 'and', 'a', 'permanent', ',', 'inclusive', 'constitutional', 'system', 'that', 'protects', 'the', 'rights', 'of', 'all', 'libyans', '.', '<EOS>', 'this', 'is', 'in', 'contrast', 'to', 'the', 'qadhafi', 'regime', ',', 'which', 'has', 'lost', 'all', 'legitimacy', 'to', 'rule', '.', '<EOS>', 'the', 'inc', 'is', 'the', 'institution', 'through', 'which', 'we', 'are', 'engaging', 'the', 'libyan', 'people', 'at', 'this', 'time', '.', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], 3)\n",
      "(['madame', 'secretary', ':', 'thank', 'you', 'for', 'reaching', 'out', 'to', 'secretary', 'solis', 'and', 'convincing', 'her', 'to', 'join', 'the', 'verification', 'commission', '.', '<EOS>', 'she', 'and', 'ricardo', 'lagos', 'will', 'make', 'for', 'a', 'very', 'high', 'profile', 'and', 'effective', 'international', 'component', 'of', 'the', 'commission', '.', '<EOS>', 'we', 'will', 'meet', 'with', 'her', 'this', 'morning', 'at', '10', 'am', 'to', 'brief', 'her', 'for', 'tuesday', \"'s\", 'journey', 'to', 'honduras', '.', '<EOS>', 'at', 'this', 'point', ',', 'it', 'appears', 'we', 'have', 'a', 'military', 'aircraft', 'available', '.', '<EOS>', 'the', 'plan', 'is', 'for', 'the', 'u.s.', 'delegation', 'to', 'depart', 'd.c.', ',', 'stop', 'in', 'miami', 'to', 'pick', 'up', 'ricardo', 'lagos', ',', 'and', 'arrive', 'in', 'tegucigalpa', 'together', '.', '<EOS>', 'we', 'think', 'this', 'will', 'send', 'a', 'powerful', 'message', 'to', 'hondurans', 'and', 'leave', 'no', 'doubt', 'about', 'our', 'commitment', 'to', 'seeing', 'this', 'process', 'through', 'to', 'a', 'successful', 'conclusion', '.', '<EOS>', 'you', 'should', 'be', 'aware', 'that', 'ambassador', 'hugo', 'llorens', 'is', 'under', 'public', 'assault', '.', '<EOS>', 'the', 'wall', 'street', 'journal', 'dedicates', 'its', 'america', \"'s\", 'column', 'this', 'morning', 'to', 'attacking', 'him', 'and', 'calling', 'for', 'his', 'removal', '.', '<EOS>', 'last', 'friday', ',', 'representative', 'connie', 'mack', 'did', 'the', 'same', '.', '<EOS>', 'this', 'chorus', 'will', 'grow', 'as', 'the', 'extent', 'of', 'our', 'accomplishment', 'is', 'understood', '.', '<EOS>', 'llorens', 'is', 'a', 'tough', ',', 'stalwart', 'guy', '.', '<EOS>', 'he', 'and', 'his', 'mission', 'have', 'held', 'firm', 'during', 'this', 'crisis', '.', '<EOS>', 'a', 'call', 'from', 'you', 'would', 'be', 'a', 'big', 'boost', '.', '<EOS>', 'finally', ',', 'we', 'will', 'hold', 'an', 'ipc', 'today', 'to', 'identify', 'further', 'steps', '.', '<EOS>', 'we', 'will', 'keep', 'you', 'up', 'to', 'date', 'on', 'these', 'steps', 'and', 'identify', 'further', 'opportunities', 'for', 'your', 'engagement', '.', '<EOS>', 'i', 'want', 'to', 'thank', 'you', 'for', 'your', 'leadership', 'and', 'support', 'during', 'this', 'long', 'crisis', '.', '<EOS>', 'your', 'willingness', 'to', 'engage', 'at', 'key', 'moments', 'and', 'take', 'risks', 'at', 'the', 'right', 'time', 'have', 'propelled', 'us', 'much', 'further', 'than', 'anyone', 'expected', '.', '<EOS>', 'your', 'diplomacy', 'prevented', 'a', 'debilitating', 'civil', 'conflict', 'in', 'honduras', 'that', 'would', 'have', 'destabilized', 'central', 'america', 'and', 'undermined', 'two', 'decades', 'of', 'our', 'efforts', '.', '<EOS>', 'we', 'now', 'have', 'a', 'big', 'opportunity', 'in', 'front', 'of', 'us', ',', 'and', 'for', 'that', 'we', 'are', 'grateful', 'to', 'you', '.', '<EOS>', 'regards', ',', 'tom', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], 3)\n"
     ]
    }
   ],
   "source": [
    "# pad the document to the same length\n",
    "def pad_document(document, max_len):\n",
    "    if len(document) >= max_len:\n",
    "        return document[:max_len]\n",
    "    else:\n",
    "        return document + ['<PAD>'] * (max_len - len(document))\n",
    "\n",
    "# find the max length of the document\n",
    "max_len = 0\n",
    "for document, label in train_data:\n",
    "    max_len = max(max_len, len(document))\n",
    "for document, label in test_data:\n",
    "    max_len = max(max_len, len(document))\n",
    "print(max_len)\n",
    "\n",
    "# pad the document\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = (pad_document(train_data[i][0], max_len), train_data[i][1])\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i] = (pad_document(test_data[i][0], max_len), test_data[i][1])\n",
    "print(train_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build vocabulary using training data\n",
    "# vocabulary = torchtext.vocab.build_vocab_from_iterator([document for document, label in train_data], specials=['<UNK>', '<PAD>', '<EOS>'], min_freq=2)\n",
    "# vocabulary.set_default_index(vocabulary['<UNK>'])\n",
    "\n",
    "# print(len(vocabulary))\n",
    "# print(vocabulary.get_itos()[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert words to indices\n",
    "# train_indices = []\n",
    "# for document, label in train_data:\n",
    "#     document_indices = []\n",
    "#     for word in document:\n",
    "#         document_indices.append(vocabulary[word])\n",
    "#     train_indices.append((document_indices, label))\n",
    "\n",
    "# test_indices = []\n",
    "# for document, label in test_data:\n",
    "#     document_indices = []\n",
    "#     for word in document:\n",
    "#         document_indices.append(vocabulary[word])\n",
    "#     test_indices.append((document_indices, label))\n",
    "\n",
    "\n",
    "# # subtract 1 from the label\n",
    "# for i in range(len(train_indices)):\n",
    "#     train_indices[i] = (train_indices[i][0], train_indices[i][1] - 1)\n",
    "# for i in range(len(test_indices)):\n",
    "#     test_indices[i] = (test_indices[i][0], test_indices[i][1] - 1)\n",
    "\n",
    "# print(train_indices[0])\n",
    "# print(test_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create dataloaders\n",
    "# batch_size = 4\n",
    "# # convert to tensors\n",
    "# train_indices = [(torch.tensor(document_indices), label) for document_indices, label in train_indices]\n",
    "# test_indices = [(torch.tensor(document_indices), label) for document_indices, label in test_indices]\n",
    "# # create dataloaders\n",
    "# train_loader = DataLoader(train_indices, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_indices, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# use pre-trained transformer model BERT to classify documents into three classes\n",
    "# BERT is a transformer model pre-trained on large corpus\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(bert.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[2048], [7047], [1996], [2149], [5328], [1996], [17459], [2120], [2473], [2004], [1996], [7082], [1013], [2069], [11476], [6970, 4135, 12690, 2953], [1997], [1996], [19232], [2111], [2076], [2023], [9455], [2558], [1010], [2004], [19232, 2015], [2272], [2362], [2000], [2933], [2037], [2219], [2925], [1998], [1037], [4568], [1010], [18678], [6543], [2291], [2008], [18227], [1996], [2916], [1997], [2035], [19232, 2015], [1012], [1026, 1041, 2891, 1028], [2023], [2003], [1999], [5688], [2000], [1996], [1053, 4215, 3270, 8873], [6939], [1010], [2029], [2038], [2439], [2035], [22568], [2000], [3627], [1012], [1026, 1041, 2891, 1028], [1996], [2149], [5328], [1996], [17459], [2120], [2473], [2004], [1996], [11476], [6970, 4135, 12690, 2953], [1997], [1996], [19232], [2111], [2076], [2023], [9455], [2558], [1010], [2004], [19232, 2015], [2272], [2362], [2000], [2933], [2037], [2219], [2925], [1998], [1037], [4568], [1010], [18678], [6543], [2291], [2008], [18227], [1996], [2916], [1997], [2035], [19232, 2015], [1012], [1026, 1041, 2891, 1028], [2023], [2003], [1999], [5688], [2000], [1996], [1053, 4215, 3270, 8873], [6939], [1010], [2029], [2038], [2439], [2035], [22568], [2000], [3627], [1012], [1026, 1041, 2891, 1028], [1996], [4297], [2003], [1996], [5145], [2083], [2029], [2057], [2024], [11973], [1996], [19232], [2111], [2012], [2023], [2051], [1012], [1026, 1041, 2891, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028]], 3)\n",
      "([[10602], [3187], [1024], [4067], [2017], [2005], [4285], [2041], [2000], [3187], [14017, 2483], [1998], [13359], [2014], [2000], [3693], [1996], [22616], [3222], [1012], [1026, 1041, 2891, 1028], [2016], [1998], [13559], [16738], [2097], [2191], [2005], [1037], [2200], [2152], [6337], [1998], [4621], [2248], [6922], [1997], [1996], [3222], [1012], [1026, 1041, 2891, 1028], [2057], [2097], [3113], [2007], [2014], [2023], [2851], [2012], [2184], [2572], [2000], [4766], [2014], [2005], [9857], [1005, 1055], [4990], [2000], [14373], [1012], [1026, 1041, 2891, 1028], [2012], [2023], [2391], [1010], [2009], [3544], [2057], [2031], [1037], [2510], [2948], [2800], [1012], [1026, 1041, 2891, 1028], [1996], [2933], [2003], [2005], [1996], [1057, 1012, 1055, 1012], [10656], [2000], [18280], [1040, 1012, 1039, 1012], [1010], [2644], [1999], [5631], [2000], [4060], [2039], [13559], [16738], [1010], [1998], [7180], [1999], [8915, 12193, 6895, 9692, 4502], [2362], [1012], [1026, 1041, 2891, 1028], [2057], [2228], [2023], [2097], [4604], [1037], [3928], [4471], [2000], [10189, 24979, 6962], [1998], [2681], [2053], [4797], [2055], [2256], [8426], [2000], [3773], [2023], [2832], [2083], [2000], [1037], [3144], [7091], [1012], [1026, 1041, 2891, 1028], [2017], [2323], [2022], [5204], [2008], [6059], [9395], [2222, 5686, 3619], [2003], [2104], [2270], [6101], [1012], [1026, 1041, 2891, 1028], [1996], [2813], [2395], [3485], [2139, 16467, 2015], [2049], [2637], [1005, 1055], [5930], [2023], [2851], [2000], [7866], [2032], [1998], [4214], [2005], [2010], [8208], [1012], [1026, 1041, 2891, 1028], [2197], [5958], [1010], [4387], [16560], [11349], [2106], [1996], [2168], [1012], [1026, 1041, 2891, 1028], [2023], [7165], [2097], [4982], [2004], [1996], [6698], [1997], [2256], [24718], [2003], [5319], [1012], [1026, 1041, 2891, 1028], [2222, 5686, 3619], [2003], [1037], [7823], [1010], [2358, 2389, 18367], [3124], [1012], [1026, 1041, 2891, 1028], [2002], [1998], [2010], [3260], [2031], [2218], [3813], [2076], [2023], [5325], [1012], [1026, 1041, 2891, 1028], [1037], [2655], [2013], [2017], [2052], [2022], [1037], [2502], [12992], [1012], [1026, 1041, 2891, 1028], [2633], [1010], [2057], [2097], [2907], [2019], [12997, 2278], [2651], [2000], [6709], [2582], [4084], [1012], [1026, 1041, 2891, 1028], [2057], [2097], [2562], [2017], [2039], [2000], [3058], [2006], [2122], [4084], [1998], [6709], [2582], [6695], [2005], [2115], [8147], [1012], [1026, 1041, 2891, 1028], [1045], [2215], [2000], [4067], [2017], [2005], [2115], [4105], [1998], [2490], [2076], [2023], [2146], [5325], [1012], [1026, 1041, 2891, 1028], [2115], [19732], [2000], [8526], [2012], [3145], [5312], [1998], [2202], [10831], [2012], [1996], [2157], [2051], [2031], [15801], [2149], [2172], [2582], [2084], [3087], [3517], [1012], [1026, 1041, 2891, 1028], [2115], [17610], [8729], [1037], [2139, 14454, 16518], [2942], [4736], [1999], [14373], [2008], [2052], [2031], [4078, 2696, 14454, 3550], [2430], [2637], [1998], [25174, 2094], [2048], [5109], [1997], [2256], [4073], [1012], [1026, 1041, 2891, 1028], [2057], [2085], [2031], [1037], [2502], [4495], [1999], [2392], [1997], [2149], [1010], [1998], [2005], [2008], [2057], [2024], [8794], [2000], [2017], [1012], [1026, 1041, 2891, 1028], [12362], [1010], [3419], [1026, 1041, 2891, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028], [1026, 11687, 1028]], 3)\n"
     ]
    }
   ],
   "source": [
    "# use the bert tokenizer to tokenize the document and convert the tokens to indices\n",
    "train_indices = []\n",
    "for document, label in train_data:\n",
    "    document_indices = []\n",
    "    for word in document:\n",
    "        document_indices.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)))\n",
    "    train_indices.append((document_indices, label-1))\n",
    "\n",
    "test_indices = []\n",
    "for document, label in test_data:\n",
    "    document_indices = []\n",
    "    for word in document:\n",
    "        document_indices.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)))\n",
    "    test_indices.append((document_indices, label-1))\n",
    "\n",
    "print(train_indices[0])\n",
    "print(test_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# define a transformer model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = Classifier(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        x = self.bert(x)[0]\n",
    "        # x: (batch_size, hidden_size)\n",
    "        x = x[:, 0, :]\n",
    "        # x: (batch_size, num_classes)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to train the model\n",
    "def train(model, optimizer, criterion, train_loader, test_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for document, label in train_loader:\n",
    "            # document: (batch_size, seq_len)\n",
    "            # label: (batch_size)\n",
    "            document = document.to(device)\n",
    "            label = label.to(device)\n",
    "            # output: (batch_size, num_classes)\n",
    "            output = model(document)\n",
    "            # output: (batch_size, num_classes)\n",
    "            loss = criterion(output, label)\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "        test(model, test_loader)\n",
    "\n",
    "# define a function to test the model\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for document, label in test_loader:\n",
    "            # document: (batch_size, seq_len)\n",
    "            # label: (batch_size)\n",
    "            document = document.to(device)\n",
    "            label = label.to(device)\n",
    "            # output: (batch_size, num_classes)\n",
    "            output = model(document)\n",
    "            # output: (batch_size)\n",
    "            output = torch.argmax(output, dim=1)\n",
    "            correct += torch.sum(torch.eq(output, label)).item()\n",
    "            total += len(label)\n",
    "    print('Accuracy: {}'.format(correct / total))\n",
    "\n",
    "# define a function to predict the label of a document\n",
    "def predict(model, document):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # document: (seq_len)\n",
    "        document = torch.LongTensor(document).unsqueeze(0).to(device)\n",
    "        # output: (1, num_classes)\n",
    "        output = model(document)\n",
    "        # output: (1)\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        return output.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transformer model\n",
    "hidden_size = bert.config.hidden_size\n",
    "num_classes = 3\n",
    "model = Transformer(hidden_size, num_classes).to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6066164970397949\n",
      "Accuracy: 0.555\n",
      "Epoch: 1, Loss: 0.7545852065086365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train(model, optimizer, criterion, train_loader, test_loader, num_epochs)\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m---> 19\u001b[0m test(model, test_loader)\n",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[39m# output: (batch_size)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39;49meq(output, label))\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m         total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(label)\n\u001b[1;32m     38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(correct \u001b[39m/\u001b[39m total))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "num_epochs = 10\n",
    "train(model, optimizer, criterion, train_loader, test_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'this', 'movie', '!']\n",
      "[1045, 2293, 2023, 3185, 999]\n"
     ]
    }
   ],
   "source": [
    "sample = 'I love this movie!'\n",
    "tokenss = tokenizer.tokenize(sample)\n",
    "print(tokenss)\n",
    "indices = tokenizer.convert_tokens_to_ids(tokenss)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
