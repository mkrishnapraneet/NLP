{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import collections\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 315157\n",
      "['this', 'has', 'some', 'great', 'tips', 'as', 'always', 'and', 'is', 'helping', 'me', 'to', 'complete', 'my', 'good', 'eats', 'collection', '.']\n"
     ]
    }
   ],
   "source": [
    "input_file = '../../reviews_Movies_and_TV.json'\n",
    "# input_file = 'try.json'\n",
    "\n",
    "# Load the data\n",
    "sentences = []\n",
    "counter = 0\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if counter > 50000:\n",
    "            break\n",
    "        # add each sentence as a list of words to the sentences list, but each line of the json object is a document containing multiple sentences\n",
    "        # sentences.append(word_tokenize(json.loads(line)['reviewText']))\n",
    "        doc_sentences = sent_tokenize(json.loads(line)['reviewText'])\n",
    "        # sentences.append([word_tokenize(sentence) for sentence in doc_sentences])\n",
    "        for sentence in doc_sentences:\n",
    "            sentences.append([word.lower() for word in word_tokenize(sentence)])\n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print(sentences[0])\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 27522\n",
      "The 10 most common words are: \n",
      "[('the', 343600), (',', 278064), ('.', 272598), ('and', 181342), ('a', 160983), ('of', 154338), ('to', 138051), ('is', 124059), ('it', 108617), ('i', 103332)]\n"
     ]
    }
   ],
   "source": [
    "# form the vocabulary\n",
    "# Flatten the list of sentences into a single list of words\n",
    "words = itertools.chain.from_iterable(sentences)\n",
    "\n",
    "# Create a Counter object to count the frequency of each word\n",
    "word_counter = collections.Counter(words)\n",
    "\n",
    "# Extract the unique words from the Counter object to form the vocabulary\n",
    "min_freq = 5\n",
    "# vocabulary = set(word_counter.keys())\n",
    "# vocabulary = set(word for word, count in word_counter.items() if count >= min_freq)\n",
    "# add the word if it occurs more than min_freq times, else add <unk> token\n",
    "vocabulary = set(word if count >= min_freq else '<unk>' for word, count in word_counter.items())\n",
    "\n",
    "# add the <pad> token\n",
    "vocabulary.add('<pad>')\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print('Vocabulary size: {}'.format(len(vocabulary)))\n",
    "\n",
    "# Create a dictionary to map each word to an index\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create a dictionary to map each index to a word\n",
    "idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# print the 10 most common words\n",
    "print('The 10 most common words are: ')\n",
    "print(word_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for training\n",
    "window_size = 2\n",
    "sliding_window_size = window_size*2 + 1\n",
    "num_neg_samples_per_context = 3\n",
    "\n",
    "vocab_indices = list(word2idx.values())\n",
    "vocab_size = len(vocab_indices)\n",
    "\n",
    "# create data with X being indices of the context words and the target word, and y being 0 or 1 based on whether the target word is correct for the context words\n",
    "# also add negative samples\n",
    "def create_data_with_negative_sampling(sentences, word2idx, window_size, num_neg_samples_per_context):\n",
    "    X = []\n",
    "    y = []\n",
    "    # counter = 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            # a list of indices of context words and the target word\n",
    "            # if it goes out of bounds, add <pad> tokens            \n",
    "            context_words = sentence[max(0, i-window_size):i] + sentence[i+1:min(len(sentence), i+window_size+1)]\n",
    "            target_word = sentence[i]\n",
    "            # if the any of the words are not in the vocabulary, replace it with <unk>\n",
    "            context_words = [word if word in word2idx else '<unk>' for word in context_words]\n",
    "            target_word = target_word if target_word in word2idx else '<unk>'\n",
    "            data_point = [word2idx[context_word] for context_word in context_words]\n",
    "            # if the size of the data point is less than the sliding window size, add <pad> tokens\n",
    "            # if len(data_point) < sliding_window_size:\n",
    "            data_point += [word2idx['<pad>']]*(sliding_window_size-len(data_point)-1)\n",
    "            data_point.append(word2idx[target_word])\n",
    "\n",
    "            # add this to X and y\n",
    "            X.append(data_point)\n",
    "            y.append(1)\n",
    "\n",
    "            # add negative samples\n",
    "            for _ in range(num_neg_samples_per_context):\n",
    "                # generate a random index between 0 and vocab_size\n",
    "                negative_word = random.randint(0, vocab_size+1)\n",
    "                X.append(data_point[:-1] + [negative_word])                \n",
    "                y.append(0)\n",
    "        # counter += 1\n",
    "        # print(counter)\n",
    "    return X, y    \n",
    "\n",
    "    \n",
    "X, y = create_data_with_negative_sampling(sentences, word2idx, window_size, num_neg_samples_per_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# save the data to a file so that it can be loaded later\n",
    "# np.savez('data.npz', X=X, y=y)\n",
    "\n",
    "# load the data from the file\n",
    "def load_data(filename):\n",
    "    data = np.load(filename)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 27932812\n",
      "Number of labels: 27932812\n",
      "index of <unk> is: 15693\n",
      "index of <pad> is: 18046\n",
      "[17492  5566 18046 18046 24029]   1\n",
      "[17492  5566 18046 18046  3402]   0\n",
      "[17492  5566 18046 18046 18963]   0\n",
      "[17492  5566 18046 18046 24333]   0\n",
      "[24029  5566 19275 18046 17492]   1\n",
      "[24029  5566 19275 18046 18987]   0\n",
      "[24029  5566 19275 18046  6081]   0\n",
      "[24029  5566 19275 18046 22028]   0\n",
      "[24029 17492 19275 14370  5566]   1\n",
      "[24029 17492 19275 14370 13480]   0\n",
      "[24029 17492 19275 14370 22244]   0\n",
      "[24029 17492 19275 14370  2323]   0\n",
      "[17492  5566 14370 16306 19275]   1\n",
      "[17492  5566 14370 16306 13318]   0\n",
      "[17492  5566 14370 16306 21800]   0\n",
      "[17492  5566 14370 16306 24455]   0\n",
      "[ 5566 19275 16306 22765 14370]   1\n",
      "[ 5566 19275 16306 22765 11729]   0\n",
      "[ 5566 19275 16306 22765 19231]   0\n",
      "[ 5566 19275 16306 22765 25433]   0\n",
      "[19275 14370 22765  7565 16306]   1\n",
      "[19275 14370 22765  7565  6089]   0\n",
      "[19275 14370 22765  7565 18565]   0\n",
      "[19275 14370 22765  7565 15216]   0\n",
      "[14370 16306  7565 10043 22765]   1\n",
      "[14370 16306  7565 10043 10367]   0\n",
      "[14370 16306  7565 10043 24111]   0\n",
      "[14370 16306  7565 10043 26849]   0\n",
      "[16306 22765 10043 17382  7565]   1\n",
      "[16306 22765 10043 17382 26804]   0\n",
      "[16306 22765 10043 17382 11890]   0\n",
      "[16306 22765 10043 17382 25103]   0\n",
      "[22765  7565 17382  2478 10043]   1\n",
      "[22765  7565 17382  2478 17639]   0\n",
      "[22765  7565 17382  2478 14163]   0\n",
      "[22765  7565 17382  2478 17053]   0\n",
      "[ 7565 10043  2478 23786 17382]   1\n",
      "[ 7565 10043  2478 23786 18557]   0\n",
      "[ 7565 10043  2478 23786 22523]   0\n",
      "[ 7565 10043  2478 23786 12166]   0\n",
      "[10043 17382 23786 25173  2478]   1\n",
      "[10043 17382 23786 25173  2263]   0\n",
      "[10043 17382 23786 25173 20627]   0\n",
      "[10043 17382 23786 25173 14474]   0\n",
      "[17382  2478 25173 16640 23786]   1\n",
      "[17382  2478 25173 16640 12999]   0\n",
      "[17382  2478 25173 16640 18944]   0\n",
      "[17382  2478 25173 16640 16074]   0\n",
      "[ 2478 23786 16640  2247 25173]   1\n",
      "[ 2478 23786 16640  2247 17348]   0\n"
     ]
    }
   ],
   "source": [
    "print('Number of data points: {}'.format(len(X)))\n",
    "print('Number of labels: {}'.format(len(y)))\n",
    "\n",
    "# print(vocab_indices)\n",
    "print('index of <unk> is: {}'.format(word2idx['<unk>']))\n",
    "print('index of <pad> is: {}'.format(word2idx['<pad>']))\n",
    "\n",
    "for i in range (50):\n",
    "    print('{}   {}'.format(X[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow with negative sampling\n",
    "# hyperparameters\n",
    "embedding_size = 100\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "\n",
    "# initialize the weights\n",
    "# embedding matrix\n",
    "embeddings = np.random.uniform(-1, 1, (len(vocabulary), embedding_size))\n",
    "\n",
    "# use the same embedding matrix for both context and target\n",
    "\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
