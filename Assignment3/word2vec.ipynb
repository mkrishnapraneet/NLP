{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import collections\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../reviews_Movies_and_TV.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m sentences \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(input_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m         \u001b[39mif\u001b[39;00m counter \u001b[39m>\u001b[39m \u001b[39m50000\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../reviews_Movies_and_TV.json'"
     ]
    }
   ],
   "source": [
    "input_file = '../../reviews_Movies_and_TV.json'\n",
    "# input_file = 'try.json'\n",
    "\n",
    "# Load the data\n",
    "sentences = []\n",
    "counter = 0\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if counter > 50000:\n",
    "            break\n",
    "        # add each sentence as a list of words to the sentences list, but each line of the json object is a document containing multiple sentences\n",
    "        # sentences.append(word_tokenize(json.loads(line)['reviewText']))\n",
    "        doc_sentences = sent_tokenize(json.loads(line)['reviewText'])\n",
    "        # sentences.append([word_tokenize(sentence) for sentence in doc_sentences])\n",
    "        for sentence in doc_sentences:\n",
    "            sentences.append([word.lower() for word in word_tokenize(sentence)])\n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print(sentences[0])\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1\n",
      "The 10 most common words are: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# form the vocabulary\n",
    "# Flatten the list of sentences into a single list of words\n",
    "words = itertools.chain.from_iterable(sentences)\n",
    "\n",
    "# Create a Counter object to count the frequency of each word\n",
    "word_counter = collections.Counter(words)\n",
    "\n",
    "# Extract the unique words from the Counter object to form the vocabulary\n",
    "min_freq = 5\n",
    "# vocabulary = set(word_counter.keys())\n",
    "# vocabulary = set(word for word, count in word_counter.items() if count >= min_freq)\n",
    "# add the word if it occurs more than min_freq times, else add <unk> token\n",
    "vocabulary = set(word if count >= min_freq else '<unk>' for word, count in word_counter.items())\n",
    "\n",
    "# add the <pad> token\n",
    "vocabulary.add('<pad>')\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print('Vocabulary size: {}'.format(len(vocabulary)))\n",
    "\n",
    "# Create a dictionary to map each word to an index\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create a dictionary to map each index to a word\n",
    "idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# print the 10 most common words\n",
    "print('The 10 most common words are: ')\n",
    "print(word_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for training\n",
    "window_size = 2\n",
    "sliding_window_size = window_size*2 + 1\n",
    "num_neg_samples_per_context = 3\n",
    "\n",
    "vocab_indices = list(word2idx.values())\n",
    "vocab_size = len(vocab_indices)\n",
    "\n",
    "# create data with X being indices of the context words and the target word, and y being 0 or 1 based on whether the target word is correct for the context words\n",
    "# also add negative samples\n",
    "def create_data_with_negative_sampling(sentences, word2idx, window_size, num_neg_samples_per_context):\n",
    "    X = []\n",
    "    y = []\n",
    "    # counter = 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            # a list of indices of context words and the target word\n",
    "            # if it goes out of bounds, add <pad> tokens            \n",
    "            context_words = sentence[max(0, i-window_size):i] + sentence[i+1:min(len(sentence), i+window_size+1)]\n",
    "            target_word = sentence[i]\n",
    "            # if the any of the words are not in the vocabulary, replace it with <unk>\n",
    "            context_words = [word if word in word2idx else '<unk>' for word in context_words]\n",
    "            target_word = target_word if target_word in word2idx else '<unk>'\n",
    "            \n",
    "            data_point = [word2idx[context_word] for context_word in context_words]\n",
    "            # if the size of the data point is less than the sliding window size, add <pad> tokens\n",
    "            # if len(data_point) < sliding_window_size:\n",
    "            data_point += [word2idx['<pad>']]*(sliding_window_size-len(data_point)-1)\n",
    "            data_point.append(word2idx[target_word])\n",
    "\n",
    "            # add this to X and y\n",
    "            X.append(data_point)\n",
    "            y.append(1)\n",
    "\n",
    "            # add negative samples\n",
    "            for _ in range(num_neg_samples_per_context):\n",
    "                # generate a random index between 0 and vocab_size\n",
    "                negative_word = random.randint(0, vocab_size-1)\n",
    "                X.append(data_point[:-1] + [negative_word])                \n",
    "                y.append(0)\n",
    "        # counter += 1\n",
    "        # print(counter)\n",
    "    return X, y \n",
    "            \n",
    "\n",
    "    #         # convert the words to indices and add to X as [target_index, context_index1]\n",
    "    #         for context_word in context_words:\n",
    "    #             data_point = [word2idx[target_word], word2idx[context_word]]\n",
    "    #             X.append(data_point)\n",
    "    #             y.append(1)\n",
    "    #             # add negative samples\n",
    "    #             for _ in range(num_neg_samples_per_context):\n",
    "    #                 # generate a random index between 0 and vocab_size\n",
    "    #                 negative_word = random.randint(0, vocab_size-1)\n",
    "    #                 X.append([word2idx[target_word], negative_word])                \n",
    "    #                 y.append(0)\n",
    "    # return X, y\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "X, y = create_data_with_negative_sampling(sentences, word2idx, window_size, num_neg_samples_per_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# split the data into train and test\n",
    "\n",
    "# save the data to a file so that it can be loaded later\n",
    "# np.savez('data.npz', X=X, y=y)\n",
    "\n",
    "# load the data from the file\n",
    "def load_data(filename):\n",
    "    data = np.load(filename)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    return X, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 0\n",
      "Number of labels: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<unk>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of labels: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(y)))\n\u001b[1;32m      4\u001b[0m \u001b[39m# print(vocab_indices)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mindex of <unk> is: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(word2idx[\u001b[39m'\u001b[39;49m\u001b[39m<unk>\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mindex of <pad> is: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(word2idx[\u001b[39m'\u001b[39m\u001b[39m<pad>\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (\u001b[39m50\u001b[39m):\n",
      "\u001b[0;31mKeyError\u001b[0m: '<unk>'"
     ]
    }
   ],
   "source": [
    "print('Number of data points: {}'.format(len(X)))\n",
    "print('Number of labels: {}'.format(len(y)))\n",
    "\n",
    "# print(vocab_indices)\n",
    "print('index of <unk> is: {}'.format(word2idx['<unk>']))\n",
    "print('index of <pad> is: {}'.format(word2idx['<pad>']))\n",
    "\n",
    "for i in range (50):\n",
    "    print('{}   {}'.format(X[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow with negative sampling\n",
    "# hyperparameters\n",
    "embedding_size = 100\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# initialize the weights\n",
    "# embedding matrix\n",
    "# embeddings = np.random.uniform(-1, 1, (len(vocabulary), embedding_size))\n",
    "\n",
    "# use the same embedding matrix for both context and target\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# create batches\n",
    "def create_batches(X, y, batch_size):\n",
    "    batches = []\n",
    "    num_batches = len(X) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch = (X[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size])\n",
    "        batches.append(batch)\n",
    "    return batches       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.38412303612435933\n",
      "Epoch: 1, Loss: 0.25830396371112274\n",
      "Epoch: 2, Loss: 0.24422869358112104\n",
      "Epoch: 3, Loss: 0.2395569920404676\n",
      "Epoch: 4, Loss: 0.23916197209992945\n",
      "Epoch: 5, Loss: 0.2401143232171849\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "'''\n",
    "first, we get the embeddings of the context words and the target word\n",
    "we then average the embeddings of the context words to get the context embedding\n",
    "we then take the cosine similarity between the context embedding and the target embedding\n",
    "we then use the sigmoid function to get the probability of the target word being the correct word for the context words\n",
    "we then calculate the loss by subtracting the probability from the actual label\n",
    "we then backpropagate the loss to update the weights\n",
    "'''\n",
    "# we can use tensors to perform the operations\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# initialize the weights\n",
    "# embedding matrix\n",
    "# embeddings = torch.randn(len(vocabulary), embedding_size, requires_grad=True)\n",
    "embeddings = torch.randn(len(vocabulary), embedding_size, requires_grad=True, device=device)\n",
    "# embeddings = torch.zeros(len(vocabulary), embedding_size, requires_grad=True, device=device)\n",
    "\n",
    "# embeddings = embeddings.to(device)\n",
    "\n",
    "# # write a manual loss function\n",
    "# def loss_fn(y_pred, y):\n",
    "#     return torch.sum(y - y_pred)\n",
    "\n",
    "# write a function to train the model using gpu\n",
    "\n",
    "def train(X, y, embeddings, learning_rate, epochs, batch_size):\n",
    "    # convert X and y to torch tensors\n",
    "    X = torch.LongTensor(X)\n",
    "    X = X.to(device)\n",
    "    y = torch.FloatTensor(y)\n",
    "    y = y.to(device)\n",
    "    # create batches\n",
    "    batches = create_batches(X, y, batch_size)\n",
    "    # create an optimizer\n",
    "    optimizer = torch.optim.Adam([embeddings], lr=learning_rate)\n",
    "    # create a loss function for regression\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    prev_loss = 1000\n",
    "    # train the model\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in batches:\n",
    "            # get the embeddings of the context words and the target word\n",
    "            # print(X_batch.shape)\n",
    "            context_embeddings = embeddings[X_batch[:, :-1]]\n",
    "            target_embeddings = embeddings[X_batch[:, -1]]\n",
    "            # context_embeddings = embeddings[X_batch[:, 1]]\n",
    "            # target_embeddings = embeddings[X_batch[:, 0]]\n",
    "            # print(context_embeddings.shape)\n",
    "            # print(target_embeddings.shape)\n",
    "\n",
    "            # average the context embeddings\n",
    "            context_embeddings = torch.mean(context_embeddings, dim=1)\n",
    "\n",
    "            # calculate the dot product between the context embedding and the target embedding\n",
    "            logits = torch.sum(context_embeddings * target_embeddings, dim=1)\n",
    "\n",
    "            \n",
    "            # # normalize the logits\n",
    "            # logits = logits / (torch.norm(context_embeddings, dim=1) * torch.norm(target_embeddings, dim=1))\n",
    "            # use the sigmoid function to get the probability of the target word being the correct word for the context words\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = loss_fn(probs, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "            # backpropagate the loss to update the weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print('Epoch: {}, Loss: {}'.format(epoch, epoch_loss/len(batches)))\n",
    "        if epoch_loss/len(batches) > prev_loss:\n",
    "            break\n",
    "        prev_loss = epoch_loss/len(batches)\n",
    "\n",
    "\n",
    "# train the model\n",
    "train(X, y, embeddings, learning_rate, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'camera'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m             most_similar\u001b[39m.\u001b[39mappend([idx2word[idx], similarities[idx]\u001b[39m.\u001b[39mitem()])\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m most_similar\n\u001b[0;32m---> 16\u001b[0m sim_words \u001b[39m=\u001b[39m most_similar(\u001b[39m'\u001b[39;49m\u001b[39mcamera\u001b[39;49m\u001b[39m'\u001b[39;49m, embeddings, \u001b[39m10\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m word, similarity \u001b[39min\u001b[39;00m sim_words:\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(word, similarity))\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mmost_similar\u001b[0;34m(word, embeddings, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmost_similar\u001b[39m(word, embeddings, k):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# get the embedding of the word\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     word_embedding \u001b[39m=\u001b[39m embeddings[word2idx[word]]\n\u001b[1;32m      5\u001b[0m     \u001b[39m# calculate the cosine similarity between the word embedding and the embeddings of all the words\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     similarities \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(word_embedding, embeddings\u001b[39m.\u001b[39mT)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'camera'"
     ]
    }
   ],
   "source": [
    "# print 10 most similar words to a given word\n",
    "def most_similar(word, embeddings, k):\n",
    "    # get the embedding of the word\n",
    "    word_embedding = embeddings[word2idx[word]]\n",
    "    # calculate the cosine similarity between the word embedding and the embeddings of all the words\n",
    "    similarities = torch.matmul(word_embedding, embeddings.T)\n",
    "    # get the k most similar words\n",
    "    top_k = torch.topk(similarities, k+1)[1].tolist()\n",
    "    most_similar = []\n",
    "    for idx in top_k:\n",
    "        if idx != word2idx[word]:\n",
    "            most_similar.append([idx2word[idx], similarities[idx].item()])\n",
    "    return most_similar\n",
    "\n",
    "\n",
    "sim_words = most_similar('camera', embeddings, 10)\n",
    "for word, similarity in sim_words:\n",
    "    print('{}\\t\\t{}'.format(word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embeddings\n",
    "def save_embeddings(embeddings, filename):\n",
    "    embeddings = embeddings.cpu().detach().numpy()\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "# load the embeddings\n",
    "def load_embeddings(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_embeddings(embeddings, 'embeddings.pkl')\n",
    "# embeddings = load_embeddings('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embeddings('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
