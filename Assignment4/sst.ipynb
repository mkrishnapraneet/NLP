{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torchtext\n",
    "\n",
    "# get dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd0fbb3e28d412da789c99e30ec6b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sst_dataset = load_dataset(\"sst\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (8544, 4), 'validation': (1101, 4), 'test': (2210, 4)}\n",
      "(8544, 4)\n",
      "(1101, 4)\n",
      "(2210, 4)\n"
     ]
    }
   ],
   "source": [
    "print(sst_dataset.shape)\n",
    "train_dataset = sst_dataset[\"train\"]\n",
    "validation_dataset = sst_dataset[\"validation\"]\n",
    "test_dataset = sst_dataset[\"test\"]\n",
    "print(train_dataset.shape)\n",
    "print(validation_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-bb5f67f2d04b9322.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-bfbc96d0e7322a91.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-010f98f8730a3aa6.arrow\n"
     ]
    }
   ],
   "source": [
    "# print(train_dataset[0])\n",
    "\n",
    "# use nltk to tokenize the sentences\n",
    "train_dataset = train_dataset.map(lambda example: {'tokens': word_tokenize(example['sentence'])})\n",
    "validation_dataset = validation_dataset.map(lambda example: {'tokens': word_tokenize(example['sentence'])})\n",
    "test_dataset = test_dataset.map(lambda example: {'tokens': word_tokenize(example['sentence'])})\n",
    "# print(train_dataset[0])\n",
    "# print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-37e9605f1b6ee538.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-55d090e59212e70f.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-a723b3a64961bd66.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 8219\n",
      "['<unk>', '<pad>', '.', 'the', ',', 'a', 'and', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "\n",
    "# convert all tokens to lowercase\n",
    "train_dataset = train_dataset.map(lambda example: {'tokens': [token.lower() for token in example['tokens']]})\n",
    "validation_dataset = validation_dataset.map(lambda example: {'tokens': [token.lower() for token in example['tokens']]})\n",
    "test_dataset = test_dataset.map(lambda example: {'tokens': [token.lower() for token in example['tokens']]})\n",
    "\n",
    "vocabulary = torchtext.vocab.build_vocab_from_iterator(train_dataset['tokens'], specials=[\"<unk>\", \"<pad>\"], min_freq=2)\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "print(f'Length of vocabulary: {len(vocabulary)}')\n",
    "print(vocabulary.get_itos()[:10])\n",
    "# print(vocabulary.get_stoi()[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-5686a57f19ff4596.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-3835c615e5dd4f80.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-770ed8583262ef70.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'label': 0.6944400072097778, 'tokens': [3, 612, 9, 3633, 8, 25, 3, 2832, 1280, 10, 106, 32, 6047, 32, 6, 12, 73, 10, 273, 8, 82, 5, 5359, 67, 3778, 40, 1882, 3292, 4, 0, 1859, 6160, 48, 892, 0, 2], 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n",
      "Largest sentence in training set has 53 tokens\n",
      "Average sentence in training set has 19.157069288389515 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-8f3559959a6891bb.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-0749e7c625bcd606.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-7c3007290063b89c.arrow\n",
      "Loading cached processed dataset at /home/prani/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-39c1fefd9cffbbef.arrow\n"
     ]
    }
   ],
   "source": [
    "# convert tokens to indices\n",
    "train_indices = train_dataset.map(lambda example: {'tokens': [vocabulary[token] for token in example['tokens']]})\n",
    "validation_indices = validation_dataset.map(lambda example: {'tokens': [vocabulary[token] for token in example['tokens']]})\n",
    "test_indices = test_dataset.map(lambda example: {'tokens': [vocabulary[token] for token in example['tokens']]})\n",
    "print(train_indices[0])\n",
    "print(f'Largest sentence in training set has {max([len(example[\"tokens\"]) for example in train_indices])} tokens')\n",
    "print(f'Average sentence in training set has {np.mean([len(example[\"tokens\"]) for example in train_indices])} tokens')\n",
    "\n",
    "# pad the sentences\n",
    "pad_index = vocabulary[\"<pad>\"]\n",
    "# insert the pad token at the end of each sentence until it reaches the max length\n",
    "max_length = max([len(example[\"tokens\"]) for example in train_indices])\n",
    "\n",
    "# if any sentence in test set is longer than max_length, truncate it\n",
    "test_indices = test_indices.map(lambda example: {'tokens': example['tokens'][:max_length]})\n",
    "\n",
    "\n",
    "train_indices = train_indices.map(lambda example: {'tokens': example['tokens'] + [pad_index] * (max_length - len(example['tokens']))})\n",
    "validation_indices = validation_indices.map(lambda example: {'tokens': example['tokens'] + [pad_index] * (max_length - len(example['tokens']))})\n",
    "test_indices = test_indices.map(lambda example: {'tokens': example['tokens'] + [pad_index] * (max_length - len(example['tokens']))})\n",
    "# print(train_indices[0])\n",
    "# print(test_indices[0])\n",
    "# print(len(train_indices[0]['tokens']))\n",
    "# print(len(test_indices[2209]['tokens']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# print the lengths of all the token lists in the test set\n",
    "# lister = ([len(example['tokens']) for example in test_indices])\n",
    "\n",
    "# for el in lister:\n",
    "#     if el != 53:\n",
    "#         print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_indices['tokens'][0]))\n",
    "# print(len(test_indices['tokens'][0]))\n",
    "\n",
    "train_indices_only =        np.array(train_indices[\"tokens\"])\n",
    "validation_indices_only =   np.array(validation_indices[\"tokens\"])\n",
    "test_indices_only =         np.array(test_indices[\"tokens\"])\n",
    "\n",
    "# print(train_indices_only[0].shape)\n",
    "# print(test_indices_only[0].shape)\n",
    "# print(test_indices_only.shape)\n",
    "# # reshape the test_indices_only\n",
    "# test_indices_only = test_indices_only.reshape(-1, max_length)\n",
    "\n",
    "# print(train_indices_only.shape)\n",
    "# print(test_indices_only.shape)\n",
    "\n",
    "train_labels_only =         np.array(train_dataset[\"label\"])\n",
    "validation_labels_only =    np.array(validation_dataset[\"label\"])\n",
    "test_labels_only =          np.array(test_dataset[\"label\"])\n",
    "\n",
    "# if label is greater than 0.5, then make it 1, else make it 0\n",
    "train_labels_only = np.where(train_labels_only >= 0.5, 1, 0)\n",
    "validation_labels_only = np.where(validation_labels_only >= 0.5, 1, 0)\n",
    "test_labels_only = np.where(test_labels_only >= 0.5, 1, 0)\n",
    "\n",
    "# print(train_indices_only.shape)\n",
    "# print(test_indices_only.shape)\n",
    "# # print(train_labels_only.shape)\n",
    "# print(test_labels_only.shape)\n",
    "\n",
    "# print(train_indices_only[0])\n",
    "# print(test_indices_only[0])\n",
    "\n",
    "# batch the data\n",
    "batch_size = 32\n",
    "train_data = DataLoader(list(zip(train_indices_only, train_labels_only)), batch_size=batch_size, shuffle=True)\n",
    "validation_data = DataLoader(list(zip(validation_indices_only, validation_labels_only)), batch_size=batch_size, shuffle=True)\n",
    "test_data = DataLoader(list(zip(test_indices_only, test_labels_only)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # check the data\n",
    "# for batch in test_data:\n",
    "#     print(batch[0])\n",
    "#     print(batch[1])\n",
    "#     print(batch[0].shape)\n",
    "#     print(batch[1].shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model as a sentence classifier using ELMO embeddings using two biLSTM layers and use glove embeddings for the words\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes=2, num_layers=2, dropout=0.5):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.lstm1f = nn.LSTM(self.embedding_dim, self.hidden_dim, 1, bidirectional=False, batch_first=True, dropout=self.dropout)\n",
    "        self.lstm1b = nn.LSTM(self.embedding_dim, self.hidden_dim, 1, bidirectional=False, batch_first=True, dropout=self.dropout)\n",
    "        self.lstm2f = nn.LSTM(self.hidden_dim, self.hidden_dim, 1, bidirectional=False, batch_first=True, dropout=self.dropout)\n",
    "        self.lstm2b = nn.LSTM(self.hidden_dim, self.hidden_dim, 1, bidirectional=False, batch_first=True, dropout=self.dropout)\n",
    "        # self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, bidirectional=True, dropout=self.dropout)\n",
    "        self.linear = nn.Linear(self.hidden_dim * 4 + self.embedding_dim, self.num_classes)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch_size, max_length)\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded.shape = (batch_size, max_length, embedding_dim)\n",
    "        # print(embedded.shape)\n",
    "        output1f, (hidden1f, cell1f) = self.lstm1f(embedded)\n",
    "        output1b, (hidden1b, cell1b) = self.lstm1b(embedded.flip(dims = [1]))\n",
    "        # output1b, (hidden1b, cell1b) = self.lstm1b(embedded)\n",
    "        output2f, (hidden2f, cell2f) = self.lstm2f(output1f)\n",
    "        output2b, (hidden2b, cell2b) = self.lstm2b(output1b)\n",
    "        # output.shape = (batch_size, max_length, hidden_dim)\n",
    "        \n",
    "        # concatenate the forward and backward outputs for both layers\n",
    "        output1 = torch.cat((output1f, output1b), dim=2)\n",
    "        output2 = torch.cat((output2f, output2b), dim=2)\n",
    "        # output.shape = (batch_size, max_length, hidden_dim * 2)\n",
    "\n",
    "        # pass these outputs through a linear layer along with the initial embedding\n",
    "        output = torch.cat((output1, output2, embedded), dim=2)\n",
    "        # output.shape = (batch_size, max_length, hidden_dim * 4 + embedding_dim)\n",
    "        output = self.linear(output)\n",
    "        # output.shape = (batch_size, max_length, num_classes)\n",
    "        output = output[:, -1, :]\n",
    "        # output.shape = (batch_size, num_classes)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes=2, num_layers=2, dropout=0.5):\n",
    "    #     super(SentenceClassifier, self).__init__()\n",
    "    #     self.embedding_dim = embedding_dim\n",
    "    #     self.hidden_dim = hidden_dim\n",
    "    #     self.vocab_size = vocab_size\n",
    "    #     self.num_classes = num_classes\n",
    "    #     self.num_layers = num_layers\n",
    "    #     self.dropout = dropout\n",
    "\n",
    "    #     self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "    #     self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, bidirectional=True, batch_first=True, dropout=self.dropout)\n",
    "    #     # self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, bidirectional=True, dropout=self.dropout)\n",
    "    #     self.linear = nn.Linear(self.hidden_dim * 2, self.num_classes)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # x.shape = (batch_size, sequence_length)\n",
    "    #     embedded = self.embedding(x)\n",
    "    #     # embedded.shape = (batch_size, sequence_length, embedding_dim)\n",
    "    #     output, (hidden, cell) = self.lstm(embedded)\n",
    "    #     # output.shape = (batch_size, sequence_length, hidden_dim * 2)\n",
    "    #     # hidden.shape = (num_layers * 2, batch_size, hidden_dim)\n",
    "    #     # cell.shape = (num_layers * 2, batch_size, hidden_dim)\n",
    "    #     hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "    #     # hidden.shape = (batch_size, hidden_dim * 2)\n",
    "    #     return self.linear(hidden)\n",
    "    \n",
    "    # def predict(self, x):\n",
    "    #     logits = self.forward(x)\n",
    "    #     return F.softmax(logits, dim=1)\n",
    "    \n",
    "    # def accuracy(self, y_pred, y_true):\n",
    "    #     return torch.sum(torch.argmax(y_pred, dim=1) == y_true) / len(y_true)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "vocab_size = len(vocabulary)\n",
    "num_classes = 2\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# initialize the model\n",
    "model = SentenceClassifier(embedding_dim, hidden_dim, vocab_size, num_classes, num_layers, dropout)\n",
    "model.to(device)\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Step: 0, Loss: 0.6982\n",
      "Epoch: 1/10, Step: 100, Loss: 0.7196\n",
      "Epoch: 1/10, Step: 200, Loss: 0.7167\n",
      "Epoch: 1/10, Validation Accuracy: 56.31%\n",
      "Epoch: 2/10, Step: 0, Loss: 0.7222\n",
      "Epoch: 2/10, Step: 100, Loss: 0.6523\n",
      "Epoch: 2/10, Step: 200, Loss: 0.5753\n",
      "Epoch: 2/10, Validation Accuracy: 67.76%\n",
      "Epoch: 3/10, Step: 0, Loss: 0.5481\n",
      "Epoch: 3/10, Step: 100, Loss: 0.4813\n",
      "Epoch: 3/10, Step: 200, Loss: 0.5190\n",
      "Epoch: 3/10, Validation Accuracy: 70.94%\n",
      "Epoch: 4/10, Step: 0, Loss: 0.4135\n",
      "Epoch: 4/10, Step: 100, Loss: 0.5315\n",
      "Epoch: 4/10, Step: 200, Loss: 0.3137\n",
      "Epoch: 4/10, Validation Accuracy: 71.12%\n",
      "Epoch: 5/10, Step: 0, Loss: 0.2442\n",
      "Epoch: 5/10, Step: 100, Loss: 0.3515\n",
      "Epoch: 5/10, Step: 200, Loss: 0.3524\n",
      "Epoch: 5/10, Validation Accuracy: 71.93%\n",
      "Epoch: 6/10, Step: 0, Loss: 0.2637\n",
      "Epoch: 6/10, Step: 100, Loss: 0.2783\n",
      "Epoch: 6/10, Step: 200, Loss: 0.2601\n",
      "Epoch: 6/10, Validation Accuracy: 71.48%\n",
      "Epoch: 7/10, Step: 0, Loss: 0.2151\n",
      "Epoch: 7/10, Step: 100, Loss: 0.2039\n",
      "Epoch: 7/10, Step: 200, Loss: 0.2535\n",
      "Epoch: 7/10, Validation Accuracy: 71.03%\n",
      "Epoch: 8/10, Step: 0, Loss: 0.1792\n",
      "Epoch: 8/10, Step: 100, Loss: 0.2020\n",
      "Epoch: 8/10, Step: 200, Loss: 0.3768\n",
      "Epoch: 8/10, Validation Accuracy: 71.93%\n",
      "Epoch: 9/10, Step: 0, Loss: 0.2330\n",
      "Epoch: 9/10, Step: 100, Loss: 0.1798\n",
      "Epoch: 9/10, Step: 200, Loss: 0.2854\n",
      "Epoch: 9/10, Validation Accuracy: 71.75%\n",
      "Epoch: 10/10, Step: 0, Loss: 0.1114\n",
      "Epoch: 10/10, Step: 100, Loss: 0.2820\n",
      "Epoch: 10/10, Step: 200, Loss: 0.2176\n",
      "Epoch: 10/10, Validation Accuracy: 70.94%\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch, (sentences, labels) in enumerate(train_data):\n",
    "        sentences = sentences.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(sentences)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch: {epoch + 1}/{num_epochs}, Step: {batch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for sentences, labels in validation_data:\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(sentences)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "\n",
    "        print(f'Epoch: {epoch + 1}/{num_epochs}, Validation Accuracy: {(correct / total) * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.49%\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for sentences, labels in test_data:\n",
    "        # print(sentences)\n",
    "        sentences = sentences.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(sentences)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {(correct / total) * 100:.2f}%')\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
