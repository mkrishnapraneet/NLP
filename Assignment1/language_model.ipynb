{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def tokenize(text, n):\n",
    "\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    final_tokens = []\n",
    "    one_word_hist_table = {}\n",
    "    one_hist_word_table = {}\n",
    "\n",
    "    two_word_hist_table = {}\n",
    "    two_hist_word_table = {}\n",
    "\n",
    "    three_word_hist_table = {}\n",
    "    three_hist_word_table = {}\n",
    "\n",
    "    four_word_hist_table = {}\n",
    "    four_hist_word_table = {}\n",
    "\n",
    "    his = tuple()\n",
    "    one_word_hist_table[his] = {}\n",
    "    one_word_hist_table[his]['<unk>'] = 1\n",
    "    one_hist_word_table['<unk>'] = {}\n",
    "    one_hist_word_table['<unk>'][his] = 1\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # print(sentence)\n",
    "        text = sentence\n",
    "        # split into tokens by white space\n",
    "        tokens = text.split()\n",
    "\n",
    "        # if it is a url, replace it with <url>\n",
    "        tokens = ['<url>' if re.match(\n",
    "            r'^https?:\\/\\/.*[\\r\\n]*$', word) else word for word in tokens]\n",
    "        # if it is a number, replace it with <number>\n",
    "        tokens = ['<number>' if re.match(\n",
    "            r'^\\d+$', word) else word for word in tokens]\n",
    "        # if it is a word with only digits, replace it with <number>\n",
    "        # tokens = ['<number>' if re.match(r'^\\d+\\w+$', word) else word for word in tokens]\n",
    "        # if it is a mention, replace it with <mention>\n",
    "        tokens = ['<mention>' if re.match(\n",
    "            r'^@\\w+$', word) else word for word in tokens]\n",
    "        # if it is a hashtag, replace it with <hashtag>\n",
    "        tokens = ['<hashtag>' if re.match(\n",
    "            r'^#\\w+$', word) else word for word in tokens]\n",
    "\n",
    "        # make separate tokens for punctuations and keep for special tokens like <url>, <number>, <mention>, <hashtag>\n",
    "        # tokens = [re.split('(\\W+)', word) for word in tokens]\n",
    "        tokens = [re.split('(\\W+)', word) if (word != '<url>' and word != '<number>' and word !=\n",
    "                                            '<mention>' and word != '<hashtag>') else [word] for word in tokens]\n",
    "        # tokens = [tok for word in tokens for tok in re.split('(\\W+)', word) if (word != '<url>' and word != '<number>' and word != '<mention>' and word != '<hashtag>')]\n",
    "\n",
    "        # flatten the elements\n",
    "        tokens = [tok for word in tokens for tok in word]\n",
    "\n",
    "        # remove empty tokens\n",
    "        tokens = [tok for tok in tokens if len(tok) > 0]\n",
    "\n",
    "        # unigram model\n",
    "        # make a dict for each word\n",
    "        for i in range(1-1, len(tokens)):\n",
    "            # store the previous n-1 words as history\n",
    "            history = tuple(tokens[i-1+1:i])\n",
    "            word = tokens[i]\n",
    "            # print(history, word)\n",
    "            # if the history is not in the dict, add it\n",
    "            if history not in one_word_hist_table:\n",
    "                one_word_hist_table[history] = {}\n",
    "            # if the word is not in the dict, add it\n",
    "            if word not in one_word_hist_table[history]:\n",
    "                one_word_hist_table[history][word] = 0\n",
    "            # increment the count\n",
    "            one_word_hist_table[history][word] += 1\n",
    "\n",
    "        # make a dict for each history\n",
    "        for i in range(1-1, len(tokens)):\n",
    "            # store the previous n-1 words as history\n",
    "            history = tuple(tokens[i-1+1:i])\n",
    "            word = tokens[i]\n",
    "            # print(history, word)\n",
    "            # if the history is not in the dict, add it\n",
    "            if word not in one_hist_word_table:\n",
    "                one_hist_word_table[word] = {}\n",
    "            # if the word is not in the dict, add it\n",
    "            if history not in one_hist_word_table[word]:\n",
    "                one_hist_word_table[word][history] = 0\n",
    "            # increment the count\n",
    "            one_hist_word_table[word][history] += 1\n",
    "\n",
    "        # for i in range(n-1):\n",
    "        #     # add start tokens\n",
    "        #     tokens.insert(0, '<start>')\n",
    "        #     # add end tokens\n",
    "        #     tokens.append('<end>')\n",
    "\n",
    "        # add start tokens\n",
    "        tokens.insert(0, '<start>')\n",
    "        # add end tokens\n",
    "        tokens.append('<end>')\n",
    "\n",
    "        # bigram model\n",
    "        # make a dict for each word\n",
    "        for i in range(2-1, len(tokens)):\n",
    "            # store the previous n-1 words as history\n",
    "            history = tuple(tokens[i-2+1:i])\n",
    "            word = tokens[i]\n",
    "            # print(history, word)\n",
    "            # if the history is not in the dict, add it\n",
    "            if history not in two_word_hist_table:\n",
    "                two_word_hist_table[history] = {}\n",
    "            # if the word is not in the dict, add it\n",
    "            if word not in two_word_hist_table[history]:\n",
    "                two_word_hist_table[history][word] = 0\n",
    "            # increment the count\n",
    "            two_word_hist_table[history][word] += 1\n",
    "\n",
    "        # make a dict for each history\n",
    "        for i in range(2-1, len(tokens)):\n",
    "            # store the previous n-1 words as history\n",
    "            history = tuple(tokens[i-2+1:i])\n",
    "            word = tokens[i]\n",
    "            # print(history, word)\n",
    "            # if the history is not in the dict, add it\n",
    "            if word not in two_hist_word_table:\n",
    "                two_hist_word_table[word] = {}\n",
    "            # if the word is not in the dict, add it\n",
    "            if history not in two_hist_word_table[word]:\n",
    "                two_hist_word_table[word][history] = 0\n",
    "            # increment the count\n",
    "            two_hist_word_table[word][history] += 1\n",
    "        \n",
    "        # add start tokens\n",
    "        tokens.insert(0, '<start>')\n",
    "        # add end tokens\n",
    "        tokens.append('<end>')\n",
    "\n",
    "        # trigram model\n",
    "        # make a dict for each word\n",
    "        for i in range(3-1, len(tokens)):\n",
    "            # store the previous n-1 words as history\n",
    "            history = tuple(tokens[i-3+1:i])\n",
    "            word = tokens[i]\n",
    "            # print(history, word)\n",
    "            # if the history is not in the dict, add it\n",
    "            if history not in three_word_hist_table:\n",
    "                three_word_hist_table[history] = {}\n",
    "            # if the word is not in the dict, add it\n",
    "            if word not in three_word_hist_table[history]:\n",
    "                three_word_hist_table[history][word] = 0\n",
    "            # increment the count\n",
    "            three_word_hist_table[history][word] += 1\n",
    "\n",
    "        # make a dict for each history\n",
    "        for i in range(3-1, len(tokens)):\n",
    "            # store the previous n-1 words as history\n",
    "            history = tuple(tokens[i-3+1:i])\n",
    "            word = tokens[i]\n",
    "            # print(history, word)\n",
    "            # if the history is not in the dict, add it\n",
    "            if word not in three_hist_word_table:\n",
    "                three_hist_word_table[word] = {}\n",
    "            # if the word is not in the dict, add it\n",
    "            if history not in three_hist_word_table[word]:\n",
    "                three_hist_word_table[word][history] = 0\n",
    "            # increment the count\n",
    "            three_hist_word_table[word][history] += 1\n",
    "        \n",
    "        # add start tokens\n",
    "        tokens.insert(0, '<start>')\n",
    "        # add end tokens\n",
    "        tokens.append('<end>')\n",
    "\n",
    "        # 4gram model\n",
    "        # make a dict for each word\n",
    "        for i in range(4-1, len(tokens)):\n",
    "            # store the previous n-1 words as history\n",
    "            history = tuple(tokens[i-4+1:i])\n",
    "            word = tokens[i]\n",
    "            # print(history, word)\n",
    "            # if the history is not in the dict, add it\n",
    "            if history not in four_word_hist_table:\n",
    "                four_word_hist_table[history] = {}\n",
    "            # if the word is not in the dict, add it\n",
    "            if word not in four_word_hist_table[history]:\n",
    "                four_word_hist_table[history][word] = 0\n",
    "            # increment the count\n",
    "            four_word_hist_table[history][word] += 1\n",
    "\n",
    "        # make a dict for each history\n",
    "        for i in range(4-1, len(tokens)):\n",
    "            # store the previous n-1 words as history\n",
    "            history = tuple(tokens[i-4+1:i])\n",
    "            word = tokens[i]\n",
    "            # print(history, word)\n",
    "            # if the history is not in the dict, add it\n",
    "            if word not in four_hist_word_table:\n",
    "                four_hist_word_table[word] = {}\n",
    "            # if the word is not in the dict, add it\n",
    "            if history not in four_hist_word_table[word]:\n",
    "                four_hist_word_table[word][history] = 0\n",
    "            # increment the count\n",
    "            four_hist_word_table[word][history] += 1\n",
    "\n",
    "        final_tokens.append(tokens)\n",
    "        # print(tokens)\n",
    "\n",
    "    # print(word_hist_table)\n",
    "    # print(hist_word_table)\n",
    "            \n",
    "                \n",
    "\n",
    "    # print(len(final_tokens))\n",
    "\n",
    "    # print(final_tokens[:100])\n",
    "\n",
    "    return final_tokens, one_word_hist_table, one_hist_word_table, two_word_hist_table, two_hist_word_table, three_word_hist_table, three_hist_word_table, four_word_hist_table, four_hist_word_table\n",
    "\n",
    "\n",
    "# issues : continuous punctuations are not separated\n",
    "\n",
    "path_to_corpus = 'test.txt'\n",
    "# path_to_corpus = 'Pride and Prejudice - Jane Austen.txt'\n",
    "\n",
    "n = 4\n",
    "with open(path_to_corpus, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "t,t,t,t,t,t,t,t,t = tokenize(text, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# witten bell smoothing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def n_gram_probabilities(n, word_hist_table, hist_word_table, input_tokens, i):\n",
    "    # prob = count(history, word) / count(history)\n",
    "    hist_to_compare = tuple(input_tokens[i-n+1:i])\n",
    "    word_to_compare = input_tokens[i]\n",
    "    # print(hist_to_compare, word_to_compare)\n",
    "\n",
    "    # if the history is not in the dict, return 0\n",
    "    if hist_to_compare not in word_hist_table:\n",
    "        return 0\n",
    "    # if the word is not in the dict, return 0\n",
    "    if word_to_compare not in word_hist_table[hist_to_compare]:\n",
    "        if(n == 1):\n",
    "            word_to_compare = '<unk>'\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # count(history, word) / count(history)\n",
    "    prob = word_hist_table[hist_to_compare][word_to_compare] / sum(word_hist_table[hist_to_compare].values())\n",
    "    # print(prob)\n",
    "    return prob    \n",
    "\n",
    "def send_correct_model(n):\n",
    "    if n == 1:\n",
    "        return one_word_hist_table, one_hist_word_table\n",
    "    elif n == 2:\n",
    "        return two_word_hist_table, two_hist_word_table\n",
    "    elif n == 3:\n",
    "        return three_word_hist_table, three_hist_word_table\n",
    "    elif n == 4:\n",
    "        return four_word_hist_table, four_hist_word_table\n",
    "    \n",
    "\n",
    "    \n",
    "n = 4\n",
    "\n",
    "# input_sentence = \"One can not decipher a man's feelings\"\n",
    "# input_sentence = \"This was invitation enough.\"\n",
    "# input_sentence = \"This was not invitation enough.\"\n",
    "# input_sentence = \"That is an evening gamer.\"\n",
    "# input_sentence = \"The boy bought a chocolate.\"\n",
    "# input_sentence = \"I sneezed loudly.\"\n",
    "input_sentence = \"He slid it into the left slot for them.\"\n",
    "\n",
    "# f_tokens, word_hist_table, hist_word_table = tokenize(input_sentence, n)\n",
    "input_tokens, ig, fig, dig, hig, gig, hig, rig, wig = tokenize(input_sentence, n)\n",
    "# print(input_tokens[0])\n",
    "\n",
    "# corpus\n",
    "# path_to_corpus = 'test.txt'\n",
    "# path_to_corpus = 'Pride and Prejudice - Jane Austen.txt'\n",
    "path_to_corpus = 'Ulysses - James Joyce.txt'\n",
    "\n",
    "with open(path_to_corpus, 'r') as f:\n",
    "    text = f.read()\n",
    "# text = \"The boy ate a chocolate. The girl bought a chocolate. The girl ate a chocolate. The boy bought a horse.\"\n",
    "corpus_tokens, one_word_hist_table, one_hist_word_table, two_word_hist_table, two_hist_word_table, three_word_hist_table, three_hist_word_table, four_word_hist_table, four_hist_word_table = tokenize(text, n)\n",
    "\n",
    "# n = 2\n",
    "# total_prob = 1\n",
    "# for i in range(n-1, len(input_tokens[0])):\n",
    "#     cur = (n_gram_probabilities(n, *send_correct_model(n), input_tokens[0], i))\n",
    "#     print(cur)\n",
    "#     total_prob *= cur\n",
    "# print(\"total prob = \" + str(total_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lambda(n, word_hist_table, hist_word_table, input_tokens, i):\n",
    "    # calculate the distinct nth words for the history\n",
    "    hist_to_compare = tuple(input_tokens[i-n+1:i])\n",
    "    word_to_compare = input_tokens[i]\n",
    "    # print(hist_to_compare)\n",
    "\n",
    "    # if the history is not in the dict, return 0\n",
    "    if hist_to_compare not in word_hist_table:\n",
    "        return 0\n",
    "    # if the word is not in the dict, return 0\n",
    "    # if word_to_compare not in word_hist_table[hist_to_compare]:\n",
    "    #     return 0\n",
    "    distinct_nth_words = len(word_hist_table[hist_to_compare])\n",
    "    # print(distinct_nth_words)\n",
    "    count_hist = sum(word_hist_table[hist_to_compare].values())\n",
    "    # print(count_hist)\n",
    "\n",
    "    _lambda = 1 - (distinct_nth_words / (distinct_nth_words + count_hist))\n",
    "    # print(_lambda)\n",
    "    return _lambda\n",
    "    \n",
    "   \n",
    "\n",
    "def witten_bell(n, word_hist_table, hist_word_table, input_tokens, i):\n",
    "    if n == 1:\n",
    "        return n_gram_probabilities(n, *send_correct_model(n), input_tokens, i)\n",
    "    _lambda = calc_lambda(n, *send_correct_model(n), input_tokens, i)\n",
    "    return _lambda * n_gram_probabilities(n, *send_correct_model(n), input_tokens, i) + (1 - _lambda) * witten_bell(n-1, *send_correct_model(n-1), input_tokens, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prob of sentence (witten bell) = 7.759150361558879e-08\n",
      "perplexity of sentence (witten bell) = 2.782188905816717\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "total_prob = 1\n",
    "for i in range(n-1, len(input_tokens[0])):\n",
    "    cur = (witten_bell(n,*send_correct_model(n), input_tokens[0], i))\n",
    "    # print(cur)\n",
    "    total_prob *= cur\n",
    "print(\"total prob of sentence (witten bell) = \" + str(total_prob))\n",
    "if total_prob == 0:\n",
    "    print(\"perplexity of sentence (witten bell) = \" + str(\"inf\"))\n",
    "else:\n",
    "    print(\"perplexity of sentence (witten bell) = \" + str((1/total_prob)**(1/len(input_tokens[0]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kn_lambda(n, word_hist_table, hist_word_table, input_tokens, i, d):\n",
    "    # calculate the distinct nth words for the history\n",
    "    hist_to_compare = tuple(input_tokens[i-n+1:i])\n",
    "    word_to_compare = input_tokens[i]\n",
    "    # print(hist_to_compare)\n",
    "\n",
    "    # if the history is not in the dict, return 0\n",
    "    if hist_to_compare not in word_hist_table:\n",
    "        return 1\n",
    "    # if the word is not in the dict, return 0\n",
    "    if word_to_compare not in word_hist_table[hist_to_compare]:\n",
    "        if (n == 1):\n",
    "            word_to_compare = '<unk>'\n",
    "        else :\n",
    "            return 1\n",
    "    distinct_nth_words = len(word_hist_table[hist_to_compare])\n",
    "    # print(distinct_nth_words)\n",
    "    count_hist = sum(word_hist_table[hist_to_compare].values())\n",
    "    # print(count_hist)\n",
    "    return d * distinct_nth_words / count_hist\n",
    "\n",
    "    \n",
    "\n",
    "def kneser_ney(n, word_hist_table, hist_word_table, input_tokens, i, d=0.250):\n",
    "    if n == 1:\n",
    "        # check this later############################################################################\n",
    "        # return n_gram_probabilities(n, *send_correct_model(n), input_tokens, i) \n",
    "        # numerator is the number of distinct bigram histories the word appears in as the last word (use the hist_word_table)\n",
    "        numerator = 0\n",
    "        if input_tokens[i] in hist_word_table:\n",
    "            numerator = len(hist_word_table[input_tokens[i]])\n",
    "        else:\n",
    "            numerator = len(hist_word_table['<unk>'])\n",
    "        # denominator is the number of distinct bigrams\n",
    "        denominator = len(two_word_hist_table)\n",
    "        \n",
    "        return numerator / denominator\n",
    "\n",
    "\n",
    "    _lambda = calc_kn_lambda(n, *send_correct_model(n), input_tokens, i, d)\n",
    "    # word_hist_table, hist_word_table = send_correct_model(n)\n",
    "    first_term = 0\n",
    "    if tuple(input_tokens[i-n+1:i]) in word_hist_table:\n",
    "        if input_tokens[i] in word_hist_table[tuple(input_tokens[i-n+1:i])]:\n",
    "            # first term is max of 0 and the count of the ngram minus d divided by the count of the history\n",
    "            first_term = max(0, (word_hist_table[tuple(input_tokens[i-n+1:i])][input_tokens[i]] - d) / sum(word_hist_table[tuple(input_tokens[i-n+1:i])].values()))\n",
    "    return first_term + _lambda * kneser_ney(n-1, *send_correct_model(n-1), input_tokens, i, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prob of sentence (witten bell) = 5.406912173067445e-07\n",
      "perplexity of sentence (witten bell) = 2.4642834922908468\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "total_prob = 1\n",
    "for i in range(n-1, len(input_tokens[0])):\n",
    "    cur = (kneser_ney(n,*send_correct_model(n), input_tokens[0], i))\n",
    "    # print(cur)\n",
    "    total_prob *= cur\n",
    "print(\"total prob of sentence (kneser ney) = \" + str(total_prob))\n",
    "if total_prob == 0:\n",
    "    print(\"perplexity of sentence (kneser ney) = inf\")\n",
    "else:\n",
    "    print(\"perplexity of sentence (kneser ney) = \" + str((1/total_prob)**(1/len(input_tokens[0]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
