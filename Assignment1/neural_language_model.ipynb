{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['<start>', 'he', 'slid', 'it', 'into', 'the', 'left', 'slot', 'for', 'them', '.', '<end>']\n",
      "5974\n",
      "4181\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "\n",
    "\n",
    "def tokenize(text, n):\n",
    "\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    final_tokens = []\n",
    "    # one_word_hist_table = {}\n",
    "    # one_hist_word_table = {}\n",
    "\n",
    "    # two_word_hist_table = {}\n",
    "    # two_hist_word_table = {}\n",
    "\n",
    "    # three_word_hist_table = {}\n",
    "    # three_hist_word_table = {}\n",
    "\n",
    "    # four_word_hist_table = {}\n",
    "    # four_hist_word_table = {}\n",
    "\n",
    "    # his = tuple()\n",
    "    # one_word_hist_table[his] = {}\n",
    "    # one_word_hist_table[his]['<unk>'] = 1\n",
    "    # one_hist_word_table['<unk>'] = {}\n",
    "    # one_hist_word_table['<unk>'][his] = 1\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # print(sentence)\n",
    "        text = sentence\n",
    "        # split into tokens by white space\n",
    "        tokens = text.split()\n",
    "\n",
    "        # if it is a url, replace it with <url>\n",
    "        tokens = ['<url>' if re.match(\n",
    "            r'^https?:\\/\\/.*[\\r\\n]*$', word) else word for word in tokens]\n",
    "        # if it is a number, replace it with <number>\n",
    "        tokens = ['<number>' if re.match(\n",
    "            r'^\\d+$', word) else word for word in tokens]\n",
    "        # if it is a word with only digits, replace it with <number>\n",
    "        # tokens = ['<number>' if re.match(r'^\\d+\\w+$', word) else word for word in tokens]\n",
    "        # if it is a mention, replace it with <mention>\n",
    "        tokens = ['<mention>' if re.match(\n",
    "            r'^@\\w+$', word) else word for word in tokens]\n",
    "        # if it is a hashtag, replace it with <hashtag>\n",
    "        tokens = ['<hashtag>' if re.match(\n",
    "            r'^#\\w+$', word) else word for word in tokens]\n",
    "\n",
    "        # make separate tokens for punctuations and keep for special tokens like <url>, <number>, <mention>, <hashtag>\n",
    "        # tokens = [re.split('(\\W+)', word) for word in tokens]\n",
    "        tokens = [re.split('(\\W+)', word) if (word != '<url>' and word != '<number>' and word !=\n",
    "                                            '<mention>' and word != '<hashtag>') else [word] for word in tokens]\n",
    "        # tokens = [tok for word in tokens for tok in re.split('(\\W+)', word) if (word != '<url>' and word != '<number>' and word != '<mention>' and word != '<hashtag>')]\n",
    "\n",
    "        # flatten the elements\n",
    "        tokens = [tok for word in tokens for tok in word]\n",
    "\n",
    "        # remove empty tokens\n",
    "        tokens = [tok for tok in tokens if len(tok) > 0]\n",
    "\n",
    "        # unigram model\n",
    "        # make a dict for each word\n",
    "        # for i in range(1-1, len(tokens)):\n",
    "        #     # store the previous n-1 words as history\n",
    "        #     history = tuple(tokens[i-1+1:i])\n",
    "        #     word = tokens[i]\n",
    "        #     # print(history, word)\n",
    "        #     # if the history is not in the dict, add it\n",
    "        #     if history not in one_word_hist_table:\n",
    "        #         one_word_hist_table[history] = {}\n",
    "        #     # if the word is not in the dict, add it\n",
    "        #     if word not in one_word_hist_table[history]:\n",
    "        #         one_word_hist_table[history][word] = 0\n",
    "        #     # increment the count\n",
    "        #     one_word_hist_table[history][word] += 1\n",
    "\n",
    "        # # make a dict for each history\n",
    "        # for i in range(1-1, len(tokens)):\n",
    "        #     # store the previous n-1 words as history\n",
    "        #     history = tuple(tokens[i-1+1:i])\n",
    "        #     word = tokens[i]\n",
    "        #     # print(history, word)\n",
    "        #     # if the history is not in the dict, add it\n",
    "        #     if word not in one_hist_word_table:\n",
    "        #         one_hist_word_table[word] = {}\n",
    "        #     # if the word is not in the dict, add it\n",
    "        #     if history not in one_hist_word_table[word]:\n",
    "        #         one_hist_word_table[word][history] = 0\n",
    "        #     # increment the count\n",
    "        #     one_hist_word_table[word][history] += 1\n",
    "\n",
    "        # for i in range(n-1):\n",
    "        #     # add start tokens\n",
    "        #     tokens.insert(0, '<start>')\n",
    "        #     # add end tokens\n",
    "        #     tokens.append('<end>')\n",
    "\n",
    "        # add start tokens\n",
    "        tokens.insert(0, '<start>')\n",
    "        # add end tokens\n",
    "        tokens.append('<end>')\n",
    "\n",
    "        final_tokens.append(tokens)\n",
    "        # print(tokens)\n",
    "\n",
    "    # print(word_hist_table)\n",
    "    # print(hist_word_table)              \n",
    "\n",
    "    print(len(final_tokens))\n",
    "\n",
    "    # print(final_tokens[:100])\n",
    "    return final_tokens\n",
    "    # return final_tokens, one_word_hist_table, one_hist_word_table, two_word_hist_table, two_hist_word_table, three_word_hist_table, three_hist_word_table, four_word_hist_table, four_hist_word_table\n",
    "\n",
    "\n",
    "# issues : continuous punctuations are not separated\n",
    "\n",
    "n = 4\n",
    "\n",
    "# input_sentence = \"One can not decipher a man's feelings\"\n",
    "# input_sentence = \"This was invitation enough.\"\n",
    "# input_sentence = \"This was not invitation enough.\"\n",
    "# input_sentence = \"That is an evening gamer.\"\n",
    "# input_sentence = \"The boy bought a chocolate.\"\n",
    "# input_sentence = \"I sneezed loudly.\"\n",
    "input_sentence = \"He slid it into the left slot for them.\"\n",
    "\n",
    "# f_tokens, word_hist_table, hist_word_table = tokenize(input_sentence, n)\n",
    "# input_tokens, ig, fig, dig, hig, gig, hig, rig, wig = tokenize(input_sentence, n)\n",
    "input_tokens = tokenize(input_sentence, n)\n",
    "print(input_tokens[0])\n",
    "\n",
    "# corpus\n",
    "# path_to_corpus = 'test.txt'\n",
    "path_to_corpus = 'Pride and Prejudice - Jane Austen.txt'\n",
    "# path_to_corpus = 'Ulysses - James Joyce.txt'\n",
    "\n",
    "with open(path_to_corpus, 'r') as f:\n",
    "    text = f.read()\n",
    "# text = \"The boy ate a chocolate. The girl bought a chocolate. The girl ate a chocolate. The boy bought a horse.\"\n",
    "# corpus_tokens, one_word_hist_table, one_hist_word_table, two_word_hist_table, two_hist_word_table, three_word_hist_table, three_hist_word_table, four_word_hist_table, four_hist_word_table = tokenize(text, n)\n",
    "corpus_tokens = tokenize(text, n)\n",
    "\n",
    "# select random 70% of the sentences as training data\n",
    "train_tokens = random.sample(corpus_tokens, int(0.7*len(corpus_tokens)))\n",
    "# remove these from corpus_tokens\n",
    "corpus_tokens = [sentence for sentence in corpus_tokens if sentence not in train_tokens]\n",
    "# select random 50% of the sentences as validation data\n",
    "val_tokens = random.sample(corpus_tokens, int(0.5*len(corpus_tokens)))\n",
    "# remove these from corpus_tokens\n",
    "corpus_tokens = [sentence for sentence in corpus_tokens if sentence not in val_tokens]\n",
    "# the remaining sentences are test data\n",
    "test_tokens = corpus_tokens\n",
    "\n",
    "corpus_tokens = train_tokens\n",
    "print(len(corpus_tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corpus_tokens)\n",
    "# write corpus tokens to file\n",
    "with open('corpus_tokens.txt', 'w') as f:\n",
    "    for tokens in corpus_tokens:\n",
    "        # f.write(' '.join(tokens))\n",
    "        # f.write('\\n')\n",
    "        f.write(str(tokens))\n",
    "        f.write('\\n')\n",
    "\n",
    "# vocabulary = {}\n",
    "# for tokens in corpus_tokens:\n",
    "#     for token in tokens:\n",
    "#         if token not in vocabulary:\n",
    "#             vocabulary[token] = 0\n",
    "#         vocabulary[token] += 1\n",
    "# vocabulary['<unk>'] = 1\n",
    "# vocabulary = set()\n",
    "# for tokens in corpus_tokens:\n",
    "#     for token in tokens:\n",
    "#         vocabulary.add(token)\n",
    "# vocabulary.add('<unk>')\n",
    "# vocabulary = list(vocabulary)\n",
    "# print(len(vocabulary))\n",
    "# print(vocabulary)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa36ec43ef0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3593\n",
      "['<unk>', ',', '<end>', '<start>', '.', 'the', 'to', 'of', 'and', 'her']\n"
     ]
    }
   ],
   "source": [
    "# build a pytorch vocabulary from the vocabulary already built\n",
    "vocabulary = torchtext.vocab.build_vocab_from_iterator(corpus_tokens, specials=['<unk>'], min_freq=2)\n",
    "vocabulary.set_default_index(vocabulary['<unk>']) \n",
    "print(len(vocabulary))                         \n",
    "print(vocabulary.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from corpus using torchtext data loader\n",
    "def form_data_for_lstm(corpus_tokens, vocabulary, batch_size):\n",
    "    # encode each token in the corpus using the vocabulary as a list of indices\n",
    "    corpus_indices = []\n",
    "    for sentence in corpus_tokens:\n",
    "        indices = [vocabulary[token] for token in sentence]\n",
    "        corpus_indices.append(indices)\n",
    "    # print(corpus_indices[:10])\n",
    "    # print(len(corpus_indices))\n",
    "    # flatten the list of lists into a list\n",
    "    corpus_indices = [index for sentence in corpus_indices for index in sentence]\n",
    "    # print(len(corpus_indices))\n",
    "    corpus_indices = torch.LongTensor(corpus_indices)\n",
    "    num_batches = corpus_indices.shape[0] // batch_size \n",
    "    corpus_indices = corpus_indices[:num_batches * batch_size]                       \n",
    "    corpus_indices = corpus_indices.view(batch_size, num_batches)          \n",
    "    return corpus_indices\n",
    "\n",
    "batch_size = 16\n",
    "train_data = form_data_for_lstm(corpus_tokens, vocabulary, batch_size)\n",
    "valid_data = form_data_for_lstm(val_tokens, vocabulary, batch_size)\n",
    "test_data = form_data_for_lstm(test_tokens, vocabulary, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # if tie_weights:\n",
    "        self.embedding.weight = self.fc.weight\n",
    "        # self.init_weights()\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)          \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "    \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 1024             \n",
    "hidden_dim = 1024                \n",
    "num_layers = 2                   \n",
    "dropout_rate = 0.65              \n",
    "# tie_weights = True                  \n",
    "lr = 0.001\n",
    "\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the data into batches\n",
    "def make_batch(data, i, seq_len, num_batches):\n",
    "    # seq_len = min(seq_len, num_batches - 1 - i)\n",
    "    x = data[:, i:i+seq_len]\n",
    "    y = data[:, i+1:i+1+seq_len]\n",
    "    return x, y\n",
    "\n",
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in range(0, num_batches - 1, seq_len):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = make_batch(data, seq_len, num_batches, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)   \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = make_batch(data, seq_len, num_batches, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.71 GiB (GPU 0; 5.80 GiB total capacity; 2.63 GiB already allocated; 1.17 GiB free; 3.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m best_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 22\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_data, optimizer, criterion, \n\u001b[1;32m     23\u001b[0m                 batch_size, seq_len, clip, device)\n\u001b[1;32m     24\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_data, criterion, batch_size, \n\u001b[1;32m     25\u001b[0m                 seq_len, device)\n\u001b[1;32m     27\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep(valid_loss)\n",
      "Cell \u001b[0;32mIn[44], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m src, target \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m batch_size \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m prediction, hidden \u001b[39m=\u001b[39m model(src, hidden)               \n\u001b[1;32m     28\u001b[0m prediction \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39mreshape(batch_size \u001b[39m*\u001b[39m seq_len, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)   \n\u001b[1;32m     29\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[42], line 20\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, src, hidden)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, hidden):\n\u001b[1;32m     19\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(src))\n\u001b[0;32m---> 20\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embedding, hidden)          \n\u001b[1;32m     21\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(output) \n\u001b[1;32m     22\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    775\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.71 GiB (GPU 0; 5.80 GiB total capacity; 2.63 GiB already allocated; 1.17 GiB free; 3.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "n_epochs = 10\n",
    "seq_len = 50\n",
    "clip = 0.25\n",
    "saved = False\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "if saved:\n",
    "    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
    "else:\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_data, optimizer, criterion, \n",
    "                    batch_size, seq_len, clip, device)\n",
    "        valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                    seq_len, device)\n",
    "        \n",
    "        lr_scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train an lstm language model\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch_size, seq_length)\n",
    "#         # print(x.shape)\n",
    "#         out = self.embedding(x)\n",
    "#         # out shape: (batch_size, seq_length, embedding_dim)\n",
    "#         # print(out.shape)\n",
    "#         out, _ = self.lstm(out)\n",
    "#         # out shape: (batch_size, seq_length, hidden_dim)\n",
    "#         # print(out.shape)\n",
    "#         out = self.fc(out)\n",
    "#         # out shape: (batch_size, seq_length, vocab_size)\n",
    "#         # print(out.shape)\n",
    "#         return out\n",
    "    \n",
    "    \n",
    "# # hyperparameters\n",
    "# embedding_dim = 256\n",
    "# hidden_dim = 256\n",
    "# num_layers = 2\n",
    "# dropout = 0.5\n",
    "# num_epochs = 10\n",
    "# learning_rate = 0.001\n",
    "# batch_size = 64\n",
    "\n",
    "# # load data from corpus using torchtext data loader\n",
    "# def form_data_for_lstm(corpus_tokens, vocabulary, batch_size):\n",
    "#     # encode each token in the corpus using the vocabulary as a list of indices\n",
    "#     corpus_indices = []\n",
    "#     for sentence in corpus_tokens:\n",
    "#         indices = [vocabulary[token] for token in sentence]\n",
    "#         corpus_indices.append(indices)\n",
    "#     # print(corpus_indices[:10])\n",
    "#     # print(len(corpus_indices))\n",
    "#     # flatten the list of lists into a list\n",
    "#     corpus_indices = [index for sentence in corpus_indices for index in sentence]\n",
    "#     # print(len(corpus_indices))\n",
    "#     # corpus_indices = torch.LongTensor(corpus_indices)\n",
    "\n",
    "#     # create a dataset from the corpus indices\n",
    "#     dataset = torch.utils.data.TensorDataset(torch.LongTensor(corpus_indices))\n",
    "#     # create a data loader from the dataset\n",
    "#     data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "#     return data_loader\n",
    "\n",
    "# data_loader = form_data_for_lstm(corpus_tokens, vocabulary, batch_size)      \n",
    "\n",
    "# # instantiate the model\n",
    "# model = LSTM(len(vocabulary), embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "# print(model)\n",
    "\n",
    "# # loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # train the model\n",
    "# def train(model, data_loader, criterion, optimizer, num_epochs, vocabulary):\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for batch, input in enumerate(data_loader):\n",
    "#             # get input and targets\n",
    "#             # input shape: (batch_size, seq_length)\n",
    "#             # target shape: (batch_size, seq_length)\n",
    "#             print(input)\n",
    "#             # make each element tensors intead of tuples\n",
    "#             # input = [torch.LongTensor(x) for x in input]\n",
    "#             # print(input)\n",
    "\n",
    "#             input = torch.stack(input[:-1]).squeeze(1).to(device)\n",
    "#             target = torch.stack(input[1:]).squeeze(1).to(device)\n",
    "#             # print(input.shape)\n",
    "#             # print(target.shape)\n",
    "\n",
    "#             # forward\n",
    "#             # output shape: (batch_size, seq_length, vocab_size)\n",
    "#             output = model(input)\n",
    "#             # print(output.shape)\n",
    "#             # print(output.view(-1, len(vocabulary)).shape)\n",
    "#             # print(target.view(-1).shape)\n",
    "\n",
    "#             # backward\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = criterion(output.view(-1, len(vocabulary)), target.view(-1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # print statistics\n",
    "#             if batch % 100 == 0:\n",
    "#                 print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
    "#         # for batch in data_loader:\n",
    "#         #     print(batch)\n",
    "#         #     print(len(batch))\n",
    "#         #     print(batch[0][0].shape)\n",
    "\n",
    "#         #     # get input and targets\n",
    "#         #     # input shape: (batch_size, seq_length)\n",
    "#         #     # target shape: (batch_size, seq_length)\n",
    "#         #     input = torch.stack(batch[:-1][0]).squeeze(1).to(device)\n",
    "#         #     target = torch.stack(batch[1:][0]).squeeze(1).to(device)\n",
    "#         #     # print(input.shape)\n",
    "#         #     # print(target.shape)\n",
    "\n",
    "#         #     # forward\n",
    "#         #     # output shape: (batch_size, seq_length, vocab_size)\n",
    "#         #     output = model(input)\n",
    "#         #     # print(output.shape)\n",
    "#         #     # print(output.view(-1, len(vocabulary)).shape)\n",
    "#         #     # print(target.view(-1).shape)\n",
    "#         #     loss = criterion(output.view(-1, len(vocabulary)), target.view(-1))\n",
    "\n",
    "#         #     # backward\n",
    "#         #     optimizer.zero_grad()\n",
    "#         #     loss.backward()\n",
    "\n",
    "#         #     # gradient clipping\n",
    "#         #     nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "#         #     # update weights\n",
    "#         #     optimizer.step()\n",
    "\n",
    "#         # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# train(model, data_loader, criterion, optimizer, num_epochs, vocabulary)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
